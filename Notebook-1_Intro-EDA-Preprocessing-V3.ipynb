{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kwcbs5bBIV2"
      },
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ],
      "id": "6Kwcbs5bBIV2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
      },
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ],
      "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
      },
      "source": [
        "## Notebook 1: Intro, EDA and Preprocessing"
      ],
      "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
      },
      "source": [
        "### By Ryan Posternak"
      ],
      "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
      },
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ],
      "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
      },
      "source": [
        "## Goal: \n",
        "\n",
        "*This is a project for learning purposes. The *** is not involved with this project in any way.*\n",
        "\n",
        "<br>"
      ],
      "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
      },
      "source": [
        "# Overview and Business Understanding"
      ],
      "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
      },
      "source": [
        "<br>"
      ],
      "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
      },
      "source": [
        "# Data Understanding"
      ],
      "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
      },
      "source": [
        "<br>"
      ],
      "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
      },
      "source": [
        "# Imports, Reading in Data, and Exploratory Data Analysis"
      ],
      "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
      },
      "source": [
        "### Google colab compatibility downloads"
      ],
      "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b300561e-9b96-466f-dd3f-dc43b089671c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,622 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,310 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,095 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,937 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,369 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,533 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,073 kB]\n",
            "Fetched 13.6 MB in 2s (6,070 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.3.0\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 73.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=baf41a1119249dff71580f40b4c40bbb37b41ebc2d0684ec257fbe000f893a64\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "!pip install pyspark==3.3.0\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_EJVZLDUCRdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddacadb-5bc5-4d80-e72a-7075a5853635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "id": "_EJVZLDUCRdy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
      },
      "source": [
        "### Import libraries, packages and modules"
      ],
      "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "from itertools import chain\n",
        "import os\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, TimestampType\n",
        "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "# from pyspark.ml import Pipeline\n",
        "# from pyspark.ml.classification import LogisticRegression\n",
        "# from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "# from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ],
      "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc_V7XVRDx1U"
      },
      "outputs": [],
      "source": [
        "helper_functions = files.upload()\n",
        "from helper_functions import spark_resample"
      ],
      "id": "vc_V7XVRDx1U"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d0ac6c-20ed-451e-aca0-88b14c07532c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ],
      "source": [
        "# Check colab GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ],
      "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "8RdtiSPt22Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b89a56d-e4be-4ba4-c9cf-822008ff1fa0"
      },
      "id": "8RdtiSPt22Qe",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
      },
      "outputs": [],
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "      pre {\n",
        "          white-space: pre-wrap;\n",
        "      }\n",
        "    </style>\n",
        "    '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "33df418e-62d7-48fc-b9ce-f8776729c26f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Initialize Spark Session - Jupyter (local)\n",
        "# spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# Initialize Spark Session - Colab\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
      },
      "source": [
        "### Description of Features"
      ],
      "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8590747c-e25c-405c-815b-3c83f10309ea"
      },
      "source": [
        "**Dataset 1 – Transactions:**\n",
        "\n",
        "`MessageId` - Globally unique identifier within this dataset for individual transactions<br>\n",
        "`UETR` - The Unique End-to-end Transaction Reference—a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction<br>\n",
        "`TransactionReference` - Unique identifier for an individual transaction<br>\n",
        "`Timestamp` - Time at which the individual transaction was initiated<br>\n",
        "`Sender` - Institution (bank) initiating/sending the individual transaction<br>\n",
        "`Receiver` - Institution (bank) receiving the individual transaction<br>\n",
        "`OrderingAccount` - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction<br>\n",
        "`OrderingName` - Name for the originating ordering entity<br>\n",
        "`OrderingStreet` - Street address for the originating ordering entity<br>\n",
        "`OrderingCountryCityZip` - Remaining address details for the originating ordering entity<br>\n",
        "`BeneficiaryAccount` - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction<br>\n",
        "`BeneficiaryName` - Name for the final beneficiary entity<br>\n",
        "`BeneficiaryStreet` - Street address for the final beneficiary entity<br>\n",
        "`BeneficiaryCountryCityZip` - Remaining address details for the final beneficiary entity<br>\n",
        "`SettlementDate` - Date the individual transaction was settled<br>\n",
        "`SettlementCurrency` - Currency used for transaction<br>\n",
        "`SettlementAmount` - Value of the transaction net of fees/transfer charges/forex<br>\n",
        "`InstructedCurrency` - Currency of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`InstructedAmount` - Value of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`Label` - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.<br>\n",
        "<br>\n",
        "**Dataset 2 – Banks:**\n",
        "\n",
        "`Bank` - Identifier for the bank<br>\n",
        "`Account` - Identifier for the account<br>\n",
        "`Name` - Name of the account<br>\n",
        "`Street` - Street address associated with the account<br>\n",
        "`CountryCityZip` - Remaining address details associated with the account<br>\n",
        "`Flags` - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are below:<br>\n",
        "00 - No flags<br>\n",
        "01 - Account closed<br>\n",
        "03 - Account recently opened<br>\n",
        "04 - Name mismatch<br>\n",
        "05 - Account under monitoring<br>\n",
        "06 - Account suspended<br>\n",
        "07 - Account frozen<br>\n",
        "08 - Non-transaction account<br>\n",
        "09 - Beneficiary deceased<br>\n",
        "10 - Invalid company ID<br>\n",
        "11 - Invalid individual ID<br>\n",
        "<br>\n",
        "Additional information from data providers:<br>\n",
        "\"Because each end-to-end transaction is defined by one originating orderer and one final beneficiary, the `OrderingAccount` and `BeneficiaryAccount` in a given row may not necessarily belong to the bank in that row's `Sender` and the bank in that row's `Receiver`, respectively. The correct way to associate an `OrderingAccount` to the correct bank is to identify the `Sender` bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a `BeneficiaryAccount` to the correct bank is to identify the `Receiver` bank in the final (last) individual transaction in that end-to-end transaction.\""
      ],
      "id": "8590747c-e25c-405c-815b-3c83f10309ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "### Read in Data"
      ],
      "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dd295db2-0718-42c6-8a70-df41e93e72fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "655ed0a3-d546-4b89-8dcd-409538df7324"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Read in transactions training and testing data csv files to Spark DataFrames - Jupyter (local)\n",
        "# train_df = spark.read.csv('data/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "# test_df = spark.read.csv('data/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in transactions training and testing data csv files to Spark DataFrames - Colab\n",
        "train_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "test_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in banks data csv file to a Spark DataFrame\n",
        "banks_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/bank_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Persist in memory\n",
        "# train_df = train_df.cache()\n",
        "# test_df = test_df.cache()\n",
        "# banks_df = banks_df.cache()"
      ],
      "id": "dd295db2-0718-42c6-8a70-df41e93e72fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
      },
      "source": [
        "### Initial EDA"
      ],
      "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
      },
      "outputs": [],
      "source": [
        "# Print shape of dataframes\n",
        "print(f\"train_df:  {train_df.count():,} Rows, {len(train_df.columns)} Columns\")\n",
        "print(f\"test_df:  {test_df.count():,} Rows, {len(test_df.columns)} Columns\")\n",
        "print(f\"banks_df:  {banks_df.count():,} Rows, {len(banks_df.columns)} Columns\")"
      ],
      "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
      },
      "outputs": [],
      "source": [
        "# Print schema of train_df dataframe\n",
        "train_df.printSchema()"
      ],
      "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
      },
      "outputs": [],
      "source": [
        "# Print schema of banks_df dataframe\n",
        "banks_df.printSchema()"
      ],
      "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "11ab8183-bd76-48c2-b647-75285e34e4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "b955c2c6-0ed0-45f2-d5a5-ba7988182fc2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0---------------------------------------------------------\n",
            " MessageId                 | TRHW9K0QNS                           \n",
            " Timestamp                 | 2022-01-04 11:51:00                  \n",
            " UETR                      | 9af6f463-0fc4-4c66-a64d-476afb8116fb \n",
            " Sender                    | DPSUFRPP                             \n",
            " Receiver                  | ABVVUS6S                             \n",
            " TransactionReference      | PETX22-FXIDA-24929                   \n",
            " OrderingAccount           | FR41714755422956983780               \n",
            " OrderingName              | TANACETUM BIPINNATUM-HURONENSE       \n",
            " OrderingStreet            | 62| RUE DE SALMON                    \n",
            " OrderingCountryCityZip    | FR/07876 DIAZ                        \n",
            " BeneficiaryAccount        | 611024064274701282                   \n",
            " BeneficiaryName           | HETEROTHECA VILLOSA-SIERRABLANCENSIS \n",
            " BeneficiaryStreet         | UNIT 1896 BOX 8650                   \n",
            " BeneficiaryCountryCityZip | US/JENNIFERBERG| KS 61663            \n",
            " SettlementDate            | 220104                               \n",
            " SettlementCurrency        | USD                                  \n",
            " SettlementAmount          | 5258637.21                           \n",
            " InstructedCurrency        | EUR                                  \n",
            " InstructedAmount          | 4698148.14                           \n",
            " Label                     | 0                                    \n",
            " OriginalSender            | DPSUFRPP                             \n",
            " FinalReceiver             | ABVVUS6S                             \n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display first row of train_df\n",
        "train_df.show(n=1, vertical=True, truncate=False)"
      ],
      "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44472749-3189-4abf-b646-e0767399adbd"
      },
      "outputs": [],
      "source": [
        "# Display first 5 rows of banks dataframe\n",
        "banks_df.show(n=5, truncate=False)"
      ],
      "id": "44472749-3189-4abf-b646-e0767399adbd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
      },
      "outputs": [],
      "source": [
        "# Print number of null/missing values in each column of train_df\n",
        "train_df_null = train_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in train_df.columns if c != 'Timestamp'])\n",
        "\n",
        "print('Number of null/missing values per column:\\n')\n",
        "train_df_null.show(vertical=True, truncate=False)"
      ],
      "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
      },
      "outputs": [],
      "source": [
        "# # Print number of null/missing values in each column of banks_df\n",
        "banks_df_null = banks_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in banks_df.columns])\n",
        "\n",
        "print('Number of null/missing values per column:\\n')\n",
        "banks_df_null.show(vertical=True, truncate=False)"
      ],
      "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column of train_df; sample 1% of df for efficiency\n",
        "train_df_unique = train_df.sample(False, 0.01).agg(*(F.countDistinct(F.col(c)) for c in train_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column (in sample of 1% of dataframe):\\n\")\n",
        "train_df_unique.show(vertical=True, truncate=False)"
      ],
      "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column in banks_df; sample 10% of df for efficiency\n",
        "banks_df_unique = banks_df.sample(False, 0.1).agg(*(F.countDistinct(F.col(c)) for c in banks_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column (in sample of 10% of dataframe):\\n\")\n",
        "banks_df_unique.show(vertical=True, truncate=False)"
      ],
      "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DJZvwkuvZbs"
      },
      "outputs": [],
      "source": [
        "# Display Pandas style summary statistics table of numeric columns in train_df\n",
        "num_cols = [item[0] for item in train_df.dtypes if item[1] == 'int' or item[1] == 'double']\n",
        "\n",
        "train_df.select(num_cols).summary().show(truncate=False)"
      ],
      "id": "0DJZvwkuvZbs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) in train_df\n",
        "class_counts = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "class_counts.show(truncate=10)"
      ],
      "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like this is an extremely imbalanced dataset - only about 0.1% of the data is in the positive class. We will need to address this class imbalance as part of the modeling process."
      ],
      "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dQ_s4GsrJQV"
      },
      "source": [
        "<br>"
      ],
      "id": "7dQ_s4GsrJQV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kmSqoPErFV1"
      },
      "source": [
        "## Detailed EDA"
      ],
      "id": "0kmSqoPErFV1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTzpjevZ-3P9"
      },
      "outputs": [],
      "source": [
        "# Sample 2% of train_df for visualizations (approximately 94k observations)\n",
        "viz_df = train_df.sample(withReplacement=False, fraction=0.02, seed=42).toPandas()"
      ],
      "id": "tTzpjevZ-3P9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8MmRG_z0UnV"
      },
      "source": [
        "### Visualize target class distributions of sender and receiver banks used in transactions"
      ],
      "id": "V8MmRG_z0UnV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZk2VXXfnCKj"
      },
      "outputs": [],
      "source": [
        "# Display unique senders in training dataset\n",
        "print(f\"train_df, {train_df.select('Sender').distinct().count()} unique senders:\")\n",
        "train_df.select('Sender').distinct().show(5)"
      ],
      "id": "YZk2VXXfnCKj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique senders in training dataset where transaction is anomalous\n",
        "unique_anom_senders_count = train_df.filter(train_df.Label == 1)\\\n",
        "                                    .select('Sender').distinct().count()\n",
        "\n",
        "print(f\"train_df, {unique_anom_senders_count} unique senders among anomalous transactions:\")\n",
        "train_df.filter(train_df.Label == 1).select('Sender').distinct().show(5)"
      ],
      "metadata": {
        "id": "31jOa17PTIHV"
      },
      "id": "31jOa17PTIHV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft4XQOtuqqvG"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 0], ax=ax1, palette='muted', \n",
        "              order=viz_df[viz_df.Label == 0]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Set color palette to match values in y-axis above\n",
        "ax2_palette = {'DPSUFRPP': '#4878D0', 'WVOLDEMM': '#EE854A', 'ZOUOGB22': '#6ACC64', 'ABVVUS6S': '#956CB4'}\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 1], ax=ax2, palette=ax2_palette, \n",
        "              order=viz_df[viz_df.Label == 1]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Sender Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Sender Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "id": "Ft4XQOtuqqvG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEq48KnKpYCt"
      },
      "outputs": [],
      "source": [
        "# Display unique receivers in training dataset\n",
        "print(f\"train_df, {train_df.select('Receiver').distinct().count()} unique receivers:\")\n",
        "train_df.select('Receiver').distinct().show(5)"
      ],
      "id": "ZEq48KnKpYCt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique receivers in training dataset where transaction is anomalous\n",
        "unique_anom_receivers_count = train_df.filter(train_df.Label == 1)\\\n",
        "                                      .select('Receiver').distinct().count()\n",
        "\n",
        "print(f\"train_df, {unique_anom_receivers_count} unique receivers among anomalous transactions:\")\n",
        "train_df.filter(train_df.Label == 1).select('Receiver').distinct().show(5)"
      ],
      "metadata": {
        "id": "IeVQ6hS2UWLx"
      },
      "id": "IeVQ6hS2UWLx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABDfjBJ1zOa6"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Set consistent colors across y-axis values\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {val: color for val, color in zip(viz_df[viz_df.Label == 0]['Receiver'].value_counts().index, palette)}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "ax1_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 0], ax=ax1, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 0]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Update palette_map with values not found above\n",
        "for val, color in zip(viz_df[viz_df.Label == 1]['Receiver'].value_counts().index, palette):\n",
        "    if val not in palette_map:\n",
        "        palette_map[val] = 'silver'  # Assign values not found above to silver\n",
        "\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "ax2_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 1], ax=ax2, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 1]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Receiver Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Receiver Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "id": "ABDfjBJ1zOa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sr4eFI81RLe"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like the choice of sender bank is very informative in terms of determining whether a transaction is anomalous or not, while the choice of receiver bank is not nearly as valuable.\n",
        "- Only 4 out of 16 sender banks tend to be utilized in anomalous transactions, while nearly all are utilized in non-anomalous transactions.\n",
        "- Looking at receiver banks, 12 out of 16 tend to be utilized for both anomalous and non-anomalous transactions, and in roughly equal distributions.\n",
        "- There is no need to choose between sender and receiver banks when selecting our features; we can engineer features in sender-receiver bank combinations."
      ],
      "id": "5sr4eFI81RLe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmD5pLD-0hIe"
      },
      "source": [
        "## Visualize target class distributions of instructed and settlement currencies used in transactions"
      ],
      "id": "lmD5pLD-0hIe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx_M2kFJm9yx"
      },
      "outputs": [],
      "source": [
        "# Display unique instructed currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('InstructedCurrency').distinct().count()} unique instructed currencies:\")\n",
        "train_df.select('InstructedCurrency').distinct().show()"
      ],
      "id": "lx_M2kFJm9yx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzOj50aB_IRR"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent colors across x-axis values\n",
        "ax1_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "ax2_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan', \\\n",
        "               'AUD': 'lightseagreen', 'CAD': 'gray', 'NZD': 'plum', 'INR': '#FF9933'}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=ax1_palette)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=ax2_palette)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x, txt_y, txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.25\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x, txt_y, txt)\n",
        "\n",
        "ax1.set_title('Instructed Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Instructed Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "id": "GzOj50aB_IRR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--a2nbKmqtlW"
      },
      "outputs": [],
      "source": [
        "# Display unique settlement currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('SettlementCurrency').distinct().count()} unique settlement currencies:\")\n",
        "train_df.select('SettlementCurrency').distinct().show()"
      ],
      "id": "--a2nbKmqtlW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_4pIbQGN8GL"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent color palettes\n",
        "ax1_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "ax2_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan', \\\n",
        "               'AUD': 'lightseagreen', 'CAD': 'gray', 'NZD': 'plum', 'INR': '#FF9933'}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=ax1_palette)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=ax2_palette)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x, txt_y, txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x, txt_y, txt)\n",
        "\n",
        "ax1.set_title('Settlement Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Settlement Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "id": "9_4pIbQGN8GL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO7iezULOz8P"
      },
      "source": [
        "**Remarks:**\n",
        "- Instructed currencies seems to be more informative in terms of being correlated with whether or not a transaction is anomalous.\n",
        "- Among instructed currencies, we see the opposite trend as we saw with chosen banks; anomalous transactions tend to use a broader selection of instructed currencies, rather than a more narrow selection as we saw with chosen sender banks.\n",
        "- Among settlement currencies, we see the same four currencies being utilized among both target classes, but in slightly different frequencies.\n",
        "- We will keep the instructed currencies (and one hot encode them) as a feature in the final dataset and drop the settlement currencies."
      ],
      "id": "FO7iezULOz8P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "x_TRgs-i5mX4"
      },
      "id": "x_TRgs-i5mX4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
      },
      "source": [
        "# Preprocessing & Feature Engineering"
      ],
      "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
      },
      "source": [
        "Steps:\n",
        "1. Create `OriginalSender` and `FinalReceiver` features\n",
        "2. Create `SenderFlag` and `ReceiverFlag` features by joining `banks_df`\n",
        "3. Train/test split\n",
        "4. Create `SenderHourFreq` feature\n",
        "5. Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features\n",
        "6. Create `SenderReceiverFreq` feature"
      ],
      "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `OriginalSender` and `FinalReceiver` features"
      ],
      "metadata": {
        "id": "1hNM-s9Jt6gJ"
      },
      "id": "1hNM-s9Jt6gJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary tables for SQL join\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON EarliestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON LatestTransaction.UETR = train_df_sql.UETR\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gRk-mbypt61M",
        "outputId": "edf751e2-07d3-4245-a6d7-ba6bf281cb08"
      },
      "id": "gRk-mbypt61M",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three UETR codes of transactions where transactor used intermediary banks\n",
        "random_rows = np.random.choice(train_df.filter(train_df.Receiver != train_df.FinalReceiver).count(), size=3, replace=False)\n",
        "UETRs = [train_df.filter(train_df.Receiver != train_df.FinalReceiver).collect()[row]['UETR'] for row in random_rows]\n",
        "\n",
        "# Display transactions to ensure OriginalSender and FinalReceiver columns are accurate\n",
        "cols_to_show = ['Timestamp', 'UETR', 'Sender', 'Receiver', 'OriginalSender', 'FinalReceiver']\n",
        "train_df.filter(train_df.UETR == UETRs[0]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[1]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[2]).select(cols_to_show).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "OMFQEpbbvUXF",
        "outputId": "56ce3358-b581-46c8-926d-09f1f03a7188"
      },
      "id": "OMFQEpbbvUXF",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------------------------+--------+--------+--------------+-------------+\n",
            "|Timestamp          |UETR                                |Sender  |Receiver|OriginalSender|FinalReceiver|\n",
            "+-------------------+------------------------------------+--------+--------+--------------+-------------+\n",
            "|2022-01-23 09:30:00|ee2ada92-b9c4-4118-b3e5-bd2ce32e8706|WVOLDEMM|DPSUFRPP|WVOLDEMM      |DECKJPJJ     |\n",
            "|2022-01-23 10:17:00|ee2ada92-b9c4-4118-b3e5-bd2ce32e8706|DPSUFRPP|DECKJPJJ|WVOLDEMM      |DECKJPJJ     |\n",
            "+-------------------+------------------------------------+--------+--------+--------------+-------------+\n",
            "\n",
            "+-------------------+------------------------------------+--------+--------+--------------+-------------+\n",
            "|Timestamp          |UETR                                |Sender  |Receiver|OriginalSender|FinalReceiver|\n",
            "+-------------------+------------------------------------+--------+--------+--------------+-------------+\n",
            "|2022-01-14 08:24:00|158dda07-ab16-4541-b7ec-3ebfb0ae17db|DECKJPJJ|WVOLDEMM|DECKJPJJ      |ABVVUS6S     |\n",
            "|2022-01-14 09:08:00|158dda07-ab16-4541-b7ec-3ebfb0ae17db|WVOLDEMM|DPSUFRPP|DECKJPJJ      |ABVVUS6S     |\n",
            "|2022-01-14 10:35:00|158dda07-ab16-4541-b7ec-3ebfb0ae17db|DPSUFRPP|ABVVUS6S|DECKJPJJ      |ABVVUS6S     |\n",
            "+-------------------+------------------------------------+--------+--------+--------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `SenderFlag` and `ReceiverFlag` features by joining `banks_df`"
      ],
      "metadata": {
        "id": "wYeuL8b51iGn"
      },
      "id": "wYeuL8b51iGn"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fLOzGMds5K77"
      },
      "id": "fLOzGMds5K77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hdRqT-MA5LNv"
      },
      "id": "hdRqT-MA5LNv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fe0S3iD95LZh"
      },
      "id": "Fe0S3iD95LZh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6CMX7wnyShD"
      },
      "source": [
        "<br>"
      ],
      "id": "e6CMX7wnyShD"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "398a670b-b7e1-43b7-835d-8feac9644414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fd4f5fbf-3848-4c93-8652-de735b361978"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# # Create temporary tables for join\n",
        "train_df.sample(False, 0.1).createOrReplaceTempView('train_df_sql')\n",
        "banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "\n",
        "# # SQL to join dataframes\n",
        "# join_sql =  \"\"\"\n",
        "#     SELECT min(Timestamp) AS EarliestTransaction, \n",
        "#     max(Timestamp) AS LatestTransaction\n",
        "#     FROM train_df_sql\n",
        "#     Group by UETR\n",
        "# \"\"\"\n",
        "join_sql =  \"\"\"WITH OrderingAccounts AS (\n",
        "    SELECT OrderingAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "BeneficiaryAccounts AS (\n",
        "    SELECT BeneficiaryAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "AccountsOrdering AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "TransactionTimes AS (\n",
        "    SELECT min(Timestamp) AS EarliestTransaction, \n",
        "    max(Timestamp) AS LatestTransaction\n",
        "    FROM train_df_sql\n",
        "    Group by UETR\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "AccountsOrdering.Account AS MatchingOrderingAccount, \n",
        "AccountsOrdering.Flags AS OrderingAccountFlag, \n",
        "AccountsBeneficiary.Account AS MatchingBeneficiaryAccount, \n",
        "AccountsBeneficiary.Flags AS BeneficiaryAccountFlag\n",
        "FROM train_df_sql\n",
        "LEFT JOIN (\n",
        "    SELECT Account, max()\n",
        ")\n",
        "\n",
        "AccountsOrdering\n",
        "    ON train_df_sql.OrderingAccount = AccountsOrdering.Account\n",
        "\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON train_df_sql.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\n",
        "\n",
        "#             \"\"\"\n",
        "# # Perform SQL join\n",
        "joined_df = spark.sql(join_sql)"
      ],
      "id": "398a670b-b7e1-43b7-835d-8feac9644414"
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql =  \"\"\"\n",
        "WITH TransactionTimes AS (\n",
        "    SELECT UETR, \n",
        "    min(Timestamp) AS EarliestTransaction, \n",
        "    max(Timestamp) AS LatestTransaction\n",
        "    FROM train_df_sql\n",
        "    Group by UETR\n",
        "), \n",
        "OrderingAccounts AS (\n",
        "    SELECT OrderingAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "BeneficiaryAccounts AS (\n",
        "    SELECT BeneficiaryAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "AccountsOrdering AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "AccountsOrdering.Account AS MatchingOrderingAccount, \n",
        "AccountsOrdering.Flags AS OrderingAccountFlag, \n",
        "AccountsBeneficiary.Account AS MatchingBeneficiaryAccount, \n",
        "AccountsBeneficiary.Flags AS BeneficiaryAccountFlag\n",
        "FROM train_df_sql\n",
        "LEFT JOIN AccountsOrdering\n",
        "    ON train_df_sql.OrderingAccount = AccountsOrdering.Account\n",
        "        WHERE train_df_sql\n",
        "\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON train_df_sql.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lT9c2wEFDbGF"
      },
      "id": "lT9c2wEFDbGF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql = \"\"\"\n",
        "SELECT UETR, \n",
        "min(Timestamp) AS EarliestTransaction, \n",
        "max(Timestamp) AS LatestTransaction\n",
        "FROM train_df_sql\n",
        "GROUP BY UETR\n",
        "\"\"\"\n",
        "joined_df1 = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "1D52j-4PFbpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "630647ba-1995-4e11-db50-6cc0053ff2f3"
      },
      "id": "1D52j-4PFbpY",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH EarliestTransaction AS (\n",
        "join_sql = \"\"\"\n",
        "\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MIN(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "joined_df2 = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "c5HCv6VHHp-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a12028e2-c988-43e6-fc2f-03eb69266fae"
      },
      "id": "c5HCv6VHHp-v",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH LatestTransaction AS (\n",
        "join_sql = \"\"\"\n",
        "\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MAX(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "joined_df3 = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "ygbgInAVK9iT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f9ae8a35-19b6-49b0-a3be-9443ff60a465"
      },
      "id": "ygbgInAVK9iT",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON EarliestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON LatestTransaction.UETR = train_df_sql.UETR\n",
        "\"\"\"\n",
        "\n",
        "joined_df_SR = spark.sql(join_sql)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gdG9YA-5sNCx",
        "outputId": "2e47cc8e-757f-434f-e3a0-d94a67ccd7cd"
      },
      "id": "gdG9YA-5sNCx",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MIN(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MAX(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "), \n",
        "AccountsOrdering AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "AccountsOrdering.Account AS MatchingOrderingAccount, \n",
        "LatestTransaction.Receiver AS FinalReceiver, \n",
        "AccountsBeneficiary.Account AS MatchingReceivingAccount\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON EarliestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON LatestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN AccountsOrdering\n",
        "    ON EarliestTransaction.OrderingAccount = AccountsOrdering.Account\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON LatestTransaction.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\"\"\"\n",
        "\n",
        "joined_dfx = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "y8np07NvM2pC"
      },
      "id": "y8np07NvM2pC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bzqnbCzYmHVY"
      },
      "id": "bzqnbCzYmHVY",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OOAxZei1Lj7O"
      },
      "id": "OOAxZei1Lj7O",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M1YZzFrmmStF"
      },
      "id": "M1YZzFrmmStF",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YmlMsICNmVae"
      },
      "id": "YmlMsICNmVae",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vwiQctBTQrnd"
      },
      "id": "vwiQctBTQrnd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jXKDKx-Psxfi"
      },
      "id": "jXKDKx-Psxfi",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z3_FKmw6sxv5"
      },
      "id": "Z3_FKmw6sxv5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl7pzyqEDTnX"
      },
      "source": [
        "### Drop intermediary transactions (only keep one row per end-to-end transaction)"
      ],
      "id": "Nl7pzyqEDTnX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYOILJD0ISSL"
      },
      "outputs": [],
      "source": [
        "# Print count of unique transactions (as identified by UETR codes)\n",
        "print('train_df:')\n",
        "train_df.select(F.countDistinct('UETR')).show()\n",
        "print('test_df:')\n",
        "test_df.select(F.countDistinct('UETR')).show()"
      ],
      "id": "LYOILJD0ISSL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qoi-7oTDTCX"
      },
      "outputs": [],
      "source": [
        "# Print total number of combined rows with duplicate UETR values (meaning sender routed transaction through one or more intermediary banks)\n",
        "print('Total number of rows with intermediary transactions in train_df:')\n",
        "train_df.select('UETR').groupBy('UETR')\\\n",
        "    .count()\\\n",
        "    .where(F.col('count') > 1)\\\n",
        "    .select(F.sum('count'))\\\n",
        "    .show()"
      ],
      "id": "-qoi-7oTDTCX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyp2WUWgIRhP"
      },
      "outputs": [],
      "source": [
        "# Drop rows with duplicate UETR codes, keeping the first occurence (sorted by Timestamp)\n",
        "train_df = train_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR'])\n",
        "test_df = test_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR'])\n",
        "\n",
        "# Ensure no duplicates\n",
        "assert train_df.groupBy(train_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "assert test_df.groupBy(test_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "\n",
        "print(f\"train_df: {train_df.count()} rows\")\n",
        "print(f\"test_df: {test_df.count()} rows\")"
      ],
      "id": "Pyp2WUWgIRhP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znGbnVs0RE9Y"
      },
      "outputs": [],
      "source": [
        "# Show value counts for 'Label' column (classification target) in new train and test dataframes\n",
        "class_counts_train = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "class_counts_test = test_df.groupBy('Label').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "\n",
        "print('train_df:')\n",
        "class_counts_train.show(truncate=10)\n",
        "print('test_df:')\n",
        "class_counts_test.show(truncate=10)"
      ],
      "id": "znGbnVs0RE9Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyY21cvovTXM"
      },
      "source": [
        "### Create `SenderHourFreq` feature: transaction hour frequency for each sender\n",
        "\n",
        "This feature will tell us the frequency with which each sender initiated transactions for each hour of the day. This should capture the signal of the correlation between the sender and target class as well as the correlation between transaction hour and target class."
      ],
      "id": "NyY21cvovTXM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kz9zFgrR5Nj"
      },
      "outputs": [],
      "source": [
        "# Define UDF to extract hour from timestamp\n",
        "hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# Create new column of transaction hours\n",
        "train_df = train_df.withColumn('Hour', hour(train_df.Timestamp))\n",
        "test_df = test_df.withColumn('Hour', hour(test_df.Timestamp))\n",
        "\n",
        "# Create list of unique senders\n",
        "senders = train_df.select('Sender').toPandas()['Sender'].unique()\n",
        "\n",
        "# Create column of senders concatenated with hours\n",
        "train_df = train_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "test_df = test_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "pd_df = train_df.select('Sender', 'Hour').toPandas()\n",
        "\n",
        "# Create dictionary of sender hour frequency values to map from sender hour values\n",
        "sender_hour_frequency = {}\n",
        "for sender in senders:\n",
        "    sender_rows = pd_df[pd_df['Sender'] == sender]\n",
        "    for hour in range(24):\n",
        "        sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_hour_frequency dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])\n",
        "test_df = test_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ],
      "id": "4kz9zFgrR5Nj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKfj2pFGQGOW"
      },
      "source": [
        "### Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features: transaction currency frequency and average transaction amount per currency for each sender\n",
        "\n",
        "These features will tell us the frequency with which each sender initiated transactions for each currency, in the case of the first feature. For the second feature, it will tell us the average amount with which each sender sent each currency. These features may also be correlated with anomalous transactions."
      ],
      "id": "iKfj2pFGQGOW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZINrnCpHQjkN"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with instructed currencies\n",
        "train_df = train_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "test_df = test_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "\n",
        "pd_train_df = train_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "pd_test_df = test_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "\n",
        "# Create dictionary of sender currency frequency values to map from sender currency values\n",
        "sender_currency_freq = {}\n",
        "# Create dictionary of average sender currency values to map from sender currency values\n",
        "sender_currency_avg = {}\n",
        "\n",
        "for sc in set(\n",
        "    list(pd_train_df['SenderCurrency'].unique()) + list(pd_test_df['SenderCurrency'].unique())\n",
        "):\n",
        "    sender_currency_freq[sc] = len(pd_train_df[pd_train_df['SenderCurrency'] == sc])\n",
        "    sender_currency_avg[sc] = pd_train_df[pd_train_df['SenderCurrency'] == sc][\n",
        "        \"InstructedAmount\"\n",
        "    ].mean()\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_avg dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_avg.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])"
      ],
      "id": "ZINrnCpHQjkN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lRfKQqI51Il"
      },
      "source": [
        "### Create `SenderReceiverFreq` feature: sender-receiver combination frequency for each sender and receiver"
      ],
      "id": "3lRfKQqI51Il"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNPtx7aS50ic"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with receivers\n",
        "train_df = train_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "test_df = test_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "\n",
        "# Create dictionary of sender receiver frequency values to map from sender receiver values\n",
        "sender_receiver_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('SenderReceiver').toPandas()\n",
        "pd_test_df = test_df.select('SenderReceiver').toPandas()\n",
        "\n",
        "for sr in set(\n",
        "    list(pd_train_df['SenderReceiver'].unique()) + list(pd_test_df['SenderReceiver'].unique())\n",
        "):\n",
        "    sender_receiver_freq[sr] = len(pd_train_df[pd_train_df['SenderReceiver'] == sr])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_receiver_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_receiver_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])\n",
        "test_df = test_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])"
      ],
      "id": "GNPtx7aS50ic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFkdce4XbMAi"
      },
      "source": [
        "### Drop unused categorical columns\n",
        "\n",
        "We're going to drop all categorical columns here, save for the one we are one hot encoding which is `InstructedCurrency`"
      ],
      "id": "gFkdce4XbMAi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs_qEOpGR0qK"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Timestamp',\n",
        "    'UETR',\n",
        "    'Sender',\n",
        "    'Receiver',\n",
        "    'TransactionReference',\n",
        "    'OrderingAccount',\n",
        "    'OrderingName',\n",
        "    'OrderingStreet',\n",
        "    'OrderingCountryCityZip',\n",
        "    'BeneficiaryAccount',\n",
        "    'BeneficiaryName',\n",
        "    'BeneficiaryStreet',\n",
        "    'BeneficiaryCountryCityZip',\n",
        "    'SettlementDate',\n",
        "    'SettlementCurrency',\n",
        "    'SenderHour',\n",
        "    'SenderCurrency',\n",
        "    'SenderReceiver'\n",
        "]\n",
        "\n",
        "train_df = train_df.drop(*cols_to_drop)\n",
        "test_df = test_df.drop(*cols_to_drop)"
      ],
      "id": "bs_qEOpGR0qK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzRag26ORz4g"
      },
      "source": [
        "<br>"
      ],
      "id": "rzRag26ORz4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyxiNwEpbYsD"
      },
      "source": [
        "# Resample Training Dataset\n",
        "\n",
        "As we saw above, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll rebalance the dataset through a combination of undersampling the majority class (non-amomalous transactions) and oversampling the minority class (anomalous transactions). Using the function I wrote in helper_functions.py, we'll specify an even positive to negative class balance in the new resampled dataframe, with 500,000 observations (so approximately 250k of each)."
      ],
      "id": "iyxiNwEpbYsD"
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark_resample.__doc__)"
      ],
      "metadata": {
        "id": "Lf4Ww6LtPD7o"
      },
      "id": "Lf4Ww6LtPD7o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJkpUEvM-uc5"
      },
      "outputs": [],
      "source": [
        "# Resample train_df; specify 500,000 observations in new resampled dataframe, with \n",
        "# balanced (0.5) ratio of positive target class (1) to negative target class (0)\n",
        "train_df_resampled = spark_resample(train_df, ratio=0.5, new_count=500000, \n",
        "                                    class_field='Label', pos_class=1, shuffle=True, random_state=42)"
      ],
      "id": "hJkpUEvM-uc5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efDPlelAE7W"
      },
      "outputs": [],
      "source": [
        "# Print shape of resampled dataframe\n",
        "print(f\"train_df_resampled:  {train_df_resampled.count()} Rows, {len(train_df_resampled.columns)} Columns\")"
      ],
      "id": "7efDPlelAE7W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
      },
      "outputs": [],
      "source": [
        "# Preview resampled dataframe\n",
        "train_df_resampled.show(3, vertical=True)"
      ],
      "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmckUs8hfF5r"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) of resampled dataframe\n",
        "resampled_class_counts = train_df_resampled.groupBy('Label')\\\n",
        "                                           .count()\\\n",
        "                                           .withColumn('percent', F.col('count')/train_df_resampled.count())\n",
        "\n",
        "resampled_class_counts.show(truncate=10)"
      ],
      "id": "XmckUs8hfF5r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save resampled training dataframe and preprocessed test dataframe as CSV files"
      ],
      "metadata": {
        "id": "dNen6kOq3P17"
      },
      "id": "dNen6kOq3P17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOsBhtSO6QUw"
      },
      "outputs": [],
      "source": [
        "# train_df_resampled.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/train_df_resampled.csv', header=True)\n",
        "# test_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/test_df_preprocessed.csv', header=True)"
      ],
      "id": "dOsBhtSO6QUw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCztBi7ueB6S"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "HCztBi7ueB6S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbjtN2WLeWli"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "BbjtN2WLeWli"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCekZqiBolKn"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "zCekZqiBolKn"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HSts_ccbSots"
      },
      "id": "HSts_ccbSots",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2X4KQRGopWq"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "G2X4KQRGopWq"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ylfcNNFiO7c1"
      },
      "id": "ylfcNNFiO7c1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Notebook-1_Intro-EDA-Preprocessing-V3.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python (spark-env)",
      "language": "python",
      "name": "spark-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}