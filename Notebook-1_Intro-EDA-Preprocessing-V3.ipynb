{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ],
      "metadata": {
        "id": "6Kwcbs5bBIV2"
      },
      "id": "6Kwcbs5bBIV2"
    },
    {
      "cell_type": "markdown",
      "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a",
      "metadata": {
        "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
      },
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a",
      "metadata": {
        "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
      },
      "source": [
        "## Notebook 1: Intro, EDA and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3",
      "metadata": {
        "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
      },
      "source": [
        "### By Ryan Posternak"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae",
      "metadata": {
        "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
      },
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b",
      "metadata": {
        "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
      },
      "source": [
        "## Goal: \n",
        "\n",
        "*This is a project for learning purposes. The *** is not involved with this project in any way.*\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e",
      "metadata": {
        "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
      },
      "source": [
        "# Overview and Business Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c",
      "metadata": {
        "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089",
      "metadata": {
        "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb",
      "metadata": {
        "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
      },
      "source": [
        "# Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1",
      "metadata": {
        "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655",
      "metadata": {
        "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776806df-5137-44ac-8d76-c06d7906e0c1",
      "metadata": {
        "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
      },
      "source": [
        "# Imports, Reading in Data, and Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1",
      "metadata": {
        "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
      },
      "source": [
        "### Google colab compatibility downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b",
      "metadata": {
        "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "!pip install pyspark==3.3.0\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_EJVZLDUCRdy"
      },
      "id": "_EJVZLDUCRdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2",
      "metadata": {
        "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
      },
      "source": [
        "### Import libraries, packages and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d",
      "metadata": {
        "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "from itertools import chain\n",
        "import os\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, TimestampType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d",
      "metadata": {
        "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
      },
      "outputs": [],
      "source": [
        "# Check colab GPU info\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aa3b4878-4efd-472f-9c10-61710c893d10",
      "metadata": {
        "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
      },
      "outputs": [],
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "      pre {\n",
        "          white-space: pre-wrap;\n",
        "      }\n",
        "    </style>\n",
        "    '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da",
      "metadata": {
        "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "5a77618f-beed-4ebe-b2a1-1fe1fe18e718"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Initialize Spark Session\n",
        "\n",
        "# spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9",
      "metadata": {
        "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
      },
      "source": [
        "### Description of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8590747c-e25c-405c-815b-3c83f10309ea",
      "metadata": {
        "id": "8590747c-e25c-405c-815b-3c83f10309ea"
      },
      "source": [
        "**Dataset 1 – Transactions:**\n",
        "\n",
        "`MessageId` - Globally unique identifier within this dataset for individual transactions<br>\n",
        "`UETR` - The Unique End-to-end Transaction Reference—a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction<br>\n",
        "`TransactionReference` - Unique identifier for an individual transaction<br>\n",
        "`Timestamp` - Time at which the individual transaction was initiated<br>\n",
        "`Sender` - Institution (bank) initiating/sending the individual transaction<br>\n",
        "`Receiver` - Institution (bank) receiving the individual transaction<br>\n",
        "`OrderingAccount` - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction<br>\n",
        "`OrderingName` - Name for the originating ordering entity<br>\n",
        "`OrderingStreet` - Street address for the originating ordering entity<br>\n",
        "`OrderingCountryCityZip` - Remaining address details for the originating ordering entity<br>\n",
        "`BeneficiaryAccount` - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction<br>\n",
        "`BeneficiaryName` - Name for the final beneficiary entity<br>\n",
        "`BeneficiaryStreet` - Street address for the final beneficiary entity<br>\n",
        "`BeneficiaryCountryCityZip` - Remaining address details for the final beneficiary entity<br>\n",
        "`SettlementDate` - Date the individual transaction was settled<br>\n",
        "`SettlementCurrency` - Currency used for transaction<br>\n",
        "`SettlementAmount` - Value of the transaction net of fees/transfer charges/forex<br>\n",
        "`InstructedCurrency` - Currency of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`InstructedAmount` - Value of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`Label` - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.<br>\n",
        "<br>\n",
        "**Dataset 2 – Banks:**\n",
        "\n",
        "`Bank` - Identifier for the bank<br>\n",
        "`Account` - Identifier for the account<br>\n",
        "`Name` - Name of the account<br>\n",
        "`Street` - Street address associated with the account<br>\n",
        "`CountryCityZip` - Remaining address details associated with the account<br>\n",
        "`Flags` - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are below:<br>\n",
        "00 - No flags<br>\n",
        "01 - Account closed<br>\n",
        "03 - Account recently opened<br>\n",
        "04 - Name mismatch<br>\n",
        "05 - Account under monitoring<br>\n",
        "06 - Account suspended<br>\n",
        "07 - Account frozen<br>\n",
        "08 - Non-transaction account<br>\n",
        "09 - Beneficiary deceased<br>\n",
        "10 - Invalid company ID<br>\n",
        "11 - Invalid individual ID<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "### Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dd295db2-0718-42c6-8a70-df41e93e72fa",
      "metadata": {
        "id": "dd295db2-0718-42c6-8a70-df41e93e72fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6030dd1e-e69d-4403-b74f-1458508a9e14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Read in transactions training and testing data csv files to Spark DataFrames\n",
        "train_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "test_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in banks data csv file to a Spark DataFrame\n",
        "# banks_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/bank_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Persist in memory\n",
        "train_df = train_df.cache()\n",
        "test_df = test_df.cache()\n",
        "# banks_df = banks_df.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529532f1-4b2e-406d-bd3d-63f8784a168c",
      "metadata": {
        "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
      },
      "source": [
        "### Initial EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d",
      "metadata": {
        "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
      },
      "outputs": [],
      "source": [
        "# Print shape of dataframes\n",
        "print(f\"train_df:  {train_df.count():,} Rows, {len(train_df.columns)} Columns\")\n",
        "print(f\"test_df:  {test_df.count():,} Rows, {len(test_df.columns)} Columns\")\n",
        "print(f\"banks_df:  {banks_df.count():,} Rows, {len(banks_df.columns)} Columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0133a49-0718-421f-9e8d-2d60913780ce",
      "metadata": {
        "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
      },
      "outputs": [],
      "source": [
        "# Print schema of dataframe\n",
        "train_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922",
      "metadata": {
        "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
      },
      "outputs": [],
      "source": [
        "# banks_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ab8183-bd76-48c2-b647-75285e34e4ac",
      "metadata": {
        "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
      },
      "outputs": [],
      "source": [
        "# Display first row of train_df\n",
        "train_df.show(n=1, vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44472749-3189-4abf-b646-e0767399adbd",
      "metadata": {
        "id": "44472749-3189-4abf-b646-e0767399adbd"
      },
      "outputs": [],
      "source": [
        "# Display first 5 rows of banks dataframe\n",
        "# banks_df.show(n=5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1",
      "metadata": {
        "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
      },
      "outputs": [],
      "source": [
        "# Print number of null/missing values in each column of train_df\n",
        "train_df_null = train_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in train_df.columns if c != 'Timestamp'])\n",
        "\n",
        "print('Number of null/missing values per column:\\n')\n",
        "train_df_null.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4",
      "metadata": {
        "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
      },
      "outputs": [],
      "source": [
        "# # Print number of null/missing values in each column of banks_df\n",
        "# banks_df_null = banks_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "#                                  .alias(c) for c in banks_df.columns])\n",
        "\n",
        "# print('Number of null/missing values per column:\\n')\n",
        "# banks_df_null.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2896607b-d653-4cda-9601-f1c4e709a0e0",
      "metadata": {
        "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column of train_df; sample 1% of df for efficiency\n",
        "train_df_unique = train_df.sample(False, 0.01).agg(*(F.countDistinct(F.col(c)) for c in train_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column (in sample of 1% of dataframe):\\n\")\n",
        "train_df_unique.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990",
      "metadata": {
        "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column in banks_df; sample 10% of df for efficiency\n",
        "# banks_df_unique = banks_df.sample(False, 0.1).agg(*(F.countDistinct(F.col(c)) for c in banks_df.columns))\n",
        "\n",
        "# print(f\"Number of unique values per column (in sample of 10% of dataframe):\\n\")\n",
        "# banks_df_unique.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Pandas style summary statistics table of numeric columns in train_df\n",
        "num_cols = [item[0] for item in train_df.dtypes if item[1] == 'int' or item[1] == 'double']\n",
        "\n",
        "train_df.select(num_cols).summary().show(truncate=False)\n"
      ],
      "metadata": {
        "id": "0DJZvwkuvZbs"
      },
      "id": "0DJZvwkuvZbs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc",
      "metadata": {
        "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) in train_df\n",
        "class_counts = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "class_counts.show(truncate=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55",
      "metadata": {
        "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like this is an extremely imbalanced dataset - only about 0.1% of the data is in the positive class. We will need to address this class imbalance as part of the modeling process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "7dQ_s4GsrJQV"
      },
      "id": "7dQ_s4GsrJQV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detailed EDA"
      ],
      "metadata": {
        "id": "0kmSqoPErFV1"
      },
      "id": "0kmSqoPErFV1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 2% of train_df for visualizations (approximately 94k observations)\n",
        "viz_df = train_df.sample(withReplacement=False, fraction=0.02, seed=42).toPandas()"
      ],
      "metadata": {
        "id": "tTzpjevZ-3P9"
      },
      "id": "tTzpjevZ-3P9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize target class distributions of sender and receiver banks used in transactions"
      ],
      "metadata": {
        "id": "V8MmRG_z0UnV"
      },
      "id": "V8MmRG_z0UnV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique senders in training dataset\n",
        "print(f\"train_df, {train_df.select('Sender').distinct().count()} unique senders:\")\n",
        "train_df.select('Sender').distinct().show(5)"
      ],
      "metadata": {
        "id": "YZk2VXXfnCKj"
      },
      "id": "YZk2VXXfnCKj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 14))\n",
        "\n",
        "# Plot non-anomalous countplot\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 0], ax=ax1, palette='muted', \n",
        "              order=viz_df[viz_df.Label == 0]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Set color palette to match values in y-axis above\n",
        "ax2_palette = {'DPSUFRPP': '#4878D0', 'WVOLDEMM': '#EE854A', 'ZOUOGB22': '#6ACC64', 'ABVVUS6S': '#956CB4'}\n",
        "\n",
        "# Plot anomalous countplot\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 1], ax=ax2, palette=ax2_palette, \n",
        "              order=viz_df[viz_df.Label == 1]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "ax1.set_title('Sender Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Sender Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "metadata": {
        "id": "Ft4XQOtuqqvG"
      },
      "id": "Ft4XQOtuqqvG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique receivers in training dataset\n",
        "print(f\"train_df, {train_df.select('Receiver').distinct().count()} unique receivers:\")\n",
        "train_df.select('Receiver').distinct().show(5)"
      ],
      "metadata": {
        "id": "ZEq48KnKpYCt"
      },
      "id": "ZEq48KnKpYCt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 14))\n",
        "\n",
        "# Set consistent colors across y-axis values\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {val: color for val, color in zip(viz_df[viz_df.Label == 0]['Receiver'].value_counts().index, palette)}\n",
        "\n",
        "# Plot non-anomalous countplot\n",
        "ax1_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 0], ax=ax1, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 0]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Update palette_map with values not found above\n",
        "for val, color in zip(viz_df[viz_df.Label == 1]['Receiver'].value_counts().index, palette):\n",
        "    if val not in palette_map:\n",
        "        palette_map[val] = 'silver'  # Assign values not found above to silver\n",
        "\n",
        "\n",
        "# Plot anomalous countplot\n",
        "ax2_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 1], ax=ax2, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 1]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "ax1.set_title('Receiver Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Receiver Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "metadata": {
        "id": "ABDfjBJ1zOa6"
      },
      "id": "ABDfjBJ1zOa6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- It looks like the choice of sender bank is very informative in terms of determining whether a transaction is anomalous or not, while the choice of receiver bank is not nearly as valuable.\n",
        "- Only 4 out of 16 sender banks tend to be utilized in anomalous transactions, while nearly all are utilized in non-anomalous transactions.\n",
        "- Looking at receiver banks, 12 out of 16 tend to be utilized for both anomalous and non-anomalous transactions, and in roughly equal distributions.\n",
        "- There is no need to choose between sender and receiver banks when selecting our features; we can engineer features in sender-receiver bank combinations."
      ],
      "metadata": {
        "id": "5sr4eFI81RLe"
      },
      "id": "5sr4eFI81RLe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize target class distributions of instructed and settlement currencies used in transactions"
      ],
      "metadata": {
        "id": "lmD5pLD-0hIe"
      },
      "id": "lmD5pLD-0hIe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique instructed currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('InstructedCurrency').distinct().count()} unique instructed currencies:\")\n",
        "train_df.select('InstructedCurrency').distinct().show()"
      ],
      "metadata": {
        "id": "lx_M2kFJm9yx"
      },
      "id": "lx_M2kFJm9yx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 7))\n",
        "\n",
        "# Set consistent colors across x-axis values\n",
        "ax1_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "ax2_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan', \\\n",
        "               'AUD': 'lightseagreen', 'CAD': 'gray', 'NZD': 'plum', 'INR': '#FF9933'}\n",
        "\n",
        "# Plot non-anomalous countplot\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=ax1_palette)\n",
        "# Plot anomalous countplot\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=ax2_palette)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x,txt_y,txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.25\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x,txt_y,txt)\n",
        "\n",
        "ax1.set_title('Instructed Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Instructed Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "metadata": {
        "id": "GzOj50aB_IRR"
      },
      "id": "GzOj50aB_IRR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique settlement currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('SettlementCurrency').distinct().count()} unique settlement currencies:\")\n",
        "train_df.select('SettlementCurrency').distinct().show()"
      ],
      "metadata": {
        "id": "--a2nbKmqtlW"
      },
      "id": "--a2nbKmqtlW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 7))\n",
        "\n",
        "# Set consistent color palettes\n",
        "ax1_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "ax2_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan', \\\n",
        "               'AUD': 'lightseagreen', 'CAD': 'gray', 'NZD': 'plum', 'INR': '#FF9933'}\n",
        "\n",
        "# Plot countplots\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=ax1_palette)\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=ax2_palette)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x,txt_y,txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x,txt_y,txt)\n",
        "\n",
        "ax1.set_title('Settlement Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Settlement Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "metadata": {
        "id": "9_4pIbQGN8GL"
      },
      "id": "9_4pIbQGN8GL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- Instructed currencies seems to be more informative in terms of being correlated with whether or not a transaction is anomalous.\n",
        "- Among instructed currencies, we see the opposite trend as we saw with chosen banks; anomalous transactions tend to use a broader selection of instructed currencies, rather than a more narrow selection as we saw with chosen sender banks.\n",
        "- Among settlement currencies, we see the same four currencies being utilized among both target classes, but in slightly different frequencies.\n",
        "- We will keep the instructed currencies (and one hot encode them) as a feature in the final dataset and drop the settlement currencies."
      ],
      "metadata": {
        "id": "FO7iezULOz8P"
      },
      "id": "FO7iezULOz8P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "e6CMX7wnyShD"
      },
      "id": "e6CMX7wnyShD"
    },
    {
      "cell_type": "markdown",
      "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4",
      "metadata": {
        "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
      },
      "source": [
        "# Preprocessing & Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3",
      "metadata": {
        "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
      },
      "source": [
        "Steps:\n",
        "1. Drop duplicate transaction rows\n",
        "2. Train/test split\n",
        "3. Create `SenderHourFreq` feature\n",
        "4. Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features\n",
        "5. Create `SenderReceiverFreq` feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398a670b-b7e1-43b7-835d-8feac9644414",
      "metadata": {
        "id": "398a670b-b7e1-43b7-835d-8feac9644414"
      },
      "outputs": [],
      "source": [
        "# # Create temporary tables for join\n",
        "# train_df.createOrReplaceTempView('train_df_sql')\n",
        "# banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "\n",
        "# # SQL to join dataframes\n",
        "# join_sql =  \"\"\"SELECT train_df_sql.*, banks_df_sql.Flags AS OrderingAccFlags\n",
        "#             FROM train_df_sql \n",
        "#             LEFT JOIN banks_df_sql \n",
        "#             ON train_df_sql.OrderingAccount = banks_df_sql.Account\n",
        "#             \"\"\"\n",
        "# # Perform SQL join\n",
        "# joined_df = spark.sql(join_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde",
      "metadata": {
        "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
      },
      "outputs": [],
      "source": [
        "# Print shape of joined dataframe\n",
        "# print(f\"{joined_df.count():,} Rows, {len(joined_df.columns)} Columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop intermediary transactions (only keep one row per end-to-end transaction)"
      ],
      "metadata": {
        "id": "Nl7pzyqEDTnX"
      },
      "id": "Nl7pzyqEDTnX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print count of unique transactions (as identified by UETR codes)\n",
        "print('train_df:')\n",
        "train_df.select(F.countDistinct('UETR')).show()\n",
        "print('test_df:')\n",
        "test_df.select(F.countDistinct('UETR')).show()"
      ],
      "metadata": {
        "id": "LYOILJD0ISSL"
      },
      "id": "LYOILJD0ISSL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print total number of combined rows with duplicate UETR values (meaning sender routed transaction through one or more intermediary banks)\n",
        "print('Total number of rows with intermediary transactions in train_df:')\n",
        "train_df.select('UETR').groupBy('UETR')\\\n",
        "    .count()\\\n",
        "    .where(F.col('count') > 1)\\\n",
        "    .select(F.sum('count'))\\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "-qoi-7oTDTCX"
      },
      "id": "-qoi-7oTDTCX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with duplicate UETR codes, keeping the first occurence (sorted by Timestamp)\n",
        "train_df = train_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR']).cache()\n",
        "test_df = test_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR']).cache()\n",
        "\n",
        "# Ensure no duplicates\n",
        "# assert train_df.groupBy(train_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "# assert test_df.groupBy(test_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "\n",
        "print(f\"train_df: {train_df.count()} rows\")\n",
        "print(f\"test_df: {test_df.count()} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Pyp2WUWgIRhP",
        "outputId": "2b47d475-34e0-476d-9eeb-b5bb5d79e929"
      },
      "id": "Pyp2WUWgIRhP",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show value counts for 'Label' column (classification target) in new train and test dataframes\n",
        "class_counts_train = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "class_counts_test = test_df.groupBy('Label').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "\n",
        "print('train_df:')\n",
        "class_counts_train.show(truncate=10)\n",
        "print('test_df:')\n",
        "class_counts_test.show(truncate=10)"
      ],
      "metadata": {
        "id": "znGbnVs0RE9Y"
      },
      "id": "znGbnVs0RE9Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "_KTP3eqWcAeW"
      },
      "id": "_KTP3eqWcAeW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `SenderHourFreq` feature: transaction hour frequency for each sender\n",
        "\n",
        "This feature will tell us the frequency with which each sender initiated transactions for each hour of the day. This should capture the signal of the correlation between the sender and target class as well as the correlation between transaction hour and target class."
      ],
      "metadata": {
        "id": "NyY21cvovTXM"
      },
      "id": "NyY21cvovTXM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define UDF to extract hour from timestamp\n",
        "hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# Create new column of transaction hours\n",
        "train_df = train_df.withColumn('Hour', hour(train_df.Timestamp))\n",
        "test_df = test_df.withColumn('Hour', hour(test_df.Timestamp))\n",
        "\n",
        "# Create list of unique senders\n",
        "senders = train_df.select('Sender').toPandas()['Sender'].unique()\n",
        "\n",
        "# Create column of senders concatenated with hours\n",
        "train_df = train_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "test_df = test_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "pd_df = train_df.select('Sender', 'Hour').toPandas()\n",
        "\n",
        "# Create dictionary of sender hour frequency values to map from sender hour values\n",
        "sender_hour_frequency = {}\n",
        "for sender in senders:\n",
        "    sender_rows = pd_df[pd_df['Sender'] == sender]\n",
        "    for hour in range(24):\n",
        "        sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_hour_frequency dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])\n",
        "test_df = test_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ],
      "metadata": {
        "id": "4kz9zFgrR5Nj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "471c3373-ae60-4965-92d9-348a7549c24c"
      },
      "id": "4kz9zFgrR5Nj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features: transaction currency frequency and average transaction amount per currency for each sender\n",
        "\n",
        "These features will tell us the frequency with which each sender initiated transactions for each currency, in the case of the first feature. For the second feature, it will tell us the average amount with which each sender sent each currency. These features may also be correlated with anomalous transactions."
      ],
      "metadata": {
        "id": "iKfj2pFGQGOW"
      },
      "id": "iKfj2pFGQGOW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create column of senders concatenated with instructed currencies\n",
        "train_df = train_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "test_df = test_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "\n",
        "pd_train_df = train_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "pd_test_df = test_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "\n",
        "# Create dictionary of sender currency frequency values to map from sender currency values\n",
        "sender_currency_freq = {}\n",
        "# Create dictionary of average sender currency values to map from sender currency values\n",
        "sender_currency_avg = {}\n",
        "\n",
        "for sc in set(\n",
        "    list(pd_train_df['SenderCurrency'].unique()) + list(pd_test_df['SenderCurrency'].unique())\n",
        "):\n",
        "    sender_currency_freq[sc] = len(pd_train_df[pd_train_df['SenderCurrency'] == sc])\n",
        "    sender_currency_avg[sc] = pd_train_df[pd_train_df['SenderCurrency'] == sc][\n",
        "        \"InstructedAmount\"\n",
        "    ].mean()\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_avg dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_avg.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])"
      ],
      "metadata": {
        "id": "ZINrnCpHQjkN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0595d7fe-2480-4985-8939-49b62534f0c8"
      },
      "id": "ZINrnCpHQjkN",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `SenderReceiverFreq` feature: sender-receiver combination frequency for each sender and receiver"
      ],
      "metadata": {
        "id": "3lRfKQqI51Il"
      },
      "id": "3lRfKQqI51Il"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create column of senders concatenated with receivers\n",
        "train_df = train_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "test_df = test_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "\n",
        "# Create dictionary of sender receiver frequency values to map from sender receiver values\n",
        "sender_receiver_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('SenderReceiver').toPandas()\n",
        "pd_test_df = test_df.select('SenderReceiver').toPandas()\n",
        "\n",
        "for sr in set(\n",
        "    list(pd_train_df['SenderReceiver'].unique()) + list(pd_test_df['SenderReceiver'].unique())\n",
        "):\n",
        "    sender_receiver_freq[sr] = len(pd_train_df[pd_train_df['SenderReceiver'] == sr])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_receiver_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_receiver_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])\n",
        "test_df = test_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])"
      ],
      "metadata": {
        "id": "GNPtx7aS50ic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d34f7dcc-cba7-463e-b146-fd29d3ebee6a"
      },
      "id": "GNPtx7aS50ic",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop unused categorical columns\n",
        "\n",
        "We're going to drop all categorical columns here, save for the one we are one hot encoding which is `InstructedCurrency`"
      ],
      "metadata": {
        "id": "gFkdce4XbMAi"
      },
      "id": "gFkdce4XbMAi"
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = [\n",
        "    'Timestamp',\n",
        "    'UETR',\n",
        "    'Sender',\n",
        "    'Receiver',\n",
        "    'TransactionReference',\n",
        "    'OrderingAccount',\n",
        "    'OrderingName',\n",
        "    'OrderingStreet',\n",
        "    'OrderingCountryCityZip',\n",
        "    'BeneficiaryAccount',\n",
        "    'BeneficiaryName',\n",
        "    'BeneficiaryStreet',\n",
        "    'BeneficiaryCountryCityZip',\n",
        "    'SettlementDate',\n",
        "    'SettlementCurrency',\n",
        "    'SenderHour',\n",
        "    'SenderCurrency',\n",
        "    'SenderReceiver'\n",
        "]\n",
        "\n",
        "train_df_2 = train_df.drop(*cols_to_drop)\n",
        "# test_df = test_df.drop(*cols_to_drop)"
      ],
      "metadata": {
        "id": "bs_qEOpGR0qK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d23d282c-3dfe-40d8-cc4a-a60fd385c900"
      },
      "id": "bs_qEOpGR0qK",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "rzRag26ORz4g"
      },
      "id": "rzRag26ORz4g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rebalance Training Dataset\n",
        "\n",
        "As we saw above, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll rebalance the dataset using a combination of over-sampling the minority class and under-sampling the majority class."
      ],
      "metadata": {
        "id": "ZA47dMIRRzM6"
      },
      "id": "ZA47dMIRRzM6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resample Training Dataset\n",
        "\n",
        "As we saw above, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll rebalance the dataset by under-sampling the majority class (non-amomalous transactions). We will set a class ratio of anomalous to non-anomalous of 0.01, which means we should end up with about 45k observations after re-sampling."
      ],
      "metadata": {
        "id": "iyxiNwEpbYsD"
      },
      "id": "iyxiNwEpbYsD"
    },
    {
      "cell_type": "code",
      "source": [
        "def resample(base_features, ratio, class_field, base_class):\n",
        "    pos = base_features.filter(F.col(class_field)==base_class)\n",
        "    neg = base_features.filter(F.col(class_field)!=base_class)\n",
        "    total_pos = pos.count()\n",
        "    total_neg = neg.count()\n",
        "    fraction=float(total_pos*ratio)/float(total_neg)\n",
        "    sampled = neg.sample(False,fraction)\n",
        "    return sampled.union(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yNJ0_PKKarNZ",
        "outputId": "65f325a1-6364-45b9-e7dd-7ce6e61fc130"
      },
      "id": "yNJ0_PKKarNZ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8",
      "metadata": {
        "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "114056d5-6f42-4f36-a701-9d49715dd745"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "resampled_df = resample(train_df_2, ratio=10, class_field='Label', base_class=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "XmckUs8hfF5r",
        "outputId": "af1707bd-7db9-4a12-e560-6e524d3c5980"
      },
      "id": "XmckUs8hfF5r",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+------------------+----------------+-----+----+--------------+------------------+--------------------+------------------+\n",
            "| MessageId|SettlementAmount|InstructedCurrency|InstructedAmount|Label|Hour|SenderHourFreq|SenderCurrencyFreq|SenderCurrencyAmtAvg|SenderReceiverFreq|\n",
            "+----------+----------------+------------------+----------------+-----+----+--------------+------------------+--------------------+------------------+\n",
            "|TROQLMZD4V|       892810.77|               USD|      1110850.78|    0|   7|         38836|            274065|  1671495.4797866924|            274065|\n",
            "|TR3GBVOO9Z|      5407327.68|               USD|      5407327.68|    0|  10|         11700|            514627|   4998350.565032887|            514627|\n",
            "|TR6W5OAXN0|   5.511968704E7|               EUR|    5.51196513E7|    0|   8|         24632|            405122|4.7688118533852145E7|            307192|\n",
            "|TR9389RQGS|      6869296.86|               GBP|      5520611.48|    0|   9|         51239|            218987|  3691762.8543079915|           1628507|\n",
            "|TRWU4Q1B3B|       935556.13|               GBP|       935536.13|    0|  10|         31727|            529744|  1674064.4259055888|            267752|\n",
            "+----------+----------------+------------------+----------------+-----+----+--------------+------------------+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dOsBhtSO6QUw",
        "outputId": "7666b243-f47f-4d91-f41e-d1a86165f199"
      },
      "id": "dOsBhtSO6QUw",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53263"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display value counts for 'Label' column (classification target) in train_df\n",
        "class_counts_test = resampled_df.groupBy('Label').count().withColumn('percent', F.col('count')/resampled_df.count())\n",
        "\n",
        "class_counts_test.show(truncate=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "HCztBi7ueB6S",
        "outputId": "b337f064-9968-4229-d61a-e933c0345388"
      },
      "id": "HCztBi7ueB6S",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+\n",
            "|Label|count|   percent|\n",
            "+-----+-----+----------+\n",
            "|    0|48411|0.90890...|\n",
            "|    1| 4852|0.09109...|\n",
            "+-----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_df.sample(True, 2.0).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "BbjtN2WLeWli",
        "outputId": "5b5496c0-b000-4f53-857a-1597f53039b5"
      },
      "id": "BbjtN2WLeWli",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106610"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (spark-env)",
      "language": "python",
      "name": "spark-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "name": "Notebook-1_Intro-EDA-Preprocessing-V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}