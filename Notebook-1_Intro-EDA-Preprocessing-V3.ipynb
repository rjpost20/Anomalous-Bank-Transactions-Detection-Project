{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kwcbs5bBIV2"
      },
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ],
      "id": "6Kwcbs5bBIV2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
      },
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ],
      "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
      },
      "source": [
        "## Notebook 1: Intro, EDA and Preprocessing"
      ],
      "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
      },
      "source": [
        "### By Ryan Posternak"
      ],
      "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
      },
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ],
      "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
      },
      "source": [
        "## Goal: \n",
        "\n",
        "*This is a project for learning purposes. The *** is not involved with this project in any way.*\n",
        "\n",
        "<br>"
      ],
      "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
      },
      "source": [
        "# Overview and Business Understanding"
      ],
      "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
      },
      "source": [
        "<br>"
      ],
      "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
      },
      "source": [
        "# Data Understanding"
      ],
      "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
      },
      "source": [
        "<br>"
      ],
      "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
      },
      "source": [
        "# Imports, Reading in Data, and Exploratory Data Analysis"
      ],
      "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
      },
      "source": [
        "### Google colab compatibility downloads"
      ],
      "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b",
        "outputId": "b1661346-1626-400a-dd90-6c94d89f620f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [4 In\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [6 In\u001b[0m\r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [90.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [903 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,369 kB]\n",
            "Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,141 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,310 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,533 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,095 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,100 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,937 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,073 kB]\n",
            "Get:24 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [47.7 kB]\n",
            "Fetched 17.0 MB in 4s (4,305 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.3.0\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 42 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 42.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=af6681928250a1b4bf640bcb0ad9698f48226ca042bbbb32369fc0d4d299f468\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "!pip install pyspark==3.3.0\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EJVZLDUCRdy",
        "outputId": "c59fd125-1656-46b1-c083-c05c43687ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "id": "_EJVZLDUCRdy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
      },
      "source": [
        "### Import libraries, packages and modules"
      ],
      "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "from itertools import chain\n",
        "import os\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, TimestampType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "# import matplotlib_inline.backend_inline\n",
        "# matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline"
      ],
      "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": "OK"
            }
          }
        },
        "id": "vc_V7XVRDx1U",
        "outputId": "cf08ad65-9960-408d-a042-4eacbf3acd6b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-949bc614-6da1-4709-be20-bc80ed4305b7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-949bc614-6da1-4709-be20-bc80ed4305b7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving helper_functions.py to helper_functions.py\n"
          ]
        }
      ],
      "source": [
        "helper_functions = files.upload()\n",
        "from helper_functions import spark_resample"
      ],
      "id": "vc_V7XVRDx1U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
      },
      "outputs": [],
      "source": [
        "# Check colab GPU info\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ],
      "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
      },
      "outputs": [],
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "      pre {\n",
        "          white-space: pre-wrap;\n",
        "      }\n",
        "    </style>\n",
        "    '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da",
        "outputId": "4819498e-1125-4777-95d6-0bd3aba015f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Initialize Spark Session\n",
        "\n",
        "# spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
      },
      "source": [
        "### Description of Features"
      ],
      "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8590747c-e25c-405c-815b-3c83f10309ea"
      },
      "source": [
        "**Dataset 1 – Transactions:**\n",
        "\n",
        "`MessageId` - Globally unique identifier within this dataset for individual transactions<br>\n",
        "`UETR` - The Unique End-to-end Transaction Reference—a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction<br>\n",
        "`TransactionReference` - Unique identifier for an individual transaction<br>\n",
        "`Timestamp` - Time at which the individual transaction was initiated<br>\n",
        "`Sender` - Institution (bank) initiating/sending the individual transaction<br>\n",
        "`Receiver` - Institution (bank) receiving the individual transaction<br>\n",
        "`OrderingAccount` - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction<br>\n",
        "`OrderingName` - Name for the originating ordering entity<br>\n",
        "`OrderingStreet` - Street address for the originating ordering entity<br>\n",
        "`OrderingCountryCityZip` - Remaining address details for the originating ordering entity<br>\n",
        "`BeneficiaryAccount` - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction<br>\n",
        "`BeneficiaryName` - Name for the final beneficiary entity<br>\n",
        "`BeneficiaryStreet` - Street address for the final beneficiary entity<br>\n",
        "`BeneficiaryCountryCityZip` - Remaining address details for the final beneficiary entity<br>\n",
        "`SettlementDate` - Date the individual transaction was settled<br>\n",
        "`SettlementCurrency` - Currency used for transaction<br>\n",
        "`SettlementAmount` - Value of the transaction net of fees/transfer charges/forex<br>\n",
        "`InstructedCurrency` - Currency of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`InstructedAmount` - Value of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`Label` - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.<br>\n",
        "<br>\n",
        "**Dataset 2 – Banks:**\n",
        "\n",
        "`Bank` - Identifier for the bank<br>\n",
        "`Account` - Identifier for the account<br>\n",
        "`Name` - Name of the account<br>\n",
        "`Street` - Street address associated with the account<br>\n",
        "`CountryCityZip` - Remaining address details associated with the account<br>\n",
        "`Flags` - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are below:<br>\n",
        "00 - No flags<br>\n",
        "01 - Account closed<br>\n",
        "03 - Account recently opened<br>\n",
        "04 - Name mismatch<br>\n",
        "05 - Account under monitoring<br>\n",
        "06 - Account suspended<br>\n",
        "07 - Account frozen<br>\n",
        "08 - Non-transaction account<br>\n",
        "09 - Beneficiary deceased<br>\n",
        "10 - Invalid company ID<br>\n",
        "11 - Invalid individual ID<br>"
      ],
      "id": "8590747c-e25c-405c-815b-3c83f10309ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "### Read in Data"
      ],
      "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dd295db2-0718-42c6-8a70-df41e93e72fa"
      },
      "outputs": [],
      "source": [
        "# Read in transactions training and testing data csv files to Spark DataFrames - Colab\n",
        "train_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "test_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in transactions training and testing data csv files to Spark DataFrames - Jupyter\n",
        "# train_df = spark.read.csv('data/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "# test_df = spark.read.csv('data/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in banks data csv file to a Spark DataFrame\n",
        "# banks_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/bank_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Persist in memory\n",
        "# train_df = train_df.cache()\n",
        "# test_df = test_df.cache()\n",
        "# banks_df = banks_df.cache()"
      ],
      "id": "dd295db2-0718-42c6-8a70-df41e93e72fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
      },
      "source": [
        "### Initial EDA"
      ],
      "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
      },
      "outputs": [],
      "source": [
        "# Print shape of dataframes\n",
        "print(f\"train_df:  {train_df.count()} Rows, {len(train_df.columns)} Columns\")\n",
        "print(f\"test_df:  {test_df.count()} Rows, {len(test_df.columns)} Columns\")\n",
        "# print(f\"banks_df:  {banks_df.count():,} Rows, {len(banks_df.columns)} Columns\")"
      ],
      "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
      },
      "outputs": [],
      "source": [
        "# Print schema of dataframe\n",
        "train_df.printSchema()"
      ],
      "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
      },
      "outputs": [],
      "source": [
        "# banks_df.printSchema()"
      ],
      "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
      },
      "outputs": [],
      "source": [
        "# Display first row of train_df\n",
        "train_df.show(n=1, vertical=True, truncate=False)"
      ],
      "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44472749-3189-4abf-b646-e0767399adbd"
      },
      "outputs": [],
      "source": [
        "# Display first 5 rows of banks dataframe\n",
        "# banks_df.show(n=5, truncate=False)"
      ],
      "id": "44472749-3189-4abf-b646-e0767399adbd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
      },
      "outputs": [],
      "source": [
        "# Print number of null/missing values in each column of train_df\n",
        "train_df_null = train_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in train_df.columns if c != 'Timestamp'])\n",
        "\n",
        "print('Number of null/missing values per column:\\n')\n",
        "train_df_null.show(vertical=True, truncate=False)"
      ],
      "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
      },
      "outputs": [],
      "source": [
        "# # Print number of null/missing values in each column of banks_df\n",
        "# banks_df_null = banks_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "#                                  .alias(c) for c in banks_df.columns])\n",
        "\n",
        "# print('Number of null/missing values per column:\\n')\n",
        "# banks_df_null.show(vertical=True, truncate=False)"
      ],
      "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column of train_df; sample 1% of df for efficiency\n",
        "train_df_unique = train_df.sample(False, 0.01).agg(*(F.countDistinct(F.col(c)) for c in train_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column (in sample of 1% of dataframe):\\n\")\n",
        "train_df_unique.show(vertical=True, truncate=False)"
      ],
      "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column in banks_df; sample 10% of df for efficiency\n",
        "# banks_df_unique = banks_df.sample(False, 0.1).agg(*(F.countDistinct(F.col(c)) for c in banks_df.columns))\n",
        "\n",
        "# print(f\"Number of unique values per column (in sample of 10% of dataframe):\\n\")\n",
        "# banks_df_unique.show(vertical=True, truncate=False)"
      ],
      "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DJZvwkuvZbs"
      },
      "outputs": [],
      "source": [
        "# Display Pandas style summary statistics table of numeric columns in train_df\n",
        "num_cols = [item[0] for item in train_df.dtypes if item[1] == 'int' or item[1] == 'double']\n",
        "\n",
        "train_df.select(num_cols).summary().show(truncate=False)\n"
      ],
      "id": "0DJZvwkuvZbs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) in train_df\n",
        "class_counts = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "class_counts.show(truncate=10)"
      ],
      "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like this is an extremely imbalanced dataset - only about 0.1% of the data is in the positive class. We will need to address this class imbalance as part of the modeling process."
      ],
      "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dQ_s4GsrJQV"
      },
      "source": [
        "<br>"
      ],
      "id": "7dQ_s4GsrJQV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kmSqoPErFV1"
      },
      "source": [
        "## Detailed EDA"
      ],
      "id": "0kmSqoPErFV1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTzpjevZ-3P9"
      },
      "outputs": [],
      "source": [
        "# Sample 2% of train_df for visualizations (approximately 94k observations)\n",
        "viz_df = train_df.sample(withReplacement=False, fraction=0.02, seed=42).toPandas()"
      ],
      "id": "tTzpjevZ-3P9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8MmRG_z0UnV"
      },
      "source": [
        "### Visualize target class distributions of sender and receiver banks used in transactions"
      ],
      "id": "V8MmRG_z0UnV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZk2VXXfnCKj"
      },
      "outputs": [],
      "source": [
        "# Display unique senders in training dataset\n",
        "print(f\"train_df, {train_df.select('Sender').distinct().count()} unique senders:\")\n",
        "train_df.select('Sender').distinct().show(5)"
      ],
      "id": "YZk2VXXfnCKj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft4XQOtuqqvG"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 14))\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 0], ax=ax1, palette='muted', \n",
        "              order=viz_df[viz_df.Label == 0]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Set color palette to match values in y-axis above\n",
        "ax2_palette = {'DPSUFRPP': '#4878D0', 'WVOLDEMM': '#EE854A', 'ZOUOGB22': '#6ACC64', 'ABVVUS6S': '#956CB4'}\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 1], ax=ax2, palette=ax2_palette, \n",
        "              order=viz_df[viz_df.Label == 1]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "ax1.set_title('Sender Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Sender Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "id": "Ft4XQOtuqqvG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEq48KnKpYCt"
      },
      "outputs": [],
      "source": [
        "# Display unique receivers in training dataset\n",
        "print(f\"train_df, {train_df.select('Receiver').distinct().count()} unique receivers:\")\n",
        "train_df.select('Receiver').distinct().show(5)"
      ],
      "id": "ZEq48KnKpYCt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABDfjBJ1zOa6"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 14))\n",
        "\n",
        "# Set consistent colors across y-axis values\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {val: color for val, color in zip(viz_df[viz_df.Label == 0]['Receiver'].value_counts().index, palette)}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "ax1_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 0], ax=ax1, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 0]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Update palette_map with values not found above\n",
        "for val, color in zip(viz_df[viz_df.Label == 1]['Receiver'].value_counts().index, palette):\n",
        "    if val not in palette_map:\n",
        "        palette_map[val] = 'silver'  # Assign values not found above to silver\n",
        "\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "ax2_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 1], ax=ax2, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 1]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "ax1.set_title('Receiver Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Receiver Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "id": "ABDfjBJ1zOa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sr4eFI81RLe"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like the choice of sender bank is very informative in terms of determining whether a transaction is anomalous or not, while the choice of receiver bank is not nearly as valuable.\n",
        "- Only 4 out of 16 sender banks tend to be utilized in anomalous transactions, while nearly all are utilized in non-anomalous transactions.\n",
        "- Looking at receiver banks, 12 out of 16 tend to be utilized for both anomalous and non-anomalous transactions, and in roughly equal distributions.\n",
        "- There is no need to choose between sender and receiver banks when selecting our features; we can engineer features in sender-receiver bank combinations."
      ],
      "id": "5sr4eFI81RLe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmD5pLD-0hIe"
      },
      "source": [
        "## Visualize target class distributions of instructed and settlement currencies used in transactions"
      ],
      "id": "lmD5pLD-0hIe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx_M2kFJm9yx"
      },
      "outputs": [],
      "source": [
        "# Display unique instructed currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('InstructedCurrency').distinct().count()} unique instructed currencies:\")\n",
        "train_df.select('InstructedCurrency').distinct().show()"
      ],
      "id": "lx_M2kFJm9yx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzOj50aB_IRR"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 7))\n",
        "\n",
        "# Set consistent colors across x-axis values\n",
        "ax1_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "ax2_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan', \\\n",
        "               'AUD': 'lightseagreen', 'CAD': 'gray', 'NZD': 'plum', 'INR': '#FF9933'}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=ax1_palette)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=ax2_palette)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x,txt_y,txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.25\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x,txt_y,txt)\n",
        "\n",
        "ax1.set_title('Instructed Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Instructed Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "id": "GzOj50aB_IRR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--a2nbKmqtlW"
      },
      "outputs": [],
      "source": [
        "# Display unique settlement currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('SettlementCurrency').distinct().count()} unique settlement currencies:\")\n",
        "train_df.select('SettlementCurrency').distinct().show()"
      ],
      "id": "--a2nbKmqtlW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_4pIbQGN8GL"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 7))\n",
        "\n",
        "# Set consistent color palettes\n",
        "ax1_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "ax2_palette = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan', \\\n",
        "               'AUD': 'lightseagreen', 'CAD': 'gray', 'NZD': 'plum', 'INR': '#FF9933'}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=ax1_palette)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=ax2_palette)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x,txt_y,txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x,txt_y,txt)\n",
        "\n",
        "ax1.set_title('Settlement Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Settlement Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "id": "9_4pIbQGN8GL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO7iezULOz8P"
      },
      "source": [
        "**Remarks:**\n",
        "- Instructed currencies seems to be more informative in terms of being correlated with whether or not a transaction is anomalous.\n",
        "- Among instructed currencies, we see the opposite trend as we saw with chosen banks; anomalous transactions tend to use a broader selection of instructed currencies, rather than a more narrow selection as we saw with chosen sender banks.\n",
        "- Among settlement currencies, we see the same four currencies being utilized among both target classes, but in slightly different frequencies.\n",
        "- We will keep the instructed currencies (and one hot encode them) as a feature in the final dataset and drop the settlement currencies."
      ],
      "id": "FO7iezULOz8P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6CMX7wnyShD"
      },
      "source": [
        "<br>"
      ],
      "id": "e6CMX7wnyShD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
      },
      "source": [
        "# Preprocessing & Feature Engineering"
      ],
      "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
      },
      "source": [
        "Steps:\n",
        "1. Drop duplicate transaction rows\n",
        "2. Train/test split\n",
        "3. Create `SenderHourFreq` feature\n",
        "4. Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features\n",
        "5. Create `SenderReceiverFreq` feature"
      ],
      "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "398a670b-b7e1-43b7-835d-8feac9644414"
      },
      "outputs": [],
      "source": [
        "# # Create temporary tables for join\n",
        "# train_df.createOrReplaceTempView('train_df_sql')\n",
        "# banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "\n",
        "# # SQL to join dataframes\n",
        "# join_sql =  \"\"\"SELECT train_df_sql.*, banks_df_sql.Flags AS OrderingAccFlags\n",
        "#             FROM train_df_sql \n",
        "#             LEFT JOIN banks_df_sql \n",
        "#             ON train_df_sql.OrderingAccount = banks_df_sql.Account\n",
        "#             \"\"\"\n",
        "# # Perform SQL join\n",
        "# joined_df = spark.sql(join_sql)"
      ],
      "id": "398a670b-b7e1-43b7-835d-8feac9644414"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
      },
      "outputs": [],
      "source": [
        "# Print shape of joined dataframe\n",
        "# print(f\"{joined_df.count():,} Rows, {len(joined_df.columns)} Columns\")"
      ],
      "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl7pzyqEDTnX"
      },
      "source": [
        "### Drop intermediary transactions (only keep one row per end-to-end transaction)"
      ],
      "id": "Nl7pzyqEDTnX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYOILJD0ISSL"
      },
      "outputs": [],
      "source": [
        "# Print count of unique transactions (as identified by UETR codes)\n",
        "print('train_df:')\n",
        "train_df.select(F.countDistinct('UETR')).show()\n",
        "print('test_df:')\n",
        "test_df.select(F.countDistinct('UETR')).show()"
      ],
      "id": "LYOILJD0ISSL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qoi-7oTDTCX"
      },
      "outputs": [],
      "source": [
        "# Print total number of combined rows with duplicate UETR values (meaning sender routed transaction through one or more intermediary banks)\n",
        "print('Total number of rows with intermediary transactions in train_df:')\n",
        "train_df.select('UETR').groupBy('UETR')\\\n",
        "    .count()\\\n",
        "    .where(F.col('count') > 1)\\\n",
        "    .select(F.sum('count'))\\\n",
        "    .show()"
      ],
      "id": "-qoi-7oTDTCX"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Pyp2WUWgIRhP"
      },
      "outputs": [],
      "source": [
        "# Drop rows with duplicate UETR codes, keeping the first occurence (sorted by Timestamp)\n",
        "train_df = train_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR'])\n",
        "test_df = test_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR'])\n",
        "\n",
        "# Ensure no duplicates\n",
        "assert train_df.groupBy(train_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "assert test_df.groupBy(test_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "\n",
        "print(f\"train_df: {train_df.count()} rows\")\n",
        "print(f\"test_df: {test_df.count()} rows\")"
      ],
      "id": "Pyp2WUWgIRhP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znGbnVs0RE9Y"
      },
      "outputs": [],
      "source": [
        "# Show value counts for 'Label' column (classification target) in new train and test dataframes\n",
        "class_counts_train = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "class_counts_test = test_df.groupBy('Label').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "\n",
        "print('train_df:')\n",
        "class_counts_train.show(truncate=10)\n",
        "print('test_df:')\n",
        "class_counts_test.show(truncate=10)"
      ],
      "id": "znGbnVs0RE9Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KTP3eqWcAeW"
      },
      "source": [
        "## Feature Engineering"
      ],
      "id": "_KTP3eqWcAeW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyY21cvovTXM"
      },
      "source": [
        "### Create `SenderHourFreq` feature: transaction hour frequency for each sender\n",
        "\n",
        "This feature will tell us the frequency with which each sender initiated transactions for each hour of the day. This should capture the signal of the correlation between the sender and target class as well as the correlation between transaction hour and target class."
      ],
      "id": "NyY21cvovTXM"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4kz9zFgrR5Nj"
      },
      "outputs": [],
      "source": [
        "# Define UDF to extract hour from timestamp\n",
        "hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# Create new column of transaction hours\n",
        "train_df = train_df.withColumn('Hour', hour(train_df.Timestamp))\n",
        "test_df = test_df.withColumn('Hour', hour(test_df.Timestamp))\n",
        "\n",
        "# Create list of unique senders\n",
        "senders = train_df.select('Sender').toPandas()['Sender'].unique()\n",
        "\n",
        "# Create column of senders concatenated with hours\n",
        "train_df = train_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "test_df = test_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "pd_df = train_df.select('Sender', 'Hour').toPandas()\n",
        "\n",
        "# Create dictionary of sender hour frequency values to map from sender hour values\n",
        "sender_hour_frequency = {}\n",
        "for sender in senders:\n",
        "    sender_rows = pd_df[pd_df['Sender'] == sender]\n",
        "    for hour in range(24):\n",
        "        sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_hour_frequency dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])\n",
        "test_df = test_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ],
      "id": "4kz9zFgrR5Nj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKfj2pFGQGOW"
      },
      "source": [
        "### Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features: transaction currency frequency and average transaction amount per currency for each sender\n",
        "\n",
        "These features will tell us the frequency with which each sender initiated transactions for each currency, in the case of the first feature. For the second feature, it will tell us the average amount with which each sender sent each currency. These features may also be correlated with anomalous transactions."
      ],
      "id": "iKfj2pFGQGOW"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZINrnCpHQjkN"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with instructed currencies\n",
        "train_df = train_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "test_df = test_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "\n",
        "pd_train_df = train_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "pd_test_df = test_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "\n",
        "# Create dictionary of sender currency frequency values to map from sender currency values\n",
        "sender_currency_freq = {}\n",
        "# Create dictionary of average sender currency values to map from sender currency values\n",
        "sender_currency_avg = {}\n",
        "\n",
        "for sc in set(\n",
        "    list(pd_train_df['SenderCurrency'].unique()) + list(pd_test_df['SenderCurrency'].unique())\n",
        "):\n",
        "    sender_currency_freq[sc] = len(pd_train_df[pd_train_df['SenderCurrency'] == sc])\n",
        "    sender_currency_avg[sc] = pd_train_df[pd_train_df['SenderCurrency'] == sc][\n",
        "        \"InstructedAmount\"\n",
        "    ].mean()\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_avg dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_avg.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])"
      ],
      "id": "ZINrnCpHQjkN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lRfKQqI51Il"
      },
      "source": [
        "### Create `SenderReceiverFreq` feature: sender-receiver combination frequency for each sender and receiver"
      ],
      "id": "3lRfKQqI51Il"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GNPtx7aS50ic"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with receivers\n",
        "train_df = train_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "test_df = test_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "\n",
        "# Create dictionary of sender receiver frequency values to map from sender receiver values\n",
        "sender_receiver_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('SenderReceiver').toPandas()\n",
        "pd_test_df = test_df.select('SenderReceiver').toPandas()\n",
        "\n",
        "for sr in set(\n",
        "    list(pd_train_df['SenderReceiver'].unique()) + list(pd_test_df['SenderReceiver'].unique())\n",
        "):\n",
        "    sender_receiver_freq[sr] = len(pd_train_df[pd_train_df['SenderReceiver'] == sr])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_receiver_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_receiver_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])\n",
        "test_df = test_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])"
      ],
      "id": "GNPtx7aS50ic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFkdce4XbMAi"
      },
      "source": [
        "### Drop unused categorical columns\n",
        "\n",
        "We're going to drop all categorical columns here, save for the one we are one hot encoding which is `InstructedCurrency`"
      ],
      "id": "gFkdce4XbMAi"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bs_qEOpGR0qK"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Timestamp',\n",
        "    'UETR',\n",
        "    'Sender',\n",
        "    'Receiver',\n",
        "    'TransactionReference',\n",
        "    'OrderingAccount',\n",
        "    'OrderingName',\n",
        "    'OrderingStreet',\n",
        "    'OrderingCountryCityZip',\n",
        "    'BeneficiaryAccount',\n",
        "    'BeneficiaryName',\n",
        "    'BeneficiaryStreet',\n",
        "    'BeneficiaryCountryCityZip',\n",
        "    'SettlementDate',\n",
        "    'SettlementCurrency',\n",
        "    'SenderHour',\n",
        "    'SenderCurrency',\n",
        "    'SenderReceiver'\n",
        "]\n",
        "\n",
        "train_df = train_df.drop(*cols_to_drop)\n",
        "test_df = test_df.drop(*cols_to_drop)"
      ],
      "id": "bs_qEOpGR0qK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzRag26ORz4g"
      },
      "source": [
        "<br>"
      ],
      "id": "rzRag26ORz4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyxiNwEpbYsD"
      },
      "source": [
        "# Resample Training Dataset\n",
        "\n",
        "As we saw above, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll rebalance the dataset through a combination of undersampling the majority class (non-amomalous transactions) and oversampling the minority class (anomalous transactions). We will take a 10% sample of the non-anomalous transactions, without replacement, and a 1,000% sample of anomalous transactions, with replacement. This means that we should have approximately 450k observations after resampling, and the class imbalance will increase to about 90%/10% non-anomalous to anomalous transactions."
      ],
      "id": "iyxiNwEpbYsD"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hJkpUEvM-uc5"
      },
      "outputs": [],
      "source": [
        "resampled_df = spark_resample(train_df, undersample_fraction=0.1, oversample_fraction=10.0, \n",
        "                              class_field='Label', pos_class=1, shuffle=True, random_state=42)"
      ],
      "id": "hJkpUEvM-uc5"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7efDPlelAE7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74641e9-0b8f-4a72-f0dc-d7257b4059a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resampled_df:  454052 Rows, 10 Columns\n"
          ]
        }
      ],
      "source": [
        "# Print shape of resampled dataframe\n",
        "print(f\"resampled_df:  {resampled_df.count()} Rows, {len(resampled_df.columns)} Columns\")"
      ],
      "id": "7efDPlelAE7W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
      },
      "outputs": [],
      "source": [
        "# Preview resampled dataframe\n",
        "resampled_df.show(3, vertical=True)"
      ],
      "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmckUs8hfF5r"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) of resampled dataframe\n",
        "resampled_class_counts = resampled_df.groupBy('Label').count().withColumn('percent', F.col('count')/resampled_df.count())\n",
        "\n",
        "resampled_class_counts.show(truncate=10)"
      ],
      "id": "XmckUs8hfF5r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save resampled training dataframe and preprocessed test dataframe as CSV files"
      ],
      "metadata": {
        "id": "dNen6kOq3P17"
      },
      "id": "dNen6kOq3P17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOsBhtSO6QUw"
      },
      "outputs": [],
      "source": [
        "# resampled_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/resampled_df.csv', header=True)\n",
        "# test_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/test_df_preprocessed.csv', header=True)"
      ],
      "id": "dOsBhtSO6QUw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCztBi7ueB6S"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "HCztBi7ueB6S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbjtN2WLeWli"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "BbjtN2WLeWli"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "zCekZqiBolKn"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "zCekZqiBolKn"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HSts_ccbSots"
      },
      "id": "HSts_ccbSots",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "G2X4KQRGopWq"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "G2X4KQRGopWq"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ylfcNNFiO7c1"
      },
      "id": "ylfcNNFiO7c1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Notebook-1_Intro-EDA-Preprocessing-V3.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python (spark-env)",
      "language": "python",
      "name": "spark-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}