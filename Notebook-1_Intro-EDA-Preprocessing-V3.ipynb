{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kwcbs5bBIV2"
      },
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ],
      "id": "6Kwcbs5bBIV2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
      },
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ],
      "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
      },
      "source": [
        "## Notebook 1: Intro, EDA and Preprocessing"
      ],
      "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
      },
      "source": [
        "### By Ryan Posternak"
      ],
      "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
      },
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ],
      "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
      },
      "source": [
        "## Goal: \n",
        "\n",
        "*This is a project for learning purposes. The *** is not involved with this project in any way.*\n",
        "\n",
        "<br>"
      ],
      "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
      },
      "source": [
        "# Overview and Business Understanding"
      ],
      "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
      },
      "source": [
        "<br>"
      ],
      "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
      },
      "source": [
        "# Data Understanding"
      ],
      "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
      },
      "source": [
        "<br>"
      ],
      "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
      },
      "source": [
        "# Imports, Reading in Data, and Exploratory Data Analysis"
      ],
      "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
      },
      "source": [
        "### Google colab compatibility downloads"
      ],
      "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "!pip install pyspark==3.3.0\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_EJVZLDUCRdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e75afd-e575-42ec-d7b2-1d6ed9b41ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "id": "_EJVZLDUCRdy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
      },
      "source": [
        "### Import libraries, packages and modules"
      ],
      "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "from itertools import chain\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, TimestampType\n",
        "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "# from pyspark.ml import Pipeline\n",
        "# from pyspark.ml.classification import LogisticRegression\n",
        "# from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "# from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ],
      "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc_V7XVRDx1U"
      },
      "outputs": [],
      "source": [
        "helper_functions = files.upload()\n",
        "from helper_functions import spark_resample"
      ],
      "id": "vc_V7XVRDx1U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
      },
      "outputs": [],
      "source": [
        "# Check colab GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ],
      "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "8RdtiSPt22Qe"
      },
      "id": "8RdtiSPt22Qe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
      },
      "outputs": [],
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "      pre {\n",
        "          white-space: pre-wrap;\n",
        "      }\n",
        "    </style>\n",
        "    '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "51119e6d-025e-47e2-8eac-27ee42535316"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Initialize Spark Session - Jupyter (local)\n",
        "# spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# Initialize Spark Session - Colab\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ],
      "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
      },
      "source": [
        "### Description of Features"
      ],
      "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8590747c-e25c-405c-815b-3c83f10309ea"
      },
      "source": [
        "**Dataset 1 – Transactions:**\n",
        "\n",
        "`MessageId` - Globally unique identifier within this dataset for individual transactions<br>\n",
        "`UETR` - The Unique End-to-end Transaction Reference—a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction<br>\n",
        "`TransactionReference` - Unique identifier for an individual transaction<br>\n",
        "`Timestamp` - Time at which the individual transaction was initiated<br>\n",
        "`Sender` - Institution (bank) initiating/sending the individual transaction<br>\n",
        "`Receiver` - Institution (bank) receiving the individual transaction<br>\n",
        "`OrderingAccount` - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction<br>\n",
        "`OrderingName` - Name for the originating ordering entity<br>\n",
        "`OrderingStreet` - Street address for the originating ordering entity<br>\n",
        "`OrderingCountryCityZip` - Remaining address details for the originating ordering entity<br>\n",
        "`BeneficiaryAccount` - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction<br>\n",
        "`BeneficiaryName` - Name for the final beneficiary entity<br>\n",
        "`BeneficiaryStreet` - Street address for the final beneficiary entity<br>\n",
        "`BeneficiaryCountryCityZip` - Remaining address details for the final beneficiary entity<br>\n",
        "`SettlementDate` - Date the individual transaction was settled<br>\n",
        "`SettlementCurrency` - Currency used for transaction<br>\n",
        "`SettlementAmount` - Value of the transaction net of fees/transfer charges/forex<br>\n",
        "`InstructedCurrency` - Currency of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`InstructedAmount` - Value of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`Label` - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.<br>\n",
        "<br>\n",
        "**Dataset 2 – Banks:**\n",
        "\n",
        "`Bank` - Identifier for the bank<br>\n",
        "`Account` - Identifier for the account<br>\n",
        "`Name` - Name of the account<br>\n",
        "`Street` - Street address associated with the account<br>\n",
        "`CountryCityZip` - Remaining address details associated with the account<br>\n",
        "`Flags` - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are below:<br>\n",
        "00 - No flags<br>\n",
        "01 - Account closed<br>\n",
        "03 - Account recently opened<br>\n",
        "04 - Name mismatch<br>\n",
        "05 - Account under monitoring<br>\n",
        "06 - Account suspended<br>\n",
        "07 - Account frozen<br>\n",
        "08 - Non-transaction account<br>\n",
        "09 - Beneficiary deceased<br>\n",
        "10 - Invalid company ID<br>\n",
        "11 - Invalid individual ID<br>\n",
        "<br>\n",
        "Additional information from data providers:<br>\n",
        "\"Because each end-to-end transaction is defined by one originating orderer and one final beneficiary, the `OrderingAccount` and `BeneficiaryAccount` in a given row may not necessarily belong to the bank in that row's `Sender` and the bank in that row's `Receiver`, respectively. The correct way to associate an `OrderingAccount` to the correct bank is to identify the `Sender` bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a `BeneficiaryAccount` to the correct bank is to identify the `Receiver` bank in the final (last) individual transaction in that end-to-end transaction.\""
      ],
      "id": "8590747c-e25c-405c-815b-3c83f10309ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "### Read in Data"
      ],
      "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dd295db2-0718-42c6-8a70-df41e93e72fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "69b9ff45-b040-4f01-a149-dc3bc70faa37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Read in transactions training and testing data csv files to Spark DataFrames - Jupyter (local)\n",
        "# train_df = spark.read.csv('data/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "# test_df = spark.read.csv('data/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in transactions training and testing data csv files to Spark DataFrames - Colab\n",
        "train_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "test_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in banks data csv file to a Spark DataFrame\n",
        "banks_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/bank_dataset.csv', header=True, inferSchema=True)"
      ],
      "id": "dd295db2-0718-42c6-8a70-df41e93e72fa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "u9ePgp3IRE2i"
      },
      "id": "u9ePgp3IRE2i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
      },
      "source": [
        "## Preliminary EDA"
      ],
      "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
      },
      "outputs": [],
      "source": [
        "# Print shape of dataframes\n",
        "print(f\"train_df:  {train_df.count():,} Rows, {len(train_df.columns)} Columns\")\n",
        "print(f\"test_df:  {test_df.count():,} Rows, {len(test_df.columns)} Columns\")\n",
        "print(f\"banks_df:  {banks_df.count():,} Rows, {len(banks_df.columns)} Columns\")"
      ],
      "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
      },
      "outputs": [],
      "source": [
        "# Print schema of train_df dataframe\n",
        "train_df.printSchema()"
      ],
      "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
      },
      "outputs": [],
      "source": [
        "# Print schema of banks_df dataframe\n",
        "banks_df.printSchema()"
      ],
      "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
      },
      "outputs": [],
      "source": [
        "# Display first row of train_df\n",
        "train_df.show(n=1, vertical=True, truncate=False)"
      ],
      "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44472749-3189-4abf-b646-e0767399adbd"
      },
      "outputs": [],
      "source": [
        "# Display first 5 rows of banks dataframe\n",
        "banks_df.show(n=5, truncate=False)"
      ],
      "id": "44472749-3189-4abf-b646-e0767399adbd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
      },
      "outputs": [],
      "source": [
        "# Print number of null/missing values in each column of train_df\n",
        "train_df_null = train_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in train_df.columns if c != 'Timestamp'])\n",
        "\n",
        "print('Number of null/missing values per column:\\n')\n",
        "train_df_null.show(vertical=True, truncate=False)"
      ],
      "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
      },
      "outputs": [],
      "source": [
        "# # Print number of null/missing values in each column of banks_df\n",
        "banks_df_null = banks_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in banks_df.columns])\n",
        "\n",
        "print('Number of null/missing values per column:\\n')\n",
        "banks_df_null.show(vertical=True, truncate=False)"
      ],
      "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column of train_df; sample 1% of df for efficiency\n",
        "train_df_unique = train_df.sample(False, 0.01).agg(*(F.countDistinct(F.col(c)) for c in train_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column (in sample of 1% of dataframe):\\n\")\n",
        "train_df_unique.show(vertical=True, truncate=False)"
      ],
      "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column of banks_df\n",
        "banks_df_unique = banks_df.agg(*(F.countDistinct(F.col(c)) for c in banks_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column:\\n\")\n",
        "banks_df_unique.show(vertical=True, truncate=False)"
      ],
      "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DJZvwkuvZbs"
      },
      "outputs": [],
      "source": [
        "# Display Pandas style summary statistics table of numeric columns in train_df\n",
        "num_cols = [item[0] for item in train_df.dtypes if item[1] == 'int' or item[1] == 'double']\n",
        "\n",
        "train_df.select(num_cols).summary().show(truncate=False)"
      ],
      "id": "0DJZvwkuvZbs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display value counts for 'Flags' feature of banks_df\n",
        "flag_counts = banks_df.groupBy('Flags').count()\n",
        "\n",
        "# Create list of accounts with non-zero flags in banks_df\n",
        "non_zero_accounts = []\n",
        "for row in banks_df.filter(banks_df.Flags != 0).collect()[:]:\n",
        "    non_zero_accounts.append(row['Account'])\n",
        "\n",
        "# Count occurences of ordering accounts associated with non-zero flags in training data\n",
        "transactions_ordering_flagged_train = train_df.where(F.col('OrderingAccount').isin(non_zero_accounts)).count()\n",
        "# Count occurences of beneficiary accounts associated with non-zero flags in training data\n",
        "transactions_beneficiary_flagged_train = train_df.where(F.col('BeneficiaryAccount').isin(non_zero_accounts)).count()\n",
        "# Count occurences of ordering accounts associated with non-zero flags in testing data\n",
        "transactions_ordering_flagged_test = test_df.where(F.col('OrderingAccount').isin(non_zero_accounts)).count()\n",
        "# Count occurences of beneficiary accounts associated with non-zero flags in testing data\n",
        "transactions_beneficiary_flagged_test = test_df.where(F.col('BeneficiaryAccount').isin(non_zero_accounts)).count()\n",
        "\n",
        "print(f\"Accounts with non-zero flags: {len(non_zero_accounts)}\")\n",
        "print(f\"Transactions with non-zero flag associated with OrderingAccount (train): {transactions_ordering_flagged_train}\")\n",
        "print(f\"Transactions with non-zero flag associated with BeneficiaryAccount (train): {transactions_beneficiary_flagged_train}\\n\")\n",
        "print(f\"Transactions with non-zero flag associated with OrderingAccount (test): {transactions_ordering_flagged_test}\")\n",
        "print(f\"Transactions with non-zero flag associated with BeneficiaryAccount (test): {transactions_beneficiary_flagged_test}\\n\")\n",
        "print('Value counts of Flags feature of banks_df:\\n')\n",
        "flag_counts.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "Yh4iw0RmnPZC",
        "outputId": "b84f1ef8-474e-4242-dbb2-bfaab2ab499e"
      },
      "id": "Yh4iw0RmnPZC",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accounts with non-zero flags: 578\n",
            "Transactions with non-zero flag associated with OrderingAccount (train): 0\n",
            "Transactions with non-zero flag associated with BeneficiaryAccount (train): 800\n",
            "\n",
            "Transactions with non-zero flag associated with OrderingAccount (test): 0\n",
            "Transactions with non-zero flag associated with BeneficiaryAccount (test): 107\n",
            "\n",
            "Value counts of Flags feature of banks_df:\n",
            "\n",
            "+-----+------+\n",
            "|Flags|count |\n",
            "+-----+------+\n",
            "|0    |526925|\n",
            "|1    |54    |\n",
            "|6    |50    |\n",
            "|3    |61    |\n",
            "|5    |54    |\n",
            "|9    |51    |\n",
            "|4    |62    |\n",
            "|8    |58    |\n",
            "|7    |64    |\n",
            "|10   |61    |\n",
            "|11   |63    |\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) in train_df\n",
        "class_counts = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "class_counts.show(truncate=10)"
      ],
      "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like this is an extremely imbalanced dataset - only about 0.1% of the data is in the positive class. We will need to address this class imbalance as part of the modeling process."
      ],
      "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dQ_s4GsrJQV"
      },
      "source": [
        "<br>"
      ],
      "id": "7dQ_s4GsrJQV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kmSqoPErFV1"
      },
      "source": [
        "## Detailed EDA"
      ],
      "id": "0kmSqoPErFV1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTzpjevZ-3P9"
      },
      "outputs": [],
      "source": [
        "# Sample 2% of train_df for visualizations (approximately 94k observations)\n",
        "viz_df = train_df.sample(withReplacement=False, fraction=0.02, seed=42).toPandas()"
      ],
      "id": "tTzpjevZ-3P9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8MmRG_z0UnV"
      },
      "source": [
        "### Visualize target class distributions of sender and receiver banks used in transactions"
      ],
      "id": "V8MmRG_z0UnV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZk2VXXfnCKj"
      },
      "outputs": [],
      "source": [
        "# Display unique senders in training dataset\n",
        "print(f\"train_df, {train_df.select('Sender').distinct().count()} unique senders:\")\n",
        "train_df.select('Sender').distinct().show(5)"
      ],
      "id": "YZk2VXXfnCKj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique senders in training dataset where transaction is anomalous\n",
        "unique_anom_senders_count = train_df.filter(train_df.Label == 1)\\\n",
        "                                    .select('Sender').distinct().count()\n",
        "\n",
        "print(f\"train_df, {unique_anom_senders_count} unique senders among anomalous transactions:\")\n",
        "train_df.filter(train_df.Label == 1).select('Sender').distinct().show(5)"
      ],
      "metadata": {
        "id": "31jOa17PTIHV"
      },
      "id": "31jOa17PTIHV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft4XQOtuqqvG"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Set consistent color palette where y-axis values in both axes, otherwise silver\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {}\n",
        "for val, color in zip(viz_df['Sender'].value_counts().index, palette):\n",
        "    if val in viz_df[viz_df.Label == 0]['Sender'].value_counts().index \\\n",
        "    and val in viz_df[viz_df.Label == 1]['Sender'].value_counts().index:\n",
        "        palette_map[val] = color\n",
        "    else:\n",
        "        palette_map[val]= 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 0], ax=ax1, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 0]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 1], ax=ax2, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 1]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Sender Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Sender Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "id": "Ft4XQOtuqqvG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEq48KnKpYCt"
      },
      "outputs": [],
      "source": [
        "# Display unique receivers in training dataset\n",
        "print(f\"train_df, {train_df.select('Receiver').distinct().count()} unique receivers:\")\n",
        "train_df.select('Receiver').distinct().show(5)"
      ],
      "id": "ZEq48KnKpYCt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique receivers in training dataset where transaction is anomalous\n",
        "unique_anom_receivers_count = train_df.filter(train_df.Label == 1)\\\n",
        "                                      .select('Receiver').distinct().count()\n",
        "\n",
        "print(f\"train_df, {unique_anom_receivers_count} unique receivers among anomalous transactions:\")\n",
        "train_df.filter(train_df.Label == 1).select('Receiver').distinct().show(5)"
      ],
      "metadata": {
        "id": "IeVQ6hS2UWLx"
      },
      "id": "IeVQ6hS2UWLx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABDfjBJ1zOa6"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Set consistent color palette where y-axis values in both axes, otherwise silver\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {}\n",
        "for val, color in zip(viz_df['Receiver'].value_counts().index, palette):\n",
        "    if val in viz_df[viz_df.Label == 0]['Receiver'].value_counts().index \\\n",
        "    and val in viz_df[viz_df.Label == 1]['Receiver'].value_counts().index:\n",
        "        palette_map[val] = color\n",
        "    else:\n",
        "        palette_map[val]= 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "ax1_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 0], ax=ax1, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 0]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Update palette_map with values not found above\n",
        "for val, color in zip(viz_df[viz_df.Label == 1]['Receiver'].value_counts().index, palette):\n",
        "    if val not in palette_map:\n",
        "        palette_map[val] = 'silver'  # Assign values not found above to silver\n",
        "\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "ax2_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 1], ax=ax2, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 1]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Receiver Banks of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Count', fontsize=14)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=14)\n",
        "ax2.set_title('Receiver Banks of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Count', fontsize=14)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=14);"
      ],
      "id": "ABDfjBJ1zOa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sr4eFI81RLe"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like the choice of sender bank is very informative in terms of determining whether a transaction is anomalous or not, while the choice of receiver bank is not nearly as valuable.\n",
        "- Only 4 out of 16 sender banks tend to be utilized in anomalous transactions, while nearly all are utilized in non-anomalous transactions.\n",
        "- Looking at receiver banks, 12 out of 16 tend to be utilized for both anomalous and non-anomalous transactions, and in roughly equal distributions.\n",
        "- There is no need to choose between sender and receiver banks when selecting our features; we can engineer features in sender-receiver bank combinations."
      ],
      "id": "5sr4eFI81RLe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmD5pLD-0hIe"
      },
      "source": [
        "## Visualize target class distributions of instructed and settlement currencies used in transactions"
      ],
      "id": "lmD5pLD-0hIe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx_M2kFJm9yx"
      },
      "outputs": [],
      "source": [
        "# Display unique instructed currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('InstructedCurrency').distinct().count()} unique instructed currencies:\")\n",
        "train_df.select('InstructedCurrency').distinct().show()"
      ],
      "id": "lx_M2kFJm9yx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzOj50aB_IRR"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent color palette where x-axis values in both axes, otherwise silver\n",
        "palette_map = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "for val in viz_df[viz_df.Label == 1]['InstructedCurrency'].value_counts().index:\n",
        "    if val not in viz_df[viz_df.Label == 0]['InstructedCurrency'].value_counts().index:\n",
        "        palette_map[val] = 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x, txt_y, txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.25\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x, txt_y, txt)\n",
        "\n",
        "ax1.set_title('Instructed Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Instructed Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Instructed Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "id": "GzOj50aB_IRR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--a2nbKmqtlW"
      },
      "outputs": [],
      "source": [
        "# Display unique settlement currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('SettlementCurrency').distinct().count()} unique settlement currencies:\")\n",
        "train_df.select('SettlementCurrency').distinct().show()"
      ],
      "id": "--a2nbKmqtlW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_4pIbQGN8GL"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent color palette\n",
        "palette_map = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x, txt_y, txt)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x, txt_y, txt)\n",
        "\n",
        "ax1.set_title('Settlement Currencies of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Settlement Currencies of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Settlement Currency', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14);"
      ],
      "id": "9_4pIbQGN8GL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO7iezULOz8P"
      },
      "source": [
        "**Remarks:**\n",
        "- Instructed currencies seems to be more informative in terms of being correlated with whether or not a transaction is anomalous.\n",
        "- Among instructed currencies, we see the opposite trend as we saw with chosen banks; anomalous transactions tend to use a broader selection of instructed currencies, rather than a more narrow selection as we saw with chosen sender banks.\n",
        "- Among settlement currencies, we see the same four currencies being utilized among both target classes, but in slightly different frequencies.\n",
        "- We will keep the instructed currencies (and one hot encode them) as a feature in the final dataset and drop the settlement currencies."
      ],
      "id": "FO7iezULOz8P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize target class distributions of instructed transaction amounts\n",
        "\n",
        "We'll only plot instructed transaction amounts, since settlement amounts are essentially equivalent to instructed amounts, with the difference being deductions for fees and transfer/forex charges"
      ],
      "metadata": {
        "id": "0LS-Q_MKP-Z8"
      },
      "id": "0LS-Q_MKP-Z8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Plot histogram of non-anomalous transactions; log x-axis due to extreme right-skew\n",
        "sns.histplot(x='InstructedAmount', data=viz_df[viz_df.Label == 0], ax=ax1, bins=10, log_scale=True)\n",
        "\n",
        "# Plot histogram of anomalous transactions; log x-axis due to extreme right-skew\n",
        "sns.histplot(x='InstructedAmount', data=viz_df[viz_df.Label == 1], ax=ax2, bins=10, log_scale=True)\n",
        "\n",
        "ax1.set_title('Instructed Transaction Amounts of Non-Anomalous Transactions (Label 0)', fontsize=16)\n",
        "ax1.set_xlabel('Instructed Amount (Log-Scale)', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Instructed Transaction Amounts of Anomalous Transactions (Label 1)', fontsize=16)\n",
        "ax2.set_xlabel('Instructed Amount (Log-Scale)', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14)\n",
        "\n",
        "# Set xticks to match bin widths\n",
        "ax1.set_xticks([bin.get_x() for bin in ax1.patches] + [viz_df[viz_df.Label == 0]['InstructedAmount'].max()])\n",
        "ax2.set_xticks([bin.get_x() for bin in ax2.patches] + [viz_df[viz_df.Label == 1]['InstructedAmount'].max()])\n",
        "# Change xtick labels to more readable format\n",
        "ax1.set_xticklabels(['$100', '$1.5k', '$21k', '$310k', '$4.5M', '$66M', '$960M', '$14B', '$205B', '$3T', '$44T'])\n",
        "ax2.set_xticklabels(['$1.2M', '$2.7M', '$6M', '$14M', '$31M', '$69M', '$154M', '$344M', '$771M', '$1.7B', '$3.9B']);"
      ],
      "metadata": {
        "id": "A_RVSEVPP_C9"
      },
      "id": "A_RVSEVPP_C9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- The distributions of instructed transaction amounts for both non-anomalous and anomalous transactions are both extremely right-skewed, which we can tell because the distributions look more or less normal after log-scaling.\n",
        "- Non-anomalous transactions appear to have a much larger range of values, ranging from \\$100 all the way into the tens of trillions of dollars (which is obviously unrealistic, but this is a synthetic dataset). The range for anomalous transactions is much more narrow, ranging from \\$1.2M to just under $4B."
      ],
      "metadata": {
        "id": "R7hRXCfoP_cL"
      },
      "id": "R7hRXCfoP_cL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "x_TRgs-i5mX4"
      },
      "id": "x_TRgs-i5mX4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
      },
      "source": [
        "# Preprocessing & Feature Engineering"
      ],
      "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
      },
      "source": [
        "Steps:\n",
        "1. Create `OriginalSender` and `FinalReceiver` features\n",
        "2. Create `BeneficiaryAccountFlag` feature by joining `Flags` column of `banks_df`\n",
        "3. Train/test split\n",
        "4. Create `SenderHourFreq` feature\n",
        "5. Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features\n",
        "6. Create `SenderReceiverFreq` feature"
      ],
      "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `OriginalSender` and `FinalReceiver` features\n",
        "\n",
        "As described in the preliminary EDA section, the correct way to associate ordering and beneficiary accounts with their proper banks is to identify the `Sender` bank in the first transaction of the end-to-end transaction and the `Receiver` bank in the last transaction of the end-to-end transaction. Below, we'll create new columns which do just that."
      ],
      "metadata": {
        "id": "1hNM-s9Jt6gJ"
      },
      "id": "1hNM-s9Jt6gJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL operation\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON EarliestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON LatestTransaction.UETR = train_df_sql.UETR\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gRk-mbypt61M",
        "outputId": "4a3dee33-6946-4777-f6bd-e08ae787dde0"
      },
      "id": "gRk-mbypt61M",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three UETR codes of transactions where transactor used intermediary banks\n",
        "random_rows = np.random.choice(train_df.filter(train_df.Receiver != train_df.FinalReceiver).count(), size=3, replace=False)\n",
        "UETRs = [train_df.filter(train_df.Receiver != train_df.FinalReceiver).collect()[row]['UETR'] for row in random_rows]\n",
        "\n",
        "# Display transactions to ensure OriginalSender and FinalReceiver columns are accurate\n",
        "cols_to_show = ['Timestamp', 'UETR', 'Sender', 'Receiver', 'OriginalSender', 'FinalReceiver']\n",
        "train_df.filter(train_df.UETR == UETRs[0]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[1]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[2]).select(cols_to_show).show(truncate=False)"
      ],
      "metadata": {
        "id": "OMFQEpbbvUXF"
      },
      "id": "OMFQEpbbvUXF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat for `test_df`"
      ],
      "metadata": {
        "id": "3tKAQvb1D5fv"
      },
      "id": "3tKAQvb1D5fv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL operation\n",
        "train_df.createOrReplaceTempView('test_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM test_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM test_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM test_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM test_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT test_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM test_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON EarliestTransaction.UETR = test_df_sql.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON LatestTransaction.UETR = test_df_sql.UETR\n",
        "\"\"\"\n",
        "\n",
        "test_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "i8y8H-yIDw9i"
      },
      "id": "i8y8H-yIDw9i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'Sender' and 'Receiver' columns\n",
        "train_df = train_df.drop('Sender', 'Receiver')\n",
        "test_df = test_df.drop('Sender', 'Receiver')"
      ],
      "metadata": {
        "id": "9kad3dp7DFfE"
      },
      "id": "9kad3dp7DFfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `BeneficiaryAccountFlag` feature by joining `Flags` column of `banks_df`\n",
        "\n",
        "As we saw in the preliminary EDA section, none of the accounts with non-zero flags are associated with `OrderingAccount`s; they are all associated with `BeneficiaryAccount`s. It therefore wouldn't make any sense to create a column of `OrderingAccontFlag`s as every value in it would be `0`. We'll only make a column of `BeneficiaryAccountFlag`s."
      ],
      "metadata": {
        "id": "wYeuL8b51iGn"
      },
      "id": "wYeuL8b51iGn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL operation\n",
        "banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"WITH AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "AccountsBeneficiary.Account AS MatchingBeneficiaryAccount, \n",
        "AccountsBeneficiary.Flags AS BeneficiaryAccountFlag\n",
        "FROM train_df_sql\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON train_df_sql.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fLOzGMds5K77",
        "outputId": "f472b01c-f69b-48c7-a8f3-cbf253e56a31"
      },
      "id": "fLOzGMds5K77",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three UETR codes of transactions with non-zero flags\n",
        "random_rows = np.random.choice(train_df.filter(train_df.BeneficiaryAccountFlag != 0).count(), size=3, replace=False)\n",
        "UETRs = [train_df.filter(train_df.BeneficiaryAccountFlag != 0).collect()[row]['UETR'] for row in random_rows]\n",
        "\n",
        "# Display transactions to ensure OrderingAccountFlag and BeneficiaryAccountFlag columns are accurate\n",
        "cols_to_show_train = ['Timestamp', 'UETR', 'OrderingAccount', 'BeneficiaryAccount', \\\n",
        "                      'MatchingBeneficiaryAccount', 'BeneficiaryAccountFlag']\n",
        "\n",
        "cols_to_show_bank = ['Account', 'Flags']\n",
        "\n",
        "first_account = train_df.filter(train_df.UETR == UETRs[0]).collect()[0]['BeneficiaryAccount']\n",
        "second_account = train_df.filter(train_df.UETR == UETRs[1]).collect()[0]['BeneficiaryAccount']\n",
        "third_account = train_df.filter(train_df.UETR == UETRs[2]).collect()[0]['BeneficiaryAccount']\n",
        "\n",
        "train_df.filter(train_df.UETR == UETRs[0]).select(cols_to_show).show(vertical=True, truncate=False)\n",
        "banks_df.filter(banks_df.Account == first_account).select(cols_to_show_bank).show(vertical=True, truncate=False)\n",
        "print('\\n')\n",
        "train_df.filter(train_df.UETR == UETRs[1]).select(cols_to_show).show(vertical=True, truncate=False)\n",
        "banks_df.filter(banks_df.Account == second_account).select(cols_to_show_bank).show(vertical=True, truncate=False)\n",
        "print('\\n')\n",
        "train_df.filter(train_df.UETR == UETRs[2]).select(cols_to_show).show(vertical=True, truncate=False)\n",
        "banks_df.filter(banks_df.Account == third_account).select(cols_to_show_bank).show(vertical=True, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "GUyncvPdsHUV",
        "outputId": "bb81c01f-e346-45ae-9d76-721c640a9aed"
      },
      "id": "GUyncvPdsHUV",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0----------------------------------------------------------\n",
            " Timestamp                  | 2022-01-08 04:58:00                  \n",
            " UETR                       | 9618c19c-47b8-4286-9e6b-e5e75233c55d \n",
            " OrderingAccount            | FR49714755422956988680               \n",
            " BeneficiaryAccount         | 6110240642747074XX                   \n",
            " MatchingBeneficiaryAccount | 6110240642747074XX                   \n",
            " BeneficiaryAccountFlag     | 11                                   \n",
            "\n",
            "-RECORD 0---------------------\n",
            " Account | 6110240642747074XX \n",
            " Flags   | 11                 \n",
            "\n",
            "\n",
            "\n",
            "-RECORD 0----------------------------------------------------------\n",
            " Timestamp                  | 2022-01-18 01:23:00                  \n",
            " UETR                       | 8db3d073-f6fc-4308-878d-dc8180b827ab \n",
            " OrderingAccount            | FR55714755422957000503               \n",
            " BeneficiaryAccount         | 6110240642746943XX                   \n",
            " MatchingBeneficiaryAccount | 6110240642746943XX                   \n",
            " BeneficiaryAccountFlag     | 7                                    \n",
            "\n",
            "-RECORD 0---------------------\n",
            " Account | 6110240642746943XX \n",
            " Flags   | 7                  \n",
            "\n",
            "\n",
            "\n",
            "-RECORD 0----------------------------------------------------------\n",
            " Timestamp                  | 2022-01-18 12:21:00                  \n",
            " UETR                       | 45d7a450-6c8a-4ad3-8bef-cc8248b07459 \n",
            " OrderingAccount            | DE57020224692198623195               \n",
            " BeneficiaryAccount         | 3587276970996552XX                   \n",
            " MatchingBeneficiaryAccount | 3587276970996552XX                   \n",
            " BeneficiaryAccountFlag     | 3                                    \n",
            "\n",
            "-RECORD 0---------------------\n",
            " Account | 3587276970996552XX \n",
            " Flags   | 3                  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat for `test_df`"
      ],
      "metadata": {
        "id": "zGnkOptssRJE"
      },
      "id": "zGnkOptssRJE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features using SQL\n",
        "join_sql = \"\"\"WITH AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        ")\n",
        "SELECT test_df_sql.*, \n",
        "AccountsBeneficiary.Account AS MatchingBeneficiaryAccount, \n",
        "AccountsBeneficiary.Flags AS BeneficiaryAccountFlag\n",
        "FROM test_df_sql\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON test_df_sql.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "v39V4pGZsmxS"
      },
      "id": "v39V4pGZsmxS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tvYS6C4G2EN0"
      },
      "id": "tvYS6C4G2EN0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'MatchingBeneficiaryAccount' columns\n",
        "train_df = train_df.drop('MatchingBeneficiaryAccount')\n",
        "test_df = test_df.drop('MatchingBeneficiaryAccount')"
      ],
      "metadata": {
        "id": "crF21MQbsHlE"
      },
      "id": "crF21MQbsHlE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.show(5, vertical=True, truncate=False)"
      ],
      "metadata": {
        "id": "EdFUW9BWq12P"
      },
      "id": "EdFUW9BWq12P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hdRqT-MA5LNv"
      },
      "id": "hdRqT-MA5LNv",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fe0S3iD95LZh"
      },
      "id": "Fe0S3iD95LZh",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6CMX7wnyShD"
      },
      "source": [
        "<br>"
      ],
      "id": "e6CMX7wnyShD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "398a670b-b7e1-43b7-835d-8feac9644414"
      },
      "outputs": [],
      "source": [
        "# # Create temporary tables for join\n",
        "train_df.sample(False, 0.1).createOrReplaceTempView('train_df_sql')\n",
        "banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "\n",
        "# # SQL to join dataframes\n",
        "# join_sql =  \"\"\"\n",
        "#     SELECT min(Timestamp) AS EarliestTransaction, \n",
        "#     max(Timestamp) AS LatestTransaction\n",
        "#     FROM train_df_sql\n",
        "#     Group by UETR\n",
        "# \"\"\"\n",
        "join_sql =  \"\"\"WITH OrderingAccounts AS (\n",
        "    SELECT OrderingAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "BeneficiaryAccounts AS (\n",
        "    SELECT BeneficiaryAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "AccountsOrdering AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "TransactionTimes AS (\n",
        "    SELECT min(Timestamp) AS EarliestTransaction, \n",
        "    max(Timestamp) AS LatestTransaction\n",
        "    FROM train_df_sql\n",
        "    Group by UETR\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "AccountsOrdering.Account AS MatchingOrderingAccount, \n",
        "AccountsOrdering.Flags AS OrderingAccountFlag, \n",
        "AccountsBeneficiary.Account AS MatchingBeneficiaryAccount, \n",
        "AccountsBeneficiary.Flags AS BeneficiaryAccountFlag\n",
        "FROM train_df_sql\n",
        "LEFT JOIN (\n",
        "    SELECT Account, max()\n",
        ")\n",
        "\n",
        "AccountsOrdering\n",
        "    ON train_df_sql.OrderingAccount = AccountsOrdering.Account\n",
        "\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON train_df_sql.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\n",
        "\n",
        "#             \"\"\"\n",
        "# # Perform SQL join\n",
        "joined_df = spark.sql(join_sql)"
      ],
      "id": "398a670b-b7e1-43b7-835d-8feac9644414"
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql =  \"\"\"\n",
        "WITH TransactionTimes AS (\n",
        "    SELECT UETR, \n",
        "    min(Timestamp) AS EarliestTransaction, \n",
        "    max(Timestamp) AS LatestTransaction\n",
        "    FROM train_df_sql\n",
        "    Group by UETR\n",
        "), \n",
        "OrderingAccounts AS (\n",
        "    SELECT OrderingAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "BeneficiaryAccounts AS (\n",
        "    SELECT BeneficiaryAccount\n",
        "    FROM train_df_sql\n",
        "), \n",
        "AccountsOrdering AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "AccountsOrdering.Account AS MatchingOrderingAccount, \n",
        "AccountsOrdering.Flags AS OrderingAccountFlag, \n",
        "AccountsBeneficiary.Account AS MatchingBeneficiaryAccount, \n",
        "AccountsBeneficiary.Flags AS BeneficiaryAccountFlag\n",
        "FROM train_df_sql\n",
        "LEFT JOIN AccountsOrdering\n",
        "    ON train_df_sql.OrderingAccount = AccountsOrdering.Account\n",
        "        WHERE train_df_sql\n",
        "\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON train_df_sql.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lT9c2wEFDbGF"
      },
      "id": "lT9c2wEFDbGF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql = \"\"\"\n",
        "SELECT UETR, \n",
        "min(Timestamp) AS EarliestTransaction, \n",
        "max(Timestamp) AS LatestTransaction\n",
        "FROM train_df_sql\n",
        "GROUP BY UETR\n",
        "\"\"\"\n",
        "joined_df1 = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "1D52j-4PFbpY"
      },
      "id": "1D52j-4PFbpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH EarliestTransaction AS (\n",
        "join_sql = \"\"\"\n",
        "\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MIN(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "joined_df2 = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "c5HCv6VHHp-v"
      },
      "id": "c5HCv6VHHp-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH LatestTransaction AS (\n",
        "join_sql = \"\"\"\n",
        "\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MAX(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "joined_df3 = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "ygbgInAVK9iT"
      },
      "id": "ygbgInAVK9iT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON EarliestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON LatestTransaction.UETR = train_df_sql.UETR\n",
        "\"\"\"\n",
        "\n",
        "joined_df_SR = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "gdG9YA-5sNCx"
      },
      "id": "gdG9YA-5sNCx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MIN(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "  (SELECT UETR, MAX(Timestamp) \n",
        "  FROM train_df_sql \n",
        "  GROUP BY UETR)\n",
        "), \n",
        "AccountsOrdering AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        "), \n",
        "AccountsBeneficiary AS (\n",
        "    SELECT Account, Flags\n",
        "    FROM banks_df_sql\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "AccountsOrdering.Account AS MatchingOrderingAccount, \n",
        "LatestTransaction.Receiver AS FinalReceiver, \n",
        "AccountsBeneficiary.Account AS MatchingReceivingAccount\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON EarliestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON LatestTransaction.UETR = train_df_sql.UETR\n",
        "LEFT JOIN AccountsOrdering\n",
        "    ON EarliestTransaction.OrderingAccount = AccountsOrdering.Account\n",
        "LEFT JOIN AccountsBeneficiary\n",
        "    ON LatestTransaction.BeneficiaryAccount = AccountsBeneficiary.Account\n",
        "\"\"\"\n",
        "\n",
        "joined_dfx = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "y8np07NvM2pC"
      },
      "id": "y8np07NvM2pC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bzqnbCzYmHVY"
      },
      "id": "bzqnbCzYmHVY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OOAxZei1Lj7O"
      },
      "id": "OOAxZei1Lj7O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M1YZzFrmmStF"
      },
      "id": "M1YZzFrmmStF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YmlMsICNmVae"
      },
      "id": "YmlMsICNmVae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vwiQctBTQrnd"
      },
      "id": "vwiQctBTQrnd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jXKDKx-Psxfi"
      },
      "id": "jXKDKx-Psxfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z3_FKmw6sxv5"
      },
      "id": "Z3_FKmw6sxv5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl7pzyqEDTnX"
      },
      "source": [
        "### Drop intermediary transactions (only keep one row per end-to-end transaction)"
      ],
      "id": "Nl7pzyqEDTnX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYOILJD0ISSL"
      },
      "outputs": [],
      "source": [
        "# Print count of unique transactions (as identified by UETR codes)\n",
        "print('train_df:')\n",
        "train_df.select(F.countDistinct('UETR')).show()\n",
        "print('test_df:')\n",
        "test_df.select(F.countDistinct('UETR')).show()"
      ],
      "id": "LYOILJD0ISSL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qoi-7oTDTCX"
      },
      "outputs": [],
      "source": [
        "# Print total number of combined rows with duplicate UETR values (meaning sender routed transaction through one or more intermediary banks)\n",
        "print('Total number of rows with intermediary transactions in train_df:')\n",
        "train_df.select('UETR').groupBy('UETR')\\\n",
        "    .count()\\\n",
        "    .where(F.col('count') > 1)\\\n",
        "    .select(F.sum('count'))\\\n",
        "    .show()"
      ],
      "id": "-qoi-7oTDTCX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyp2WUWgIRhP"
      },
      "outputs": [],
      "source": [
        "# Drop rows with duplicate UETR codes, keeping the first occurence (sorted by Timestamp)\n",
        "train_df = train_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR'])\n",
        "test_df = test_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR'])\n",
        "\n",
        "# Ensure no duplicates\n",
        "assert train_df.groupBy(train_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "assert test_df.groupBy(test_df.UETR).count().where(F.col('count') > 1).count() == 0\n",
        "\n",
        "print(f\"train_df: {train_df.count()} rows\")\n",
        "print(f\"test_df: {test_df.count()} rows\")"
      ],
      "id": "Pyp2WUWgIRhP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znGbnVs0RE9Y"
      },
      "outputs": [],
      "source": [
        "# Show value counts for 'Label' column (classification target) in new train and test dataframes\n",
        "class_counts_train = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "class_counts_test = test_df.groupBy('Label').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "\n",
        "print('train_df:')\n",
        "class_counts_train.show(truncate=10)\n",
        "print('test_df:')\n",
        "class_counts_test.show(truncate=10)"
      ],
      "id": "znGbnVs0RE9Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyY21cvovTXM"
      },
      "source": [
        "### Create `SenderHourFreq` feature: transaction hour frequency for each sender\n",
        "\n",
        "This feature will tell us the frequency with which each sender initiated transactions for each hour of the day. This should capture the signal of the correlation between the sender and target class as well as the correlation between transaction hour and target class."
      ],
      "id": "NyY21cvovTXM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kz9zFgrR5Nj"
      },
      "outputs": [],
      "source": [
        "# Define UDF to extract hour from timestamp\n",
        "hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# Create new column of transaction hours\n",
        "train_df = train_df.withColumn('Hour', hour(train_df.Timestamp))\n",
        "test_df = test_df.withColumn('Hour', hour(test_df.Timestamp))\n",
        "\n",
        "# Create list of unique senders\n",
        "senders = train_df.select('Sender').toPandas()['Sender'].unique()\n",
        "\n",
        "# Create column of senders concatenated with hours\n",
        "train_df = train_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "test_df = test_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "pd_df = train_df.select('Sender', 'Hour').toPandas()\n",
        "\n",
        "# Create dictionary of sender hour frequency values to map from sender hour values\n",
        "sender_hour_frequency = {}\n",
        "for sender in senders:\n",
        "    sender_rows = pd_df[pd_df['Sender'] == sender]\n",
        "    for hour in range(24):\n",
        "        sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_hour_frequency dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])\n",
        "test_df = test_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ],
      "id": "4kz9zFgrR5Nj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKfj2pFGQGOW"
      },
      "source": [
        "### Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features: transaction currency frequency and average transaction amount per currency for each sender\n",
        "\n",
        "These features will tell us the frequency with which each sender initiated transactions for each currency, in the case of the first feature. For the second feature, it will tell us the average amount with which each sender sent each currency. These features may also be correlated with anomalous transactions."
      ],
      "id": "iKfj2pFGQGOW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZINrnCpHQjkN"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with instructed currencies\n",
        "train_df = train_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "test_df = test_df.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "\n",
        "pd_train_df = train_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "pd_test_df = test_df.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "\n",
        "# Create dictionary of sender currency frequency values to map from sender currency values\n",
        "sender_currency_freq = {}\n",
        "# Create dictionary of average sender currency values to map from sender currency values\n",
        "sender_currency_avg = {}\n",
        "\n",
        "for sc in set(\n",
        "    list(pd_train_df['SenderCurrency'].unique()) + list(pd_test_df['SenderCurrency'].unique())\n",
        "):\n",
        "    sender_currency_freq[sc] = len(pd_train_df[pd_train_df['SenderCurrency'] == sc])\n",
        "    sender_currency_avg[sc] = pd_train_df[pd_train_df['SenderCurrency'] == sc][\n",
        "        \"InstructedAmount\"\n",
        "    ].mean()\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_avg dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_avg.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])"
      ],
      "id": "ZINrnCpHQjkN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lRfKQqI51Il"
      },
      "source": [
        "### Create `SenderReceiverFreq` feature: sender-receiver combination frequency for each sender and receiver"
      ],
      "id": "3lRfKQqI51Il"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNPtx7aS50ic"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with receivers\n",
        "train_df = train_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "test_df = test_df.withColumn('SenderReceiver', F.concat(F.col('Sender'), F.col('Receiver')))\n",
        "\n",
        "# Create dictionary of sender receiver frequency values to map from sender receiver values\n",
        "sender_receiver_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('SenderReceiver').toPandas()\n",
        "pd_test_df = test_df.select('SenderReceiver').toPandas()\n",
        "\n",
        "for sr in set(\n",
        "    list(pd_train_df['SenderReceiver'].unique()) + list(pd_test_df['SenderReceiver'].unique())\n",
        "):\n",
        "    sender_receiver_freq[sr] = len(pd_train_df[pd_train_df['SenderReceiver'] == sr])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_receiver_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_receiver_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])\n",
        "test_df = test_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])"
      ],
      "id": "GNPtx7aS50ic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFkdce4XbMAi"
      },
      "source": [
        "### Drop unused categorical columns\n",
        "\n",
        "We're going to drop all categorical columns here, save for the ones which are one hot encoding which are  . We'll also drop `SettlementAmount`, since it is essentially equivalent to `InstructedAmount` less deductions for fees and transfer/forex charges."
      ],
      "id": "gFkdce4XbMAi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs_qEOpGR0qK"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Timestamp',\n",
        "    'UETR',\n",
        "    'Sender',\n",
        "    'Receiver',\n",
        "    'TransactionReference',\n",
        "    'OrderingAccount',\n",
        "    'OrderingName',\n",
        "    'OrderingStreet',\n",
        "    'OrderingCountryCityZip',\n",
        "    'BeneficiaryAccount',\n",
        "    'BeneficiaryName',\n",
        "    'BeneficiaryStreet',\n",
        "    'BeneficiaryCountryCityZip',\n",
        "    'SettlementDate',\n",
        "    'SettlementCurrency',\n",
        "    'SettlementAmount'\n",
        "    'SenderHour',\n",
        "    'SenderCurrency',\n",
        "    'SenderReceiver'\n",
        "]\n",
        "\n",
        "train_df = train_df.drop(*cols_to_drop)\n",
        "test_df = test_df.drop(*cols_to_drop)"
      ],
      "id": "bs_qEOpGR0qK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzRag26ORz4g"
      },
      "source": [
        "<br>"
      ],
      "id": "rzRag26ORz4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyxiNwEpbYsD"
      },
      "source": [
        "# Resample Training Dataset\n",
        "\n",
        "As we saw above, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll rebalance the dataset through a combination of undersampling the majority class (non-amomalous transactions) and oversampling the minority class (anomalous transactions). Using the function I wrote in helper_functions.py, we'll specify an even positive to negative class balance in the new resampled dataframe, with 500,000 observations (so approximately 250k of each)."
      ],
      "id": "iyxiNwEpbYsD"
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark_resample.__doc__)"
      ],
      "metadata": {
        "id": "Lf4Ww6LtPD7o"
      },
      "id": "Lf4Ww6LtPD7o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJkpUEvM-uc5"
      },
      "outputs": [],
      "source": [
        "# Resample train_df; specify 500,000 observations in new resampled dataframe, with \n",
        "# balanced (0.5) ratio of positive target class (1) to negative target class (0)\n",
        "train_df_resampled = spark_resample(train_df, ratio=0.5, new_count=500000, \n",
        "                                    class_field='Label', pos_class=1, shuffle=True, random_state=42)"
      ],
      "id": "hJkpUEvM-uc5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efDPlelAE7W"
      },
      "outputs": [],
      "source": [
        "# Print shape of resampled dataframe\n",
        "print(f\"train_df_resampled:  {train_df_resampled.count()} Rows, {len(train_df_resampled.columns)} Columns\")"
      ],
      "id": "7efDPlelAE7W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
      },
      "outputs": [],
      "source": [
        "# Preview resampled dataframe\n",
        "train_df_resampled.show(3, vertical=True)"
      ],
      "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmckUs8hfF5r"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) of resampled dataframe\n",
        "resampled_class_counts = train_df_resampled.groupBy('Label')\\\n",
        "                                           .count()\\\n",
        "                                           .withColumn('percent', F.col('count')/train_df_resampled.count())\n",
        "\n",
        "resampled_class_counts.show(truncate=10)"
      ],
      "id": "XmckUs8hfF5r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save resampled training dataframe and preprocessed test dataframe as CSV files"
      ],
      "metadata": {
        "id": "dNen6kOq3P17"
      },
      "id": "dNen6kOq3P17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOsBhtSO6QUw"
      },
      "outputs": [],
      "source": [
        "# train_df_resampled.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/train_df_resampled.csv', header=True)\n",
        "# test_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/test_df_preprocessed.csv', header=True)"
      ],
      "id": "dOsBhtSO6QUw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCztBi7ueB6S"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "HCztBi7ueB6S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbjtN2WLeWli"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "BbjtN2WLeWli"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCekZqiBolKn"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "zCekZqiBolKn"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HSts_ccbSots"
      },
      "id": "HSts_ccbSots",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2X4KQRGopWq"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "G2X4KQRGopWq"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ylfcNNFiO7c1"
      },
      "id": "ylfcNNFiO7c1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Notebook-1_Intro-EDA-Preprocessing-V3.ipynb",
      "provenance": [],
      "background_execution": "on"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python (spark-env)",
      "language": "python",
      "name": "spark-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}