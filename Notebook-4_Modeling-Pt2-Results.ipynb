{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook-4_Modeling-Pt2-Results",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyNpM0zLQ6KzJDvEedN5Fq2m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ],
      "metadata": {
        "id": "_6bs9GYKTazR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ],
      "metadata": {
        "id": "7VB6bZymTwee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook 4: Modeling Part 2, Analysis and Results"
      ],
      "metadata": {
        "id": "nCbcIwMIT8L2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By Ryan Posternak"
      ],
      "metadata": {
        "id": "xIF5Rp36UDq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ],
      "metadata": {
        "id": "02ym0eygUI_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "tITdkCs8UbQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Reading in Data"
      ],
      "metadata": {
        "id": "YVvo7KHTUQbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google colab compatibility downloads"
      ],
      "metadata": {
        "id": "xsGuZAD3URpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "!pip install pyspark==3.3.0\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "5vbzjcsbUSJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "id": "ZL5VI-6OUSmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, \\\n",
        "MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, TrainValidationSplitModel\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ],
      "metadata": {
        "id": "rKWlJ4EbUTF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc_V7XVRDx1U"
      },
      "outputs": [],
      "source": [
        "# Import helper functions\n",
        "helper_functions = files.upload()\n",
        "from helper_functions import set_weight_col, spark_resample, grid_search, score_model, plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Colab RAM info\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "PE0CcaaTrsP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "def set_css():\n",
        "    display(HTML(\"\"\"\n",
        "    <style>\n",
        "      pre {\n",
        "          white-space: pre-wrap;\n",
        "      }\n",
        "    </style>\n",
        "    \"\"\"))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "7Q03mQqlVAa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config(\"spark.ui.port\", \"4050\")\\\n",
        "        .config(\"spark.driver.memory\", \"15g\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "NLJkXpZlVAi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "## Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in weighted_df and resampled_df (training data) and test_df_full (testing data) data csv files as Spark DataFrames\n",
        "train_df_full = spark.read.csv(drive_path + 'train_df_full.csv', header=True, inferSchema=True)\n",
        "test_df_full = spark.read.csv(drive_path + 'test_df_full.csv', header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "2XuhVucTVAqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "Z9ozj765PqrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add `Weight` column to dataframe\n",
        "\n",
        "As we've seen, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll add a new column `Weight` to `train_df_full` specifying the weights to use, which we pass in to PySpark models in the `weightCol` parameter. We'll create the new `Weight` column using the `set_weight_col` function in `helper_functions`.\n",
        "\n",
        "Our initial `Weight` column will specify equal weights across all observations, which is the default in PySpark."
      ],
      "metadata": {
        "id": "09uu3jqXN0D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(set_weight_col.__doc__)"
      ],
      "metadata": {
        "id": "3uBAlM6OQgyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_full = set_weight_col(train_df_full, label_col='Label', pos_class_weight=1.0, neg_class_weight=1.0)"
      ],
      "metadata": {
        "id": "gVqOxSzCP5r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview Weight column\n",
        "cols_to_show = ['MessageId', 'Label', 'Weight']\n",
        "train_df_full.select(cols_to_show).where(train_df_full.Label == 0).show(1, truncate=False, vertical=True)\n",
        "train_df_full.select(cols_to_show).where(train_df_full.Label == 1).show(1, truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "1yOERTKURGF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shape of dataframes\n",
        "print(f\"train_df_full:  {train_df_full.count():,} Rows, {len(train_df_full.columns)} Columns\")\n",
        "print(f\"test_df_full:  {test_df_full.count():,} Rows, {len(test_df_full.columns)} Columns\")"
      ],
      "metadata": {
        "id": "UcoMf27IVOLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print schema of training dataframe\n",
        "print('train_df_full:')\n",
        "train_df_full.printSchema()"
      ],
      "metadata": {
        "id": "OWm1UBS1VOSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print schema of test dataframe\n",
        "print('test_df_full:')\n",
        "test_df_full.printSchema()"
      ],
      "metadata": {
        "id": "SAlIlsWPVObJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'MessageId' individual transaction identifier column - will not be used in modeling\n",
        "train_df_full = train_df_full.drop('MessageId')\n",
        "test_df_full = test_df_full.drop('MessageId')\n",
        "\n",
        "# Rename target variable 'Label' column to more descriptive 'Anomalous'\n",
        "train_df_full = train_df_full.withColumnRenamed('Label', 'Anomalous')\n",
        "test_df_full = test_df_full.withColumnRenamed('Label', 'Anomalous')"
      ],
      "metadata": {
        "id": "s7nTo2BMXmWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first row of train_df_full dataframe\n",
        "train_df_full.show(n=1, truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "B2jaFCBIwpiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first row of test_df_full dataframe\n",
        "test_df_full.show(n=1, truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "7WB9ibsFWhC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display value counts for 'Anomalous' column (classification target)\n",
        "class_counts = train_df_full.groupBy('Anomalous').count().withColumn('percent', F.col('count')/train_df_full.count())\n",
        "\n",
        "class_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "A4NBBo5Z8HcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "jTFLL4FrXelJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create `Pipeline` to Preprocess and Model Data"
      ],
      "metadata": {
        "id": "eytS6s3lWXio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Index string columns with `StringIndexer`"
      ],
      "metadata": {
        "id": "ndf2Vu3xWzKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stages = []\n",
        "\n",
        "categoricalCols = [item[0] for item in train_df_full.dtypes if item[1].startswith('string')]\n",
        "\n",
        "indexers = []\n",
        "for col in categoricalCols:\n",
        "    indexer = StringIndexer(inputCol=col, outputCol=col + '_index', handleInvalid='keep')\n",
        "    indexers.append(indexer)\n",
        "    \n",
        "indexed_features = []\n",
        "for si in indexers:\n",
        "    indexed_features.append(si.getOutputCol())\n",
        "    \n",
        "print(f\"Indexed nominal categorical features: {len(indexed_features)}\")\n",
        "print(indexed_features)"
      ],
      "metadata": {
        "id": "3YdFfLtdWzRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a `OneHotEncoder` to encode the indexed string features"
      ],
      "metadata": {
        "id": "s6X8UUZCWzXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder(inputCols=indexed_features, \n",
        "                        outputCols=[col + '_ohe' for col in indexed_features], \n",
        "                        dropLast=True)\n",
        "\n",
        "print(f\"One hot encoded nominal categorical features: {len(encoder.getOutputCols())}\")\n",
        "print(encoder.getOutputCols())"
      ],
      "metadata": {
        "id": "GEIvAPM5Wzcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile numeric features, not including target or class weight columns"
      ],
      "metadata": {
        "id": "8EK2-6LwZa8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = []\n",
        "for column, dtype in train_df_full.dtypes:\n",
        "    if column != 'Anomalous' and column != 'Weight' and dtype != 'string':\n",
        "        numeric_features.append(column)\n",
        "\n",
        "# Confirm equal column counts\n",
        "assert len(train_df_full.drop('Anomalous', 'Weight').columns) == len(indexed_features) + len(numeric_features)\n",
        "print(f\"Numeric features: {len(numeric_features)}\\n{numeric_features}\")"
      ],
      "metadata": {
        "id": "3s3zpOEkWzhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print names of final features going into the model\n",
        "features = encoder.getOutputCols() + numeric_features\n",
        "print(f\"Final features: \\n{features}\")"
      ],
      "metadata": {
        "id": "TJyrAkENWzmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a `VectorAssembler` to combine all features"
      ],
      "metadata": {
        "id": "8Iz7ulwAavec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=features, outputCol='vectorized_features')\n",
        "\n",
        "# Assemble a list of stages that includes the vector assembler and standard scaler\n",
        "scaler = StandardScaler(inputCol='vectorized_features', outputCol='scaled_features')\n",
        "\n",
        "stages = indexers + [encoder, assembler, scaler]\n",
        "print(\"Stages:\", stages)"
      ],
      "metadata": {
        "id": "27pnqdyva17G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preview modeling pipeline"
      ],
      "metadata": {
        "id": "ZYwGcc_dbciE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "pipeline_model = pipeline.fit(train_df_full)\n",
        "\n",
        "pipeline_df = pipeline_model.transform(train_df_full)"
      ],
      "metadata": {
        "id": "MlwoYrCKbjYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first row of train_df_full after running through pipeline\n",
        "pipeline_df.show(1, vertical=True, truncate=False)"
      ],
      "metadata": {
        "id": "STgdImWXbwSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_test = pipeline.fit(test_df_full)\n",
        "\n",
        "pipeline_df_test = pipeline_model.transform(test_df_full)\n",
        "\n",
        "pipeline.fit(test_df_full).transform(test_df_full).head()['scaled_features'].size"
      ],
      "metadata": {
        "id": "lH2vg1cjbzij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first row of test_df_full after running through pipeline\n",
        "pipeline_df_test.show(1, vertical=True, truncate=False)"
      ],
      "metadata": {
        "id": "1gZDYHp_c2Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "AB1-xUWMjSFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Baseline Model**: Decision Tree Classifier with Tuned Max Depth\n",
        "\n",
        "For our baseline model on the full training dataset, we'll use the best classifier from notebook-3, which was a `DecisionTreeClassifier`."
      ],
      "metadata": {
        "id": "4wbyZc3Qj4Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_1 = DecisionTreeClassifier(\n",
        "    featuresCol='scaled_features',\n",
        "    labelCol='Anomalous',\n",
        "    weightCol='Weight')\n",
        "\n",
        "dt_1_stages = stages + [dt_1]\n",
        "\n",
        "# Specify parameter grid\n",
        "dt_1_grid = ParamGridBuilder()\\\n",
        "            .addGrid(dt_1.maxDepth, [2, 3, 4, 5, 6])\\\n",
        "            .build()"
      ],
      "metadata": {
        "id": "jON0zgwYj4Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Run grid search using grid_search function\n",
        "if not os.path.isdir(drive_path + 'dt_1_model_full'):\n",
        "    dt_1_model = grid_search(stages_with_classifier=dt_1_stages, \n",
        "                             train_df=train_df_full, \n",
        "                             model_grid=dt_1_grid, \n",
        "                             parallelism=5)"
      ],
      "metadata": {
        "id": "5upLtn2yj4fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model, or upload if already saved\n",
        "if not os.path.isdir(drive_path + 'dt_1_model_full'):\n",
        "    dt_1_model.save(drive_path + 'dt_1_model_full')\n",
        "else:\n",
        "    dt_1_model = TrainValidationSplitModel.load(drive_path + 'dt_1_model_full')\n",
        "    print(dt_1_grid[np.argmax(dt_1_model.validationMetrics)])"
      ],
      "metadata": {
        "id": "D7GPF8pyj4lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model scores\n",
        "score_model(dt_1_model, train_df_full, test_df_full)"
      ],
      "metadata": {
        "id": "RH-rOVRfj4sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot model confusion matrix\n",
        "plot_confusion_matrix(dt_1_model, test_df_full)"
      ],
      "metadata": {
        "id": "U1AYbQjzj4xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FJTipsDBj43Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dQyo1xZIj49O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uDTNhaDXj5CH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}