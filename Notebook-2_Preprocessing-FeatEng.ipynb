{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook-2_Preprocessing-FeatEng",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPlwqOOkRA/rbCMdO6oZARg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kwcbs5bBIV2"
      },
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
      },
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
      },
      "source": [
        "## Notebook 2: Preprocessing and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
      },
      "source": [
        "### By Ryan Posternak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
      },
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
      },
      "source": [
        "## Goal: Build a model for FinCEN that can accurately identify anomalous financial transactions, as measured by area under the precision-recall curve (AUPRC)\n",
        "\n",
        "*This is a project for learning purposes. FinCEN is not involved with this project in any way.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
      },
      "source": [
        "# Imports and Reading in Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
      },
      "source": [
        "### Google colab compatibility downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "!pip install pyspark==3.3.0\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EJVZLDUCRdy"
      },
      "outputs": [],
      "source": [
        "# Connect to Google drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
      },
      "source": [
        "### Import libraries, packages and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
      },
      "outputs": [],
      "source": [
        "# Check Colab GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Colab RAM info\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "8RdtiSPt22Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
      },
      "outputs": [],
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "def set_css():\n",
        "    display(HTML(\"\"\"\n",
        "    <style>\n",
        "      pre {\n",
        "          white-space: pre-wrap;\n",
        "      }\n",
        "    </style>\n",
        "    \"\"\"))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config(\"spark.ui.port\", \"4050\")\\\n",
        "        .config(\"spark.driver.memory\", \"15g\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
      },
      "source": [
        "### Description of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8590747c-e25c-405c-815b-3c83f10309ea"
      },
      "source": [
        "**Dataset 1 – Transactions:**\n",
        "\n",
        "`MessageId` - Globally unique identifier within this dataset for individual transactions<br>\n",
        "`UETR` - The Unique End-to-end Transaction Reference—a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction<br>\n",
        "`TransactionReference` - Unique identifier for an individual transaction<br>\n",
        "`Timestamp` - Time at which the individual transaction was initiated<br>\n",
        "`Sender` - Institution (bank) initiating/sending the individual transaction<br>\n",
        "`Receiver` - Institution (bank) receiving the individual transaction<br>\n",
        "`OrderingAccount` - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction<br>\n",
        "`OrderingName` - Name for the originating ordering entity<br>\n",
        "`OrderingStreet` - Street address for the originating ordering entity<br>\n",
        "`OrderingCountryCityZip` - Remaining address details for the originating ordering entity<br>\n",
        "`BeneficiaryAccount` - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction<br>\n",
        "`BeneficiaryName` - Name for the final beneficiary entity<br>\n",
        "`BeneficiaryStreet` - Street address for the final beneficiary entity<br>\n",
        "`BeneficiaryCountryCityZip` - Remaining address details for the final beneficiary entity<br>\n",
        "`SettlementDate` - Date the individual transaction was settled<br>\n",
        "`SettlementCurrency` - Currency used for transaction<br>\n",
        "`SettlementAmount` - Value of the transaction net of fees/transfer charges/forex<br>\n",
        "`InstructedCurrency` - Currency of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`InstructedAmount` - Value of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`Label` - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.<br>\n",
        "<br>\n",
        "**Dataset 2 – Banks:**\n",
        "\n",
        "`Bank` - Identifier for the bank<br>\n",
        "`Account` - Identifier for the account<br>\n",
        "`Name` - Name of the account<br>\n",
        "`Street` - Street address associated with the account<br>\n",
        "`CountryCityZip` - Remaining address details associated with the account<br>\n",
        "`Flags` - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are below:<br>\n",
        "00 - No flags<br>\n",
        "01 - Account closed<br>\n",
        "03 - Account recently opened<br>\n",
        "04 - Name mismatch<br>\n",
        "05 - Account under monitoring<br>\n",
        "06 - Account suspended<br>\n",
        "07 - Account frozen<br>\n",
        "08 - Non-transaction account<br>\n",
        "09 - Beneficiary deceased<br>\n",
        "10 - Invalid company ID<br>\n",
        "11 - Invalid individual ID<br>\n",
        "<br>\n",
        "Additional information from data providers:<br>\n",
        "\"Because each end-to-end transaction is defined by one originating orderer and one final beneficiary, the `OrderingAccount` and `BeneficiaryAccount` in a given row may not necessarily belong to the bank in that row's `Sender` and the bank in that row's `Receiver`, respectively. The correct way to associate an `OrderingAccount` to the correct bank is to identify the `Sender` bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a `BeneficiaryAccount` to the correct bank is to identify the `Receiver` bank in the final (last) individual transaction in that end-to-end transaction.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "## Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd295db2-0718-42c6-8a70-df41e93e72fa"
      },
      "outputs": [],
      "source": [
        "# Read in transactions training and testing data csv files as Spark DataFrames\n",
        "train_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "test_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in banks data csv file to a Spark DataFrame\n",
        "banks_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/bank_dataset.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "x_TRgs-i5mX4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
      },
      "source": [
        "# Preprocessing & Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
      },
      "source": [
        "Steps:\n",
        "1. Create `OrderingCountry` and `BeneficiaryCountry` features to be used for `OrderingCountryFreq` and `BeneficiaryCountryFreq`\n",
        "2. Create `InstructedAmountUSD` feature by converting all currencies in `InstructedAmount` to USD-scale\n",
        "3. Create `IntermediaryTransactions` feature by counting number of repeated UETRs for each end-to-end transaction\n",
        "4. Create `OriginalSender` and `FinalReceiver` features\n",
        "5. Create `Flagged` feature by joining `Flags` column of `banks_df`\n",
        "6. Create `OrderingCountryFreq` and `BeneficiaryCountryFreq` features\n",
        "7. Create `SenderHourFreq` feature\n",
        "8. Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features\n",
        "9. Create `SenderFreq` and `ReceiverFreq` features\n",
        "9. Create `SenderReceiverFreq` feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `OrderingCountry` and `BeneficiaryCountry` features"
      ],
      "metadata": {
        "id": "MGYjYVm_7WGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the content of OrderingCountryCityZip and BeneficiaryCountryCityZip on the forward-slash character ('/')\n",
        "split_cols_ordering_train = F.split(train_df['OrderingCountryCityZip'], '/')\n",
        "split_cols_beneficiary_train = F.split(train_df['BeneficiaryCountryCityZip'], '/')\n",
        "\n",
        "# Add the OrderingCountry as a column to train_df and test_df\n",
        "train_df = train_df.withColumn('OrderingCountry', split_cols_ordering_train.getItem(0))\n",
        "train_df = train_df.withColumn('BeneficiaryCountry', split_cols_beneficiary_train.getItem(0))\n",
        "\n",
        "# Repeat for test_df\n",
        "split_cols_ordering_test = F.split(test_df['OrderingCountryCityZip'], '/')\n",
        "split_cols_beneficiary_test = F.split(test_df['BeneficiaryCountryCityZip'], '/')\n",
        "test_df = test_df.withColumn('OrderingCountry', split_cols_ordering_test.getItem(0))\n",
        "test_df = test_df.withColumn('BeneficiaryCountry', split_cols_beneficiary_test.getItem(0))"
      ],
      "metadata": {
        "id": "SaJuwALY4A5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy check"
      ],
      "metadata": {
        "id": "ZozO_v1K74kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three random UETR codes\n",
        "sample = train_df.sample(False, 0.00001).limit(3).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(3)]\n",
        "\n",
        "# Only show relevant columns\n",
        "cols_to_show = ['UETR', 'OrderingCountryCityZip', 'OrderingCountry', 'BeneficiaryCountryCityZip', 'BeneficiaryCountry']\n",
        "\n",
        "# Preview transactions\n",
        "train_df.filter(train_df.UETR.isin(UETRs)).select(cols_to_show).show(truncate=False)"
      ],
      "metadata": {
        "id": "0-kAKEwA4BH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display value counts for new 'OrderingCountry' and 'BeneficiaryCountry' features of train_df\n",
        "ordering_country_counts = train_df.groupBy('OrderingCountry').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "beneficiary_country_counts = train_df.groupBy('BeneficiaryCountry').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "ordering_country_counts.show(28, truncate=False)\n",
        "beneficiary_country_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "rjw4keRn4BSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- It looks like there are some rows with incorrect/unusual values, but for most of them it looks pretty obvious what the intended value was supposed to be. A couple are a bit ambiguous, e.g. 'E' in `OrderingCountries` could be either `DE`, `ES`, `EC` or `PE`. For these ones, we'll just impute the most frequent likely intended value."
      ],
      "metadata": {
        "id": "a8Y0r4cXHgdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace incorrect/unusual ordering country values with most likely intended value\n",
        "train_df = train_df.replace('F', 'FR', subset='OrderingCountry')\n",
        "train_df = train_df.replace('GBS1 1TD', 'GB', subset='OrderingCountry')\n",
        "train_df = train_df.replace('B', 'GB', subset='OrderingCountry')\n",
        "train_df = train_df.replace('GBE5C 7TG', 'GB', subset='OrderingCountry')\n",
        "train_df = train_df.replace('U', 'US', subset='OrderingCountry')\n",
        "train_df = train_df.replace('D', 'DE', subset='OrderingCountry')\n",
        "train_df = train_df.replace('R', 'FR', subset='OrderingCountry')\n",
        "train_df = train_df.replace('GBBD6 2YA', 'GB', subset='OrderingCountry')\n",
        "train_df = train_df.replace('E', 'ES', subset='OrderingCountry')\n",
        "train_df = train_df.replace('FR46216 JOSEPH', 'FR', subset='OrderingCountry')\n",
        "train_df = train_df.replace('FR97705 PEREIRA', 'FR', subset='OrderingCountry')\n",
        "train_df = train_df.replace('G', 'GB', subset='OrderingCountry')\n",
        "train_df = train_df.replace('S', 'US', subset='OrderingCountry')\n",
        "train_df = train_df.replace('DE51111 WITTSTOCK', 'DE', subset='OrderingCountry')\n",
        "\n",
        "# Replace incorrect/unusual beneficiary country values with most likely intended value\n",
        "train_df = train_df.replace('E', 'ES', subset='BeneficiaryCountry')\n",
        "train_df = train_df.replace('D', 'DE', subset='BeneficiaryCountry')\n",
        "train_df = train_df.replace('USMACKENZIESTAD| KS 08333', 'US', subset='BeneficiaryCountry')\n",
        "train_df = train_df.replace('USMICHAELVIEW| NH 97912', 'US', subset='BeneficiaryCountry')\n",
        "train_df = train_df.replace('U', 'US', subset='BeneficiaryCountry')\n",
        "train_df = train_df.replace('G', 'GB', subset='BeneficiaryCountry')\n",
        "train_df = train_df.replace('USEMILYVIEW| KS 16890', 'US', subset='BeneficiaryCountry')\n",
        "train_df = train_df.replace('USAGUILARSIDE| KY 08409', 'US', subset='BeneficiaryCountry')\n",
        "\n",
        "# Display new value counts for 'OrderingCountry' and 'BeneficiaryCountry' features of train_df\n",
        "ordering_country_counts = train_df.groupBy('OrderingCountry').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "beneficiary_country_counts = train_df.groupBy('BeneficiaryCountry').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "ordering_country_counts.show(truncate=False)\n",
        "beneficiary_country_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "jmYz8WA34BnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat for `test_df`"
      ],
      "metadata": {
        "id": "5jI84zzN4BxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display value counts for new 'OrderingCountry' and 'BeneficiaryCountry' features of test_df\n",
        "ordering_country_counts = test_df.groupBy('OrderingCountry').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "beneficiary_country_counts = test_df.groupBy('BeneficiaryCountry').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "\n",
        "ordering_country_counts.show(28, truncate=False)\n",
        "beneficiary_country_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "ndHvpECvPO8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace incorrect/unusual ordering country values with most likely intended value\n",
        "test_df = test_df.replace('F', 'FR', subset='OrderingCountry')\n",
        "test_df = test_df.replace('R', 'FR', subset='OrderingCountry')\n",
        "test_df = test_df.replace('D', 'DE', subset='OrderingCountry')\n",
        "test_df = test_df.replace('S', 'US', subset='OrderingCountry')\n",
        "\n",
        "# Replace incorrect/unusual beneficiary country values with most likely intended value\n",
        "test_df = test_df.replace('U', 'US', subset='BeneficiaryCountry')\n",
        "test_df = test_df.replace('USDPO AA 17341', 'US', subset='BeneficiaryCountry')\n",
        "test_df = test_df.replace('USFPO AE 29185', 'US', subset='BeneficiaryCountry')\n",
        "test_df = test_df.replace('S', 'US', subset='BeneficiaryCountry')\n",
        "\n",
        "# Display new value counts for 'OrderingCountry' and 'BeneficiaryCountry' features of test_df\n",
        "ordering_country_counts = test_df.groupBy('OrderingCountry').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "beneficiary_country_counts = test_df.groupBy('BeneficiaryCountry').count().withColumn('percent', F.col('count')/test_df.count())\n",
        "\n",
        "ordering_country_counts.show(truncate=False)\n",
        "beneficiary_country_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "G1FXHbSLPVbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize target class distributions or ordering countries and beneficiary countries of ordering and beneficiary entities"
      ],
      "metadata": {
        "id": "H-Mrrwz0PVuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5% of train_df for visualizations (approximately 235k observations) for efficiency\n",
        "viz_cols = ['OrderingCountry', 'BeneficiaryCountry', 'Label']\n",
        "viz_df = train_df.select(viz_cols).sample(withReplacement=False, fraction=0.05, seed=42).toPandas()"
      ],
      "metadata": {
        "id": "ZveJ5oHpU-1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent color palette where y-axis values in both axes, otherwise silver\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {}\n",
        "for val, color in zip(viz_df['OrderingCountry'].value_counts().index, palette):\n",
        "    if val in viz_df[viz_df.Label == 0]['OrderingCountry'].value_counts().index \\\n",
        "    and val in viz_df[viz_df.Label == 1]['OrderingCountry'].value_counts().index:\n",
        "        palette_map[val] = color\n",
        "    else:\n",
        "        palette_map[val]= 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='OrderingCountry', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['OrderingCountry'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='OrderingCountry', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['OrderingCountry'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.05\n",
        "    txt_y = p.get_height()+350\n",
        "    ax1.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.29\n",
        "    txt_y = p.get_height()+0.7\n",
        "    ax2.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "ax1.set_title('Ordering Entity Countries of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Ordering Entity Country', fontsize=16)\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), fontsize=12)\n",
        "ax1.set_ylabel('Count (5% Sample)', fontsize=16)\n",
        "ax1.set_yticklabels(['{:,}'.format(int(x)) for x in ax1.get_yticks()], fontsize=12)\n",
        "ax2.set_title('Ordering Entity Countries of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Ordering Entity Country', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), fontsize=12)\n",
        "ax2.set_ylabel('Count (5% Sample)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticks().astype(int), fontsize=12);"
      ],
      "metadata": {
        "id": "s4SbkQ1FPV9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent color palette where y-axis values in both axes, otherwise silver\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {}\n",
        "for val, color in zip(viz_df['BeneficiaryCountry'].value_counts().index, palette):\n",
        "    if val in viz_df[viz_df.Label == 0]['BeneficiaryCountry'].value_counts().index \\\n",
        "    and val in viz_df[viz_df.Label == 1]['BeneficiaryCountry'].value_counts().index:\n",
        "        palette_map[val] = color\n",
        "    else:\n",
        "        palette_map[val]= 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='BeneficiaryCountry', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['BeneficiaryCountry'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='BeneficiaryCountry', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['BeneficiaryCountry'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.12\n",
        "    txt_y = p.get_height()+350\n",
        "    ax1.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.29\n",
        "    txt_y = p.get_height()+0.7\n",
        "    ax2.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "ax1.set_title('Beneficiary Entity Countries of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Beneficiary Entity Country', fontsize=16)\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), fontsize=12)\n",
        "ax1.set_ylabel('Count (5% Sample)', fontsize=16)\n",
        "ax1.set_yticklabels(['{:,}'.format(int(x)) for x in ax1.get_yticks()], fontsize=12)\n",
        "ax2.set_title('Beneficiary Entity Countries of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Beneficiary Entity Country', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), fontsize=12)\n",
        "ax2.set_ylabel('Count (5% Sample)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticks().astype(int), fontsize=12);"
      ],
      "metadata": {
        "id": "1k_aXayXWuVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks**:\n",
        "- These visualizations bear resemblance to the sender banks visualization, where we have a large selection of values (countries in this case) among non-anomalous transactions and a much smaller subset of values among anomalous transactions. It does look like there is signal in this feature, given the marked difference between non-anomalous and anomalous transactions for both ordering and beneficiary entity countries."
      ],
      "metadata": {
        "id": "pyPBbfykYVGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "qja5DZhqUWVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `InstructedAmountUSD` feature\n",
        "\n",
        "Here we are going to create a new column with standardized instructed transaction amounts, as it wouldn't make much sense to leave the amounts in completely different scales. We'll scale all amounts to their USD conversion rates on 2022/01/12, the median transaction date in `train_df`.\n",
        "\n",
        "Exchange rates were obtained from <a href='https://www.xe.com/currencytables/' >xe.com</a>."
      ],
      "metadata": {
        "id": "dIJtcRwffYNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exchange rate values on 2022/01/12 (median date of transactions in dataframe)\n",
        "'1 NZD = 0.6845945591 USD'\n",
        "'1 GBP = 1.3696247772 USD'\n",
        "'1 CAD = 0.7999730579 USD'\n",
        "'1 EUR = 1.1431474656 USD'\n",
        "'1 AUD = 0.7277363105 USD'\n",
        "'1 JPY = 0.0087157154 USD'\n",
        "'1 INR = 0.0135530538 USD'\n",
        "\n",
        "conversion_rates = {'NZD': 0.6845945591, 'GBP': 1.3696247772, 'CAD': 0.7999730579, 'EUR': 1.1431474656, \\\n",
        "                    'AUD': 0.7277363105, 'JPY': 0.0087157154, 'INR': 0.0135530538, 'USD': 1.0}\n",
        "\n",
        "# Create new column in train and test dataframes with conversion_rates dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*conversion_rates.items())])\n",
        "\n",
        "train_df = train_df.withColumn('InstructedAmountUSD', \\\n",
        "                               mapping_expr[F.col('InstructedCurrency')]*F.col('InstructedAmount'))\n",
        "test_df = test_df.withColumn('InstructedAmountUSD', \\\n",
        "                             mapping_expr[F.col('InstructedCurrency')]*F.col('InstructedAmount'))\n",
        "\n",
        "# Convert column to integer type\n",
        "train_df = train_df.withColumn('InstructedAmountUSD', F.round(train_df['InstructedAmountUSD']).cast(IntegerType()))\n",
        "test_df = test_df.withColumn('InstructedAmountUSD', F.round(test_df['InstructedAmountUSD']).cast(IntegerType()))"
      ],
      "metadata": {
        "id": "RV99fdyYhoEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 10 transactions to verify accuracy of InstructedAmountUSD column\n",
        "train_df.select('InstructedCurrency', 'InstructedAmount', 'InstructedAmountUSD').show(10, truncate=False)"
      ],
      "metadata": {
        "id": "XJpQC56Jq4y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize target class distributions of instructed transaction amounts in USD\n",
        "\n",
        "We'll only plot instructed transaction amounts, since settlement amounts are essentially equivalent to instructed amounts less deductions for fees and transfer/forex charges. We'll also only plot the `InstructedAmountUSD` column values, i.e. instructed amounts that have been standardized to the USD exchange rate, as it wouldn't be very informative to plot the wildly varying scales of the different currencies together."
      ],
      "metadata": {
        "id": "0LS-Q_MKP-Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5% of train_df for visualizations (approximately 235k observations) for efficiency\n",
        "viz_cols = ['InstructedAmountUSD', 'Label']\n",
        "viz_df = train_df.select(viz_cols).sample(withReplacement=False, fraction=0.05, seed=42).toPandas()"
      ],
      "metadata": {
        "id": "tQzkGhCqxlbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Plot histogram of non-anomalous transactions; log x-axis due to extreme right-skew\n",
        "sns.histplot(x='InstructedAmountUSD', data=viz_df[viz_df.Label == 0], ax=ax1, bins=10, log_scale=True, color='#85bb65')\n",
        "\n",
        "# Plot histogram of anomalous transactions; log x-axis due to extreme right-skew\n",
        "sns.histplot(x='InstructedAmountUSD', data=viz_df[viz_df.Label == 1], ax=ax2, bins=10, log_scale=True, color='#85bb65')\n",
        "\n",
        "ax1.set_title('Instructed Transaction Amounts of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Instructed Amount USD (Log-Scale)', fontsize=16)\n",
        "ax1.set_ylabel('Count (5% Sample)', fontsize=16)\n",
        "ax2.set_title('Instructed Transaction Amounts of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Instructed Amount USD (Log-Scale)', fontsize=16)\n",
        "ax2.set_ylabel('Count (5% Sample)', fontsize=16);\n",
        "\n",
        "# Set xticks to match bin widths\n",
        "ax1.set_xticks([bin.get_x() for bin in ax1.patches] + [viz_df[viz_df.Label == 0]['InstructedAmountUSD'].max()])\n",
        "ax2.set_xticks([bin.get_x() for bin in ax2.patches] + [viz_df[viz_df.Label == 1]['InstructedAmountUSD'].max()]);\n",
        "# Change xtick labels to more readable format\n",
        "ax1.set_xticklabels(['$1', '$10', '$200', '$2.8k', '$41k', '$590k', '$8.6M', '$125M', '$1.8B', '$26B', '$380B'], fontsize=12)\n",
        "ax1.set_yticklabels(['{:,}'.format(int(x)) for x in ax1.get_yticks()], fontsize=12)\n",
        "ax2.set_xticklabels(['$14k', '$51k', '$180k', '$640k', '$2.3M', '$8.0M', '$28M', '$100M', '$350M', '$1.3B', '$4.4B'], fontsize=12)\n",
        "ax2.set_yticklabels(ax2.get_yticks().astype(int), fontsize=12);"
      ],
      "metadata": {
        "id": "A_RVSEVPP_C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- The distributions of instructed transaction amounts for both non-anomalous and anomalous transactions are both extremely right-skewed, which we can tell because the distributions look more or less normal after log-scaling.\n",
        "- Non-anomalous transactions appear to have a much larger range of values, ranging from \\$1 all the way to over \\$380B. The range for anomalous transactions is much more narrow, ranging from \\$14k to just under $4.5B."
      ],
      "metadata": {
        "id": "WuV-8cQ6Mkrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "vAAgzvMkPo6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `IntermediaryTransactions` feature by counting number of duplicate UETR occurrences for each unique UETR code\n",
        "\n",
        "This feature will tell us the number of intermediary transactions conducted in each end-to-end transaction. This can be calculated for each end-to-end transaction by counting how many times the UETR for that end-to-end transaction code appears."
      ],
      "metadata": {
        "id": "cbFubaRbM93B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "\n",
        "# Create new feature using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH UETRCounts AS (\n",
        "SELECT UETR, \n",
        "COUNT(UETR)-1 AS IntermediaryTransactions\n",
        "FROM train_df_sql\n",
        "GROUP BY UETR\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "UETRCounts.IntermediaryTransactions\n",
        "FROM train_df_sql\n",
        "LEFT JOIN UETRCounts\n",
        "    ON train_df_sql.UETR = UETRCounts.UETR\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "_b5ZaGXFKO7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy check"
      ],
      "metadata": {
        "id": "d_jUjLAuPs06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve random UETR codes of transactions where entity used 0, 1 and 2 intermediary banks\n",
        "sample1_UETR = train_df.filter(train_df.IntermediaryTransactions == 0).sample(False, 0.00001).limit(1).collect()[0]['UETR']\n",
        "sample2_UETR = train_df.filter(train_df.IntermediaryTransactions == 1).sample(False, 0.00001).limit(1).collect()[0]['UETR']\n",
        "sample3_UETR = train_df.filter(train_df.IntermediaryTransactions == 2).sample(False, 0.0001).limit(1).collect()[0]['UETR']\n",
        "\n",
        "# Display sample of transactions to verify accuracy of OriginalSender and FinalReceiver\n",
        "#  columns; IntermediaryTransactions value should match number of rows in sample - 1\n",
        "cols_to_show = ['MessageId', 'UETR', 'Sender', 'Receiver', 'IntermediaryTransactions']\n",
        "train_df.filter(train_df.UETR == sample1_UETR).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == sample2_UETR).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == sample3_UETR).select(cols_to_show).show(truncate=False)"
      ],
      "metadata": {
        "id": "LkfTn9ffPe_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat for `test_df`"
      ],
      "metadata": {
        "id": "z5d2N12eNUZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "test_df.createOrReplaceTempView('test_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH UETRCounts AS (\n",
        "SELECT UETR, \n",
        "COUNT(UETR)-1 AS IntermediaryTransactions\n",
        "FROM test_df_sql\n",
        "GROUP BY UETR\n",
        ")\n",
        "SELECT test_df_sql.*, \n",
        "UETRCounts.IntermediaryTransactions\n",
        "FROM test_df_sql\n",
        "LEFT JOIN UETRCounts\n",
        "    ON test_df_sql.UETR = UETRCounts.UETR\n",
        "\"\"\"\n",
        "\n",
        "test_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "hy_mi5XwNXGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "V-zwi4-VOySw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `OriginalSender` and `FinalReceiver` features by identifying original `Sender` and final `Receiver` for each end-to-end transaction\n",
        "\n",
        "As described in the preliminary EDA section, the correct way to associate ordering and beneficiary accounts with their proper banks is to identify the `Sender` bank in the first transaction of the end-to-end transaction and the `Receiver` bank in the last transaction of the end-to-end transaction. Below, we'll create new columns which do just that."
      ],
      "metadata": {
        "id": "1hNM-s9Jt6gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON train_df_sql.UETR = EarliestTransaction.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON train_df_sql.UETR = LatestTransaction.UETR\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "gRk-mbypt61M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy check"
      ],
      "metadata": {
        "id": "Qh-DOM0uPgpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three random UETR codes of transactions where entity used intermediary banks\n",
        "sample = train_df.filter(train_df.IntermediaryTransactions > 0).sample(False, 0.0001).limit(3).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(3)]\n",
        "\n",
        "# Display sample of transactions to verify accuracy of OriginalSender and FinalReceiver columns\n",
        "cols_to_show = ['Timestamp', 'UETR', 'Sender', 'Receiver', 'OriginalSender', 'FinalReceiver']\n",
        "train_df.filter(train_df.UETR == UETRs[0]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[1]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[2]).select(cols_to_show).show(truncate=False)"
      ],
      "metadata": {
        "id": "OMFQEpbbvUXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat for `test_df`"
      ],
      "metadata": {
        "id": "3tKAQvb1D5fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "test_df.createOrReplaceTempView('test_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM test_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM test_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM test_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM test_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT test_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM test_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON test_df_sql.UETR = EarliestTransaction.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON test_df_sql.UETR = LatestTransaction.UETR\n",
        "\"\"\"\n",
        "\n",
        "test_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "i8y8H-yIDw9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize target class distributions of senders, final senders, receivers, and final receivers\n",
        "\n",
        "Here we will do a similar visualization as we did in the EDA section, except instead of comparing the same banks across non-anomalous and anomalous transactions, the primary comparison here will be the difference in frequency between how often a given bank is a regular sender vs. original sender, and regular receiver vs. final receiver.\n",
        "\n",
        "If these visualizations show a great deal of difference between the two across multiple banks, it'll tell us that these features carry a lot of signal and are worth bringing into the final dataframe as one hot encoded features."
      ],
      "metadata": {
        "id": "_oNxWqbyuz8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5% of train_df for visualizations (approximately 235k observations) for efficiency\n",
        "viz_cols = ['Sender', 'OriginalSender', 'Label']\n",
        "viz_df = train_df.select(viz_cols).sample(withReplacement=False, fraction=0.05, seed=42).toPandas()\n",
        "# Melt dataframe so original/final senders and receivers and can be plotted side-by-side\n",
        "viz_df = pd.melt(viz_df, id_vars=['Label'], value_vars=['Sender', 'OriginalSender'], var_name='Type', value_name='Bank')"
      ],
      "metadata": {
        "id": "nlb0elVNbqiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(y='Bank', data=viz_df[viz_df.Label == 0], ax=ax1, hue='Type', \n",
        "              order=viz_df[viz_df.Label == 0]['Bank'].value_counts().index,  # Order descending\n",
        "              palette='tab10')\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(y='Bank', data=viz_df[viz_df.Label == 1], ax=ax2, hue='Type', \n",
        "              order=viz_df[viz_df.Label == 1]['Bank'].value_counts().index,  # Order descending\n",
        "              palette='tab10')\n",
        "\n",
        "# # Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()+100\n",
        "    y = p.get_y() + p.get_height()-0.15\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# # Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()+0.3\n",
        "    y = p.get_y() + p.get_height()-0.18\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Sender Banks of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Count (5% Sample)', fontsize=16)\n",
        "ax1.set_xticklabels(['{:,}'.format(int(x)) for x in ax1.get_xticks()], fontsize=12)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax1.set_yticklabels(ax1.get_yticklabels(), fontsize=11)\n",
        "ax1.legend(title='Type', loc='center right', fontsize=12, title_fontsize=13)\n",
        "ax2.set_title('Sender Banks of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Count (5% Sample)', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticks().astype(int), fontsize=12)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticklabels(), fontsize=11)\n",
        "ax2.legend(title='Type', loc='center right', fontsize=12, title_fontsize=13);"
      ],
      "metadata": {
        "id": "T6Gm3B51bwee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5% of train_df for visualizations (approximately 235k observations) for efficiency\n",
        "viz_cols = ['Receiver', 'FinalReceiver', 'Label']\n",
        "viz_df = train_df.select(viz_cols).sample(withReplacement=False, fraction=0.05, seed=42).toPandas()\n",
        "viz_df = pd.melt(viz_df, id_vars=['Label'], value_vars=['Receiver', 'FinalReceiver'], var_name='Type', value_name='Bank')"
      ],
      "metadata": {
        "id": "g_YuWoqJt_M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(y='Bank', data=viz_df[viz_df.Label == 0], ax=ax1, hue='Type', \n",
        "              order=viz_df[viz_df.Label == 0]['Bank'].value_counts().index,  # Order descending\n",
        "              palette='tab10')\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(y='Bank', data=viz_df[viz_df.Label == 1], ax=ax2, hue='Type', \n",
        "              order=viz_df[viz_df.Label == 1]['Bank'].value_counts().index,  # Order descending\n",
        "              palette='tab10')\n",
        "\n",
        "# # Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()+100\n",
        "    y = p.get_y() + p.get_height()-0.15\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# # Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()+0.3\n",
        "    y = p.get_y() + p.get_height()-0.1\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Receiver Banks of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Count (5% Sample)', fontsize=16)\n",
        "ax1.set_xticklabels(['{:,}'.format(int(x)) for x in ax1.get_xticks()], fontsize=12)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax1.set_yticklabels(ax1.get_yticklabels(), fontsize=11)\n",
        "ax1.legend(title='Type', loc='center right', fontsize=12, title_fontsize=13)\n",
        "ax2.set_title('Receiver Banks of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Count (5% Sample)', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticks().astype(int), fontsize=12)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticklabels(), fontsize=11)\n",
        "ax2.legend(title='Type', loc='center right', fontsize=12, title_fontsize=13);"
      ],
      "metadata": {
        "id": "AGQMrlYbtmqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks**\n",
        "- While there is definitely a noticeable difference in frequency between original vs. final sender/receiver for some banks, overall the difference is minor for the majority. However, we see an interesting trend where original senders appear to be concentrated among four banks for anomalous transactions, while non-anomalous transactions include many more. This trend reverses though for receiver banks, where there is a greater variety of banks among anomalous transactions than among non-anomalous ones."
      ],
      "metadata": {
        "id": "dGR4R2Piv1mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "FbDrx_YM09YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `Flagged` feature by joining `Flags` column of `banks_df`\n",
        "\n",
        "As we saw in the preliminary EDA section, none of the accounts with non-zero flags are associated with `OrderingAccount`s; they are all associated with `BeneficiaryAccount`s. It therefore wouldn't make any sense to create a column of `OrderingAccontFlag`s as every value in it would be `0`. We'll only make a column of `BeneficiaryAccountFlag`s."
      ],
      "metadata": {
        "id": "wYeuL8b51iGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary tables to use for SQL query\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "# Create new feature using SQL\n",
        "join_sql = \"\"\"\n",
        "SELECT train_df_sql.*, \n",
        "banks_df_sql.Account AS MatchingBeneficiaryAccount, \n",
        "banks_df_sql.Flags AS BeneficiaryAccountFlag\n",
        "FROM train_df_sql\n",
        "LEFT JOIN banks_df_sql\n",
        "    ON train_df_sql.BeneficiaryAccount = banks_df_sql.Account\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "fLOzGMds5K77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy check"
      ],
      "metadata": {
        "id": "jO1i-_EpPktA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three random UETR codes of transactions with non-zero flags\n",
        "sample = train_df.filter(train_df.BeneficiaryAccountFlag != 0).sample(False, 0.1).limit(3).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(3)]\n",
        "\n",
        "# Only show relevant columns of train_df\n",
        "cols_to_show_train = ['Timestamp', 'UETR', 'OrderingAccount', 'BeneficiaryAccount', \\\n",
        "                      'MatchingBeneficiaryAccount', 'BeneficiaryAccountFlag']\n",
        "\n",
        "# Only show relevant columns of banks_df\n",
        "cols_to_show_bank = ['Account', 'Flags']\n",
        "\n",
        "# Get selected beneficiary accounts of train_df to display side-by-side with matching account in banks_df\n",
        "first_account = train_df.filter(train_df.UETR == UETRs[0]).collect()[0]['BeneficiaryAccount']\n",
        "second_account = train_df.filter(train_df.UETR == UETRs[1]).collect()[0]['BeneficiaryAccount']\n",
        "third_account = train_df.filter(train_df.UETR == UETRs[2]).collect()[0]['BeneficiaryAccount']\n",
        "\n",
        "train_df.filter(train_df.UETR == UETRs[0]).select(cols_to_show_train).show(truncate=False, vertical=True)\n",
        "banks_df.filter(banks_df.Account == first_account).select(cols_to_show_bank).show(truncate=False, vertical=True)\n",
        "print('\\n')\n",
        "train_df.filter(train_df.UETR == UETRs[1]).select(cols_to_show_train).show(truncate=False, vertical=True)\n",
        "banks_df.filter(banks_df.Account == second_account).select(cols_to_show_bank).show(truncate=False, vertical=True)\n",
        "print('\\n')\n",
        "train_df.filter(train_df.UETR == UETRs[2]).select(cols_to_show_train).show(truncate=False, vertical=True)\n",
        "banks_df.filter(banks_df.Account == third_account).select(cols_to_show_bank).show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "GUyncvPdsHUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat for `test_df`"
      ],
      "metadata": {
        "id": "HnKfgguueMyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "test_df.createOrReplaceTempView('test_df_sql')\n",
        "\n",
        "# Create new feature using SQL\n",
        "join_sql = \"\"\"\n",
        "SELECT test_df_sql.*, \n",
        "banks_df_sql.Account AS MatchingBeneficiaryAccount, \n",
        "banks_df_sql.Flags AS BeneficiaryAccountFlag\n",
        "FROM test_df_sql\n",
        "LEFT JOIN banks_df_sql\n",
        "    ON test_df_sql.BeneficiaryAccount = banks_df_sql.Account\n",
        "\"\"\"\n",
        "\n",
        "test_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "v39V4pGZsmxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in new BeneficiaryAccountFlag column\n",
        "train_df.where(F.col('BeneficiaryAccountFlag').isNull() | F.isnan('BeneficiaryAccountFlag')).count()"
      ],
      "metadata": {
        "id": "NTWhPJNGuS5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace null values in BeneficiaryAccountFlag with value of '2' which will signify that there is no \n",
        "# matching account in banks_df\n",
        "train_df = train_df.fillna(value=2, subset='BeneficiaryAccountFlag')\n",
        "test_df = test_df.fillna(value=2, subset='BeneficiaryAccountFlag')\n",
        "\n",
        "assert train_df.where(F.col('BeneficiaryAccountFlag').isNull() | F.isnan('BeneficiaryAccountFlag')).count() == 0\n",
        "assert test_df.where(F.col('BeneficiaryAccountFlag').isNull() | F.isnan('BeneficiaryAccountFlag')).count() == 0"
      ],
      "metadata": {
        "id": "9wBHF1LAugQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check distribution of flagged beneficiary accounts in train_df\n",
        "train_df.groupBy('Label', 'BeneficiaryAccountFlag').agg({'Label': 'count'}).show()"
      ],
      "metadata": {
        "id": "GWkwLBiyixkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- The math checks out on the number of flagged beneficiary accounts - we saw 800 flagged accounts in the EDA section, and after filling in the 111 missing values with 2s, we have 4,100 zero-flagged anomalous transactions and 800 non-zero-flagged anomalous transactions.\n",
        "- It looks like there are no non-anomalous beneficiary accounts that are non-zero-flagged. In other words, every single non-anomalous transaction has a flag of 0 associated with the beneficiary account. This doesn't mean the classification will be easy now though; 3,989 out of 4,900 anomalous transactions are also zero-flagged.\n",
        "- Since every single non-zero-flagged account, regardless of the type of flag, is anomalous, it actually doesn't matter what kind of flag it is as far as predictive modeling goes, so to one hot encode this feature would pointlessly add excess dimensionality to the dataset. Let's replace `BeneficiaryAccountFlag` with a simple binary categorical `Flagged` column: `0` if zero-flagged and `1` if non-zero-flagged."
      ],
      "metadata": {
        "id": "HgwCI3bgxK8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.withColumn('Flagged', F.when(train_df.BeneficiaryAccountFlag != 0, 1).otherwise(0))\n",
        "test_df = test_df.withColumn('Flagged', F.when(test_df.BeneficiaryAccountFlag != 0, 1).otherwise(0))\n",
        "\n",
        "# Check distribution of new Flagged feature of train_df\n",
        "train_df.groupBy('Label', 'Flagged').agg({'Label': 'count'}).show()"
      ],
      "metadata": {
        "id": "4JSFiGLeRhCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "bwfiKQIhoHjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `OrderingCountryFreq` and `BeneficiaryCountryFreq` features\n",
        "\n",
        "We saw in the creation of `OrderingCountry` and `BeneficiaryCountry` above that there are noticeable difference between non-anomalous and anomalous transactions in the frequency of ordering and beneficiary countries. Let's create numerical features here which capture that difference."
      ],
      "metadata": {
        "id": "U57gyMnjoHpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary of ordering country frequency values to map from sender country values\n",
        "ordering_country_freq = {}\n",
        "# Create dictionary of beneficiary country currency values to map from beneficiary country values\n",
        "beneficiary_country_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('OrderingCountry', 'BeneficiaryCountry').toPandas()\n",
        "pd_test_df = test_df.select('OrderingCountry', 'BeneficiaryCountry').toPandas()\n",
        "\n",
        "for oc in set(\n",
        "    list(pd_train_df['OrderingCountry'].unique()) + list(pd_test_df['OrderingCountry'].unique())\n",
        "):\n",
        "    ordering_country_freq[oc] = len(pd_train_df[pd_train_df['OrderingCountry'] == oc])\n",
        "\n",
        "for bc in set(\n",
        "    list(pd_train_df['BeneficiaryCountry'].unique()) + list(pd_test_df['BeneficiaryCountry'].unique())\n",
        "):\n",
        "    beneficiary_country_freq[bc] = len(pd_train_df[pd_train_df['BeneficiaryCountry'] == bc])\n",
        "\n",
        "# Create new column in train and test dataframes with ordering_country_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*ordering_country_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('OrderingCountryFreq', mapping_expr[F.col('OrderingCountry')])\n",
        "test_df = test_df.withColumn('OrderingCountryFreq', mapping_expr[F.col('OrderingCountry')])\n",
        "\n",
        "# Create new column in train and test dataframes with beneficiary_country_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*beneficiary_country_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('BeneficiaryCountryFreq', mapping_expr[F.col('BeneficiaryCountry')])\n",
        "test_df = test_df.withColumn('BeneficiaryCountryFreq', mapping_expr[F.col('BeneficiaryCountry')])"
      ],
      "metadata": {
        "id": "NNsCHNW2oHu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve five random UETR codes of transactions\n",
        "sample = train_df.sample(False, 0.00001).limit(5).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(5)]\n",
        "\n",
        "# Display sample of transactions to verify accuracy of new columns\n",
        "cols_to_show = ['UETR', 'OrderingCountry', 'OrderingCountryFreq', 'BeneficiaryCountry', 'BeneficiaryCountryFreq', 'Label']\n",
        "train_df.select(cols_to_show).filter(F.col('UETR').isin(UETRs)).show(truncate=15)"
      ],
      "metadata": {
        "id": "q69kySd2oHyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "M2wCEEddPksM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyY21cvovTXM"
      },
      "source": [
        "## Create `SenderHourFreq` feature: transaction hour frequency for each `OriginalSender`\n",
        "\n",
        "This feature will tell us the frequency with which each sender initiated transactions for each hour of the day. This should capture some of the signal of the correlation between the sender and target class as well as the correlation between transaction hour and target class.\n",
        "\n",
        "For this feature, it was not immediately apparent whether we should use the individual transaction sender (`Sender`) or the end-to-end transaction sender (`OriginalSender`). On the one hand, the transaction sender is the actual bank that is associated with the hour with which that transaction was initiated; on the other hand, the `OriginalSender` is the \"correct\" bank to associate with the account-holder. In the end, we ended up trying both and doing a correlation analysis, and we found that the `OriginalSender` had a higher correlation with the `Label` column in the training data, so we used the end-to-end `OriginalSender`.\n",
        "\n",
        "For the testing dataset, we will map the sender hour frequencies from the training dataset; if we were to repeat the process entirely it would result in data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kz9zFgrR5Nj"
      },
      "outputs": [],
      "source": [
        "# Define UDF to extract hour from timestamp\n",
        "hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# Create new column of transaction hours\n",
        "train_df = train_df.withColumn('Hour', hour(train_df.Timestamp))\n",
        "test_df = test_df.withColumn('Hour', hour(test_df.Timestamp))\n",
        "\n",
        "# Create list of unique original senders\n",
        "senders = train_df.select('OriginalSender').toPandas()['OriginalSender'].unique()\n",
        "\n",
        "# Create column of senders concatenated with hours\n",
        "train_df = train_df.withColumn('SenderHour', F.concat(F.col('OriginalSender'), F.col('Hour').cast(StringType())))\n",
        "test_df = test_df.withColumn('SenderHour', F.concat(F.col('OriginalSender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "pd_df = train_df.select('OriginalSender', 'Hour').toPandas()\n",
        "\n",
        "# Create dictionary of sender hour frequency values to map from sender hour values\n",
        "sender_hour_frequency = {}\n",
        "\n",
        "for sender in senders:\n",
        "    sender_rows = pd_df[pd_df['OriginalSender'] == sender]\n",
        "    for hour in range(24):\n",
        "        sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_hour_frequency dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])\n",
        "test_df = test_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve five random UETR codes of transactions\n",
        "sample = train_df.sample(False, 0.00001).limit(5).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(5)]\n",
        "\n",
        "# Display sample of transactions to verify accuracy of new columns\n",
        "cols_to_show = ['UETR', 'OriginalSender', 'Hour', 'SenderHour', 'SenderHourFreq', 'Label']\n",
        "train_df.select(cols_to_show).filter(F.col('UETR').isin(UETRs)).show(truncate=15)"
      ],
      "metadata": {
        "id": "uyWRm5kBs42e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "vyYKWfRaPicy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKfj2pFGQGOW"
      },
      "source": [
        "## Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features: transaction currency frequency and average transaction amount per currency for each sender\n",
        "\n",
        "These features will tell us the frequency with which each sender initiated transactions for each currency, in the case of the first feature. For the second feature, it will tell us the average amount with which each sender sent each currency. These features may also be correlated with anomalous transactions.\n",
        "\n",
        "We'll make sure to use `InstructedAmountUSD` and not `InstructedAmount` to keep a consistent scale. Like before, we will use the `OriginalSender` as we know that the correct way to identify an account with its `Sender` bank is to look at the original sender in the end-to-end transaction.\n",
        "\n",
        "Additionally, we'll use the training data to map values to the testing data to avoid data leakage (the testing data is converted to a Pandas dataframe and used in the `for` loop only to retrieve `Sender`-currency combinations that might not be in the training data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZINrnCpHQjkN"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with instructed currencies\n",
        "train_df = train_df.withColumn('SenderCurrency', F.concat(F.col('OriginalSender'), F.col('InstructedCurrency')))\n",
        "test_df = test_df.withColumn('SenderCurrency', F.concat(F.col('OriginalSender'), F.col('InstructedCurrency')))\n",
        "\n",
        "# Create temporary Pandas dataframes of sender currencies and amounts in USD\n",
        "pd_train_df = train_df.select('SenderCurrency', 'InstructedAmountUSD').toPandas()\n",
        "pd_test_df = test_df.select('SenderCurrency', 'InstructedAmountUSD').toPandas()\n",
        "\n",
        "# Create dictionary of sender currency frequency values to map from sender currency values\n",
        "sender_currency_freq = {}\n",
        "# Create dictionary of average sender currency values to map from sender currency values\n",
        "sender_currency_avg = {}\n",
        "\n",
        "for sc in set(\n",
        "    list(pd_train_df['SenderCurrency'].unique()) + list(pd_test_df['SenderCurrency'].unique())\n",
        "):\n",
        "    sender_currency_freq[sc] = len(pd_train_df[pd_train_df['SenderCurrency'] == sc])\n",
        "    sender_currency_avg[sc] = pd_train_df[pd_train_df['SenderCurrency'] == sc][\n",
        "        'InstructedAmountUSD'\n",
        "    ].mean()\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_avg dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_avg.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Convert column to integer type\n",
        "train_df = train_df.withColumn('SenderCurrencyAmtAvg', F.round(train_df['SenderCurrencyAmtAvg']).cast(IntegerType()))\n",
        "test_df = test_df.withColumn('SenderCurrencyAmtAvg', F.round(test_df['SenderCurrencyAmtAvg']).cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve five random UETR codes of transactions\n",
        "sample = train_df.sample(False, 0.00001).limit(5).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(5)]\n",
        "\n",
        "# Display sample of transactions to verify accuracy of new columns\n",
        "cols_to_show = ['UETR', 'OriginalSender', 'InstructedCurrency', 'InstructedAmountUSD', 'SenderCurrencyFreq', 'SenderCurrencyAmtAvg', 'Label']\n",
        "train_df.select(cols_to_show).filter(F.col('UETR').isin(UETRs)).show(truncate=20)"
      ],
      "metadata": {
        "id": "WAdRMi91vbE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "hZ9qYnOISRj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `SenderFreq` and `ReceiverFreq` features: `OriginalSender` and `FinalReceiver` frequencies for each unique `OriginalSender` and `FinalReceiver`"
      ],
      "metadata": {
        "id": "aIKIKGgAm_gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary of sender bank frequency values to map from original sender banks\n",
        "sender_freq = {}\n",
        "# Create dictionary of receiver bank frequency values to map from final receiver banks\n",
        "receiver_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('OriginalSender', 'FinalReceiver').toPandas()\n",
        "pd_test_df = test_df.select('OriginalSender', 'FinalReceiver').toPandas()\n",
        "\n",
        "for os in set(\n",
        "    list(pd_train_df['OriginalSender'].unique()) + list(pd_test_df['OriginalSender'].unique())\n",
        "):\n",
        "    sender_freq[os] = len(pd_train_df[pd_train_df['OriginalSender'] == os])\n",
        "\n",
        "for fr in set(\n",
        "    list(pd_train_df['FinalReceiver'].unique()) + list(pd_test_df['FinalReceiver'].unique())\n",
        "):\n",
        "    receiver_freq[fr] = len(pd_train_df[pd_train_df['FinalReceiver'] == fr])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderFreq', mapping_expr[F.col('OriginalSender')])\n",
        "test_df = test_df.withColumn('SenderFreq', mapping_expr[F.col('OriginalSender')])\n",
        "\n",
        "# Create new column in train and test dataframes with receiver_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*receiver_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('ReceiverFreq', mapping_expr[F.col('FinalReceiver')])\n",
        "test_df = test_df.withColumn('ReceiverFreq', mapping_expr[F.col('FinalReceiver')])"
      ],
      "metadata": {
        "id": "XGxY9nGmm_Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve five random UETR codes of transactions\n",
        "sample = train_df.sample(False, 0.00001).limit(5).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(5)]\n",
        "\n",
        "# Display sample of transactions to verify accuracy of new columns\n",
        "cols_to_show = ['UETR', 'OriginalSender', 'SenderFreq', 'FinalReceiver', 'ReceiverFreq', 'Label']\n",
        "train_df.select(cols_to_show).filter(F.col('UETR').isin(UETRs)).show(truncate=20)"
      ],
      "metadata": {
        "id": "bItEwXpHm_HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "lcBEuIMcm-Fh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lRfKQqI51Il"
      },
      "source": [
        "## Create `SenderReceiverFreq` feature: `OriginalSender`-`FinalReceiver` combination frequency for each sender and receiver pair\n",
        "\n",
        "This feature will tell us the frequency with which each original sender and final receiver executed transactions to one another. This should capture some of the signal in the `FinalReceiver` column that we haven't captured yet up to this point, since we've mostly been focused on the senders in the transactions.\n",
        "\n",
        "Once again, we will use the training data to map the values to the testing data. The testing data is only used to retrieve any `Sender`-`Receiver` pairs that might not be in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNPtx7aS50ic"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with receivers\n",
        "train_df = train_df.withColumn('SenderReceiver', F.concat(F.col('OriginalSender'), F.col('FinalReceiver')))\n",
        "test_df = test_df.withColumn('SenderReceiver', F.concat(F.col('OriginalSender'), F.col('FinalReceiver')))\n",
        "\n",
        "# Create dictionary of sender receiver frequency values to map from sender receiver values\n",
        "sender_receiver_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('SenderReceiver').toPandas()\n",
        "pd_test_df = test_df.select('SenderReceiver').toPandas()\n",
        "\n",
        "for sr in set(\n",
        "    list(pd_train_df['SenderReceiver'].unique()) + list(pd_test_df['SenderReceiver'].unique())\n",
        "):\n",
        "    sender_receiver_freq[sr] = len(pd_train_df[pd_train_df['SenderReceiver'] == sr])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_receiver_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_receiver_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])\n",
        "test_df = test_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve five random UETR codes of transactions\n",
        "sample = train_df.sample(False, 0.00001).limit(5).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(5)]\n",
        "\n",
        "# Display sample of transactions to verify accuracy of new columns\n",
        "cols_to_show = ['UETR', 'OriginalSender', 'FinalReceiver', 'SenderReceiver', 'SenderReceiverFreq', 'Label']\n",
        "train_df.select(cols_to_show).filter(F.col('UETR').isin(UETRs)).show(truncate=20)"
      ],
      "metadata": {
        "id": "nuYUP__h59Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "fzLlb62cee_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Steps"
      ],
      "metadata": {
        "id": "Ah-nAnEo71V3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFkdce4XbMAi"
      },
      "source": [
        "## Drop extraneous columns that will not be used in modeling\n",
        "\n",
        "We're going to drop all categorical columns here. Even though there are multiple nominal categorical features that we could include as one hot encoded \n",
        "features, we likely got most of the signal out of them with the current numerical features, and the extra dimensionality would require a lot of additional processing time for modeling.\n",
        "\n",
        "We'll also drop `InstructedAmount` since it was supplanted with `InstructedAmountUSD`, as well as `SettlementAmount` for the reasons already mentioned.\n",
        "\n",
        "We'll keep `MessageId` for now as an index to identify unique transactions, but we're going to drop it before modeling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = [\n",
        "    'Timestamp',\n",
        "    'UETR',\n",
        "    'Sender',\n",
        "    'Receiver',\n",
        "    'TransactionReference',\n",
        "    'OrderingAccount',\n",
        "    'OrderingName',\n",
        "    'OrderingStreet',\n",
        "    'OrderingCountryCityZip',\n",
        "    'BeneficiaryAccount',\n",
        "    'BeneficiaryName',\n",
        "    'BeneficiaryStreet',\n",
        "    'BeneficiaryCountryCityZip',\n",
        "    'SettlementDate',\n",
        "    'SettlementCurrency',\n",
        "    'SettlementAmount', \n",
        "    'InstructedCurrency', \n",
        "    'InstructedAmount', \n",
        "    'OrderingCountry', \n",
        "    'BeneficiaryCountry', \n",
        "    'SenderHour',\n",
        "    'SenderCurrency',\n",
        "    'SenderReceiver', \n",
        "    'OriginalSender', \n",
        "    'FinalReceiver', \n",
        "    'MatchingBeneficiaryAccount', \n",
        "    'BeneficiaryAccountFlag'\n",
        "]\n",
        "\n",
        "train_df = train_df.drop(*cols_to_drop)\n",
        "test_df = test_df.drop(*cols_to_drop)"
      ],
      "metadata": {
        "id": "_Qki9kYKS5Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review dataframes before saving"
      ],
      "metadata": {
        "id": "KwvPrpeCehpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify we didn't forget to repeat any steps for test_df\n",
        "assert train_df.columns == test_df.columns"
      ],
      "metadata": {
        "id": "Ck5sIrNTfygj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display random row of train_df\n",
        "train_df.sample(False, 0.000001).limit(1).show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "mgpciYdaFdae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display random row of test_df\n",
        "test_df.sample(False, 0.00001).limit(1).show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "JWYsNKzQFiAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null/NaN values in train_df and test_df\n",
        "train_df_null = train_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in train_df.columns if c != 'Timestamp'])\n",
        "\n",
        "test_df_null = test_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                               .alias(c) for c in test_df.columns if c != 'Timestamp'])\n",
        "\n",
        "print('Number of null/NaN values per column, train_df:\\n')\n",
        "train_df_null.show(truncate=False, vertical=True)\n",
        "\n",
        "print('Number of null/NaN values per column, test_df:\\n')\n",
        "test_df_null.show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "rrOkHQ_lzM8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks**:\n",
        "- No null/NaN values - good to go!"
      ],
      "metadata": {
        "id": "wmvSjExHuazz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "9_ZAOfg1J187"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save preprocessed dataframes as CSV files"
      ],
      "metadata": {
        "id": "FzpRyH28OEUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkunOiddOEUH"
      },
      "outputs": [],
      "source": [
        "train_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/train_df_preprocessed.csv', header=True)\n",
        "test_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/test_df_preprocessed.csv', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "MyvCtWOUd8bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "EbS0Jq6md-Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Dataframe 1: `weighted_df`\n",
        "\n",
        "As we saw above, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll create two different dataframes for modeling: one that sets class weights to use in modeling, which is done through a new column specifying the weights to use in `Weight`, and another that we resample through a combination of over and undersampling.\n",
        "\n",
        "We'll create the new `Weight` column using the `set_weight_col` function in helper_functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "09uu3jqXN0D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_weight_col(df, label_col, neg_class_weight):\n",
        "    \"\"\"\n",
        "    Calculates and creates a column of class weights \n",
        "    in a PySpark dataframe with an imbalanced binary \n",
        "    target class distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : Spark `DataFrame`\n",
        "        The Spark `DataFrame` to assign the \n",
        "        `ClassWeight` column to\n",
        "    label_col : Spark `Column`\n",
        "        Label column name\n",
        "    neg_class_weight : '`float` or 'balanced'\n",
        "        New class weight to assign to negative class \n",
        "        (`0`) in weight column. If 'balanced', assigned \n",
        "        class weights will be equal to 1 - proportion of \n",
        "        class in dataframe. If `float`, negative class \n",
        "        will be assigned `neg_class_weight` and positive \n",
        "        class weights remain at 1.0.\n",
        "    \"\"\"\n",
        "    if neg_class_weight == 'balanced':\n",
        "        balancing_ratio = df.filter(F.col(label_col) == 1).count() / df.count()\n",
        "        calculate_weights = F.udf(lambda x: balancing_ratio if x == 0 \n",
        "                                  else 1.0 - balancing_ratio, DoubleType())\n",
        "    else:\n",
        "        calculate_weights = F.udf(lambda x: neg_class_weight if x == 0 \n",
        "                                  else 1.0, DoubleType())\n",
        "\n",
        "    df = df.withColumn('Weight', calculate_weights(label_col))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Ag8L-m7Y6_ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set_weight_col.__doc__)"
      ],
      "metadata": {
        "id": "3ALqIE_qrg5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = set_weight_col(train_df, label_col='Label')"
      ],
      "metadata": {
        "id": "AiuEdyF5z73i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_show = ['MessageId', 'Label', 'ClassWeight']\n",
        "train_df.select(cols_to_show).where(train_df.Label == 0).show(1, truncate=False, vertical=True)\n",
        "train_df.select(cols_to_show).where(train_df.Label == 1).show(1, truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "OcJYBPAm0CL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "D894x7_bNzVu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyxiNwEpbYsD"
      },
      "source": [
        "# Modeling Dataframe 2: `resampled_df`\n",
        "\n",
        "For the second dataset, we'll rebalance the data through a combination of undersampling the majority class (non-amomalous transactions) and oversampling the minority class (anomalous transactions). Using the function I wrote in helper_functions.py, we'll specify a more balanced positive to negative class ratio of 0.25 in the new resampled dataframe, with 4,690,000 observations (roughly equivalent to the number of rows in the original training dataset)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark_resample.__doc__)"
      ],
      "metadata": {
        "id": "Lf4Ww6LtPD7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJkpUEvM-uc5"
      },
      "outputs": [],
      "source": [
        "# Drop ClassWeight column of train_df\n",
        "train_df = train_df.drop('ClassWeight')\n",
        "\n",
        "# Resample train_df; specify approximately 4,690,000 observations in new resampled dataframe, \n",
        "# with more balanced (0.25) ratio of positive target class (1) to negative target class (0)\n",
        "train_df = spark_resample(train_df, ratio=0.25, new_count=4690000, \n",
        "                          class_field='Label', pos_class=1, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efDPlelAE7W"
      },
      "outputs": [],
      "source": [
        "# Print shape of resampled dataframe\n",
        "print(f\"Resampled train_df:  {train_df.count():,} Rows, {len(train_df.columns)} Columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
      },
      "outputs": [],
      "source": [
        "# Preview resampled dataframe\n",
        "train_df.show(3, truncate=False, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmckUs8hfF5r"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) of resampled dataframe\n",
        "resampled_class_counts = train_df.groupBy('Label')\\\n",
        "                                 .count()\\\n",
        "                                 .withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "resampled_class_counts.show(truncate=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save resampled training dataframe as CSV file"
      ],
      "metadata": {
        "id": "dNen6kOq3P17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOsBhtSO6QUw"
      },
      "outputs": [],
      "source": [
        "train_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/train_df_resampled.csv', header=True)"
      ]
    }
  ]
}