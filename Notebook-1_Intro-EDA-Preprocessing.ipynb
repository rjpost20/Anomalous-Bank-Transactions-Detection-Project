{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ],
      "metadata": {
        "id": "6Kwcbs5bBIV2"
      },
      "id": "6Kwcbs5bBIV2"
    },
    {
      "cell_type": "markdown",
      "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a",
      "metadata": {
        "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
      },
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a",
      "metadata": {
        "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
      },
      "source": [
        "## Notebook 1: Intro, EDA and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3",
      "metadata": {
        "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
      },
      "source": [
        "### By Ryan Posternak"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae",
      "metadata": {
        "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
      },
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b",
      "metadata": {
        "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
      },
      "source": [
        "## Goal: \n",
        "\n",
        "*This is a project for learning purposes. The *** is not involved with this project in any way.*\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e",
      "metadata": {
        "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
      },
      "source": [
        "# Overview and Business Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c",
      "metadata": {
        "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089",
      "metadata": {
        "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb",
      "metadata": {
        "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
      },
      "source": [
        "# Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1",
      "metadata": {
        "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655",
      "metadata": {
        "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776806df-5137-44ac-8d76-c06d7906e0c1",
      "metadata": {
        "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
      },
      "source": [
        "# Initial Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1",
      "metadata": {
        "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
      },
      "source": [
        "### Google colab compatibility downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b",
      "metadata": {
        "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "!pip install pyspark==3\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_EJVZLDUCRdy"
      },
      "id": "_EJVZLDUCRdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2",
      "metadata": {
        "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
      },
      "source": [
        "### Import libraries, packages and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d",
      "metadata": {
        "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "from itertools import chain\n",
        "import os\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, TimestampType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d",
      "metadata": {
        "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
      },
      "outputs": [],
      "source": [
        "# Check colab GPU info\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3b4878-4efd-472f-9c10-61710c893d10",
      "metadata": {
        "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
      },
      "outputs": [],
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "    </style>\n",
        "    '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da",
      "metadata": {
        "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "\n",
        "# spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9",
      "metadata": {
        "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
      },
      "source": [
        "### Description of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8590747c-e25c-405c-815b-3c83f10309ea",
      "metadata": {
        "id": "8590747c-e25c-405c-815b-3c83f10309ea"
      },
      "source": [
        "**Dataset 1 – Transactions:**\n",
        "\n",
        "`MessageId` - Globally unique identifier within this dataset for individual transactions<br>\n",
        "`UETR` - The Unique End-to-end Transaction Reference—a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction<br>\n",
        "`TransactionReference` - Unique identifier for an individual transaction<br>\n",
        "`Timestamp` - Time at which the individual transaction was initiated<br>\n",
        "`Sender` - Institution (bank) initiating/sending the individual transaction<br>\n",
        "`Receiver` - Institution (bank) receiving the individual transaction<br>\n",
        "`OrderingAccount` - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction<br>\n",
        "`OrderingName` - Name for the originating ordering entity<br>\n",
        "`OrderingStreet` - Street address for the originating ordering entity<br>\n",
        "`OrderingCountryCityZip` - Remaining address details for the originating ordering entity<br>\n",
        "`BeneficiaryAccount` - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction<br>\n",
        "`BeneficiaryName` - Name for the final beneficiary entity<br>\n",
        "`BeneficiaryStreet` - Street address for the final beneficiary entity<br>\n",
        "`BeneficiaryCountryCityZip` - Remaining address details for the final beneficiary entity<br>\n",
        "`SettlementDate` - Date the individual transaction was settled<br>\n",
        "`SettlementCurrency` - Currency used for transaction<br>\n",
        "`SettlementAmount` - Value of the transaction net of fees/transfer charges/forex<br>\n",
        "`InstructedCurrency` - Currency of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`InstructedAmount` - Value of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`Label` - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.<br>\n",
        "<br>\n",
        "**Dataset 2 – Banks:**\n",
        "\n",
        "`Bank` - Identifier for the bank<br>\n",
        "`Account` - Identifier for the account<br>\n",
        "`Name` - Name of the account<br>\n",
        "`Street` - Street address associated with the account<br>\n",
        "`CountryCityZip` - Remaining address details associated with the account<br>\n",
        "`Flags` - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are below:<br>\n",
        "00 - No flags<br>\n",
        "01 - Account closed<br>\n",
        "03 - Account recently opened<br>\n",
        "04 - Name mismatch<br>\n",
        "05 - Account under monitoring<br>\n",
        "06 - Account suspended<br>\n",
        "07 - Account frozen<br>\n",
        "08 - Non-transaction account<br>\n",
        "09 - Beneficiary deceased<br>\n",
        "10 - Invalid company ID<br>\n",
        "11 - Invalid individual ID<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "### Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dd295db2-0718-42c6-8a70-df41e93e72fa",
      "metadata": {
        "id": "dd295db2-0718-42c6-8a70-df41e93e72fa"
      },
      "outputs": [],
      "source": [
        "# Read in transactions data csv file to a Spark DataFrame\n",
        "transactions_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_dataset.csv', header=True, inferSchema=True)\n",
        "# Persist in memory\n",
        "transactions_df = transactions_df.cache()\n",
        "\n",
        "# Read in banks data csv file to a Spark DataFrame\n",
        "# banks_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/bank_dataset.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529532f1-4b2e-406d-bd3d-63f8784a168c",
      "metadata": {
        "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
      },
      "source": [
        "### Initial EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d",
      "metadata": {
        "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
      },
      "outputs": [],
      "source": [
        "# Print shape of dataframes\n",
        "print(f\"transactions_df:  {transactions_df.count():,} Rows, {len(transactions_df.columns)} Columns\")\n",
        "# print(f\"banks_df:  {banks_df.count():,} Rows, {len(banks_df.columns)} Columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0133a49-0718-421f-9e8d-2d60913780ce",
      "metadata": {
        "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
      },
      "outputs": [],
      "source": [
        "transactions_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922",
      "metadata": {
        "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
      },
      "outputs": [],
      "source": [
        "# banks_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ab8183-bd76-48c2-b647-75285e34e4ac",
      "metadata": {
        "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
      },
      "outputs": [],
      "source": [
        "# Print first 3 rows of transactions dataframe\n",
        "transactions_df.show(n=3, vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44472749-3189-4abf-b646-e0767399adbd",
      "metadata": {
        "id": "44472749-3189-4abf-b646-e0767399adbd"
      },
      "outputs": [],
      "source": [
        "# Print first 5 rows of banks dataframe\n",
        "# banks_df.show(n=5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1",
      "metadata": {
        "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
      },
      "outputs": [],
      "source": [
        "# Print number of null/missing values in each column of transactions_df\n",
        "transactions_df_null = transactions_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                               .alias(c) for c in transactions_df.columns])\n",
        "\n",
        "print('Number of null/missing values per column:\\n')\n",
        "transactions_df_null.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4",
      "metadata": {
        "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
      },
      "outputs": [],
      "source": [
        "# Print number of null/missing values in each column of banks_df\n",
        "# banks_df_null = banks_df.select([F.count(F.when(F.col(c).contains('None') | \\\n",
        "#                                                 F.col(c).contains('NULL') | \\\n",
        "#                                                 (F.col(c) == '' ) | \\\n",
        "#                                                 F.col(c).isNull() | \\\n",
        "#                                                 F.isnan(c), c ))\\\n",
        "#                                  .alias(c) for c in banks_df.columns])\n",
        "\n",
        "# print('Number of null/missing values per column:\\n')\n",
        "# banks_df_null.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2896607b-d653-4cda-9601-f1c4e709a0e0",
      "metadata": {
        "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column in transactions_df; sample 10% of df for efficiency\n",
        "transactions_df_unique = transactions_df.sample(False, 0.1).agg(*(F.countDistinct(F.col(c)) for c in transactions_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column (in sample of 10% of dataframe):\\n\")\n",
        "transactions_df_unique.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990",
      "metadata": {
        "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column in banks_df; sample 10% of df for efficiency\n",
        "# banks_df_unique = banks_df.sample(False, 0.1).agg(*(F.countDistinct(F.col(c)) for c in banks_df.columns))\n",
        "\n",
        "# print(f\"Number of unique values per column (in sample of 10% of dataframe):\\n\")\n",
        "# banks_df_unique.show(vertical=True, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Pandas style summary statistics table of numeric columns\n",
        "num_cols = [item[0] for item in transactions_df.dtypes if item[1] == 'int' or item[1] == 'double']\n",
        "\n",
        "transactions_df.select(num_cols).summary().show(truncate=False)\n"
      ],
      "metadata": {
        "id": "0DJZvwkuvZbs"
      },
      "id": "0DJZvwkuvZbs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc",
      "metadata": {
        "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
      },
      "outputs": [],
      "source": [
        "# Show value counts for 'Label' column (classification target) in transactions_df\n",
        "class_counts = transactions_df.groupBy('Label').count().withColumn('percent', F.col('count')/transactions_df.count())\n",
        "\n",
        "class_counts.show(truncate=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55",
      "metadata": {
        "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like this is an extremely imbalanced dataset - only about 0.1% of the data is in the positive class. We will need to address this class imbalance as part of the modeling process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "e6CMX7wnyShD"
      },
      "id": "e6CMX7wnyShD"
    },
    {
      "cell_type": "markdown",
      "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4",
      "metadata": {
        "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
      },
      "source": [
        "# Preprocessing & Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3",
      "metadata": {
        "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
      },
      "source": [
        "Steps:\n",
        "1. Change column datatypes to correct type\n",
        "2. Drop duplicate transaction rows\n",
        "3. Train/test split\n",
        "4. Create `SenderCurrencyFreq` feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398a670b-b7e1-43b7-835d-8feac9644414",
      "metadata": {
        "id": "398a670b-b7e1-43b7-835d-8feac9644414"
      },
      "outputs": [],
      "source": [
        "# Specify join condition\n",
        "# join_condition = (transactions_df.OrderingAccount == banks_df.Account) | (transactions_df.BeneficiaryAccount == banks_df.Account)\n",
        "\n",
        "# Join dataframes\n",
        "# preprocessed_df = transactions_df.join(banks_df, on=join_condition, how='left')\n",
        "\n",
        "# Unpersist old dataframes from memory\n",
        "# display(transactions_df.unpersist())\n",
        "# display(banks_df.unpersist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde",
      "metadata": {
        "id": "6c870a63-a9a9-4160-894b-ec28e2f94cde"
      },
      "outputs": [],
      "source": [
        "# Print shape of joined dataframe\n",
        "# print(f\"{preprocessed_df.count():,} Rows, {len(preprocessed_df.columns)} Columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert `Timestamp` column to TimestampType"
      ],
      "metadata": {
        "id": "C8f2ccL_vJNt"
      },
      "id": "C8f2ccL_vJNt"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "69edab8e-1177-4ee6-99f9-c1e1020656e6",
      "metadata": {
        "id": "69edab8e-1177-4ee6-99f9-c1e1020656e6"
      },
      "outputs": [],
      "source": [
        "# Convert 'Timestamp' column to TimestampType\n",
        "transactions_df = transactions_df.withColumn('Timestamp', transactions_df.Timestamp.cast(TimestampType()))\n",
        "\n",
        "assert transactions_df.select('Timestamp').dtypes[0][0] == 'Timestamp'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop intermediary transactions (only keep one row per end-to-end transaction)"
      ],
      "metadata": {
        "id": "Nl7pzyqEDTnX"
      },
      "id": "Nl7pzyqEDTnX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print count of unique transactions (as identified by UETR codes)\n",
        "transactions_df.select(F.countDistinct('UETR')).show()"
      ],
      "metadata": {
        "id": "LYOILJD0ISSL"
      },
      "id": "LYOILJD0ISSL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print total number of rows with duplicate UETR values (meaning sender routed transaction through one or more intermediary banks)\n",
        "transactions_df.select('UETR').groupBy('UETR')\\\n",
        "    .count()\\\n",
        "    .where(F.col('count') > 1)\\\n",
        "    .select(F.sum('count'))\\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "-qoi-7oTDTCX"
      },
      "id": "-qoi-7oTDTCX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with duplicate UETR codes, keeping the first occurence (sorted by Timestamp)\n",
        "transactions_df = transactions_df.orderBy('Timestamp').coalesce(1).dropDuplicates(subset = ['UETR'])\n",
        "transactions_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyp2WUWgIRhP",
        "outputId": "ddff0b01-ed5f-4cb4-f5de-bd94c8101341"
      },
      "id": "Pyp2WUWgIRhP",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4054278"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure no duplicates\n",
        "transactions_df.groupBy(transactions_df.UETR).count().where(F.col('count') > 1).count()"
      ],
      "metadata": {
        "id": "OKuFnLt6NHP5"
      },
      "id": "OKuFnLt6NHP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show value counts for 'Label' column (classification target) in new transactions_df\n",
        "class_counts = transactions_df.groupBy('Label').count().withColumn('percent', F.col('count')/transactions_df.count())\n",
        "\n",
        "class_counts.show(truncate=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znGbnVs0RE9Y",
        "outputId": "3ec97042-8abe-4034-ccaf-37a57c95cad8"
      },
      "id": "znGbnVs0RE9Y",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+----------+\n",
            "|Label|  count|   percent|\n",
            "+-----+-------+----------+\n",
            "|    0|4049426|0.99880...|\n",
            "|    1|   4852|0.00119...|\n",
            "+-----+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Test Split"
      ],
      "metadata": {
        "id": "B2-sd2HHPTOv"
      },
      "id": "B2-sd2HHPTOv"
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = transactions_df.randomSplit(weights=[0.75, 0.25], seed=42)"
      ],
      "metadata": {
        "id": "av6JOZDjPSpJ"
      },
      "id": "av6JOZDjPSpJ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `SenderHourFreq` feature: transaction hour frequency for each sender\n",
        "\n",
        "This feature will tell us the frequency with which each sender initiated transactions for each hour of the day. Certain hours of the day are potentially correlated with more anomalous transactions."
      ],
      "metadata": {
        "id": "NyY21cvovTXM"
      },
      "id": "NyY21cvovTXM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define UDF to extract hour from timestamp\n",
        "hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# Create new column of transaction hours\n",
        "train = train.withColumn('Hour', hour(train.Timestamp))\n",
        "test = test.withColumn('Hour', hour(test.Timestamp))\n",
        "\n",
        "# Create list of unique senders\n",
        "senders = train.select('Sender').toPandas()['Sender'].unique()\n",
        "\n",
        "# Create column of senders concatenated with hours\n",
        "train = train.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "test = test.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "\n",
        "df = train.select('Sender', 'Hour').toPandas()\n",
        "\n",
        "# Create dictionary of sender hour frequency values to map from sender hour values\n",
        "sender_hour_frequency = {}\n",
        "for sender in senders:\n",
        "    sender_rows = df[df['Sender'] == sender]\n",
        "    for hour in range(24):\n",
        "        sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_hour_frequency dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "\n",
        "train = train.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])\n",
        "test = test.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ],
      "metadata": {
        "id": "4kz9zFgrR5Nj"
      },
      "id": "4kz9zFgrR5Nj",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c12bd9fa-b95d-4b36-a5c6-1714c4c7be28",
      "metadata": {
        "id": "c12bd9fa-b95d-4b36-a5c6-1714c4c7be28"
      },
      "outputs": [],
      "source": [
        "# # Define UDF to extract hour from timestamp\n",
        "# hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# # Create new column of transaction hours\n",
        "# transactions_df = transactions_df.withColumn('Hour', hour(transactions_df.Timestamp))\n",
        "\n",
        "# # Create list of unique senders\n",
        "# senders = transactions_df.select('Sender').toPandas()['Sender'].unique()\n",
        "\n",
        "# # Create column of unique senders concatenated with hours\n",
        "# transactions_df = transactions_df.withColumn('SenderHour', F.concat(F.col('Sender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "# # Create dictionary of sender hour frequency values to map from sender hour values\n",
        "# df = transactions_df.select('Sender', 'Hour').toPandas()\n",
        "# sender_hour_frequency = {}\n",
        "# for sender in senders:\n",
        "#     sender_rows = df[df['Sender'] == sender]\n",
        "#     for hour in range(24):\n",
        "#         sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# # Create new column in transactions_df with sender_hour_frequency dictionary\n",
        "# mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "# transactions_df = transactions_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features: transaction currency frequency and average transaction amount per currency for each sender"
      ],
      "metadata": {
        "id": "iKfj2pFGQGOW"
      },
      "id": "iKfj2pFGQGOW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create column of senders concatenated with instructed currencies\n",
        "train = train.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "test = test.withColumn('SenderCurrency', F.concat(F.col('Sender'), F.col('InstructedCurrency')))\n",
        "\n",
        "df_train = train.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "df_test = test.select('SenderCurrency', 'InstructedAmount').toPandas()\n",
        "\n",
        "# Create dictionary of sender currency frequency values to map from sender currency values\n",
        "sender_currency_freq = {}\n",
        "# Create dictionary of average sender currency values to map from sender currency values\n",
        "sender_currency_avg = {}\n",
        "\n",
        "for sc in set(\n",
        "    list(df_train['SenderCurrency'].unique()) + list(df_test['SenderCurrency'].unique())\n",
        "):\n",
        "    sender_currency_freq[sc] = len(df_train[df_train['SenderCurrency'] == sc])\n",
        "    sender_currency_avg[sc] = df_train[df_train['SenderCurrency'] == sc][\n",
        "        \"InstructedAmount\"\n",
        "    ].mean()\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_freq.items())])\n",
        "train = train.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "test = test.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_avg dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_avg.items())])\n",
        "train = train.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "test = test.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])"
      ],
      "metadata": {
        "id": "ZINrnCpHQjkN"
      },
      "id": "ZINrnCpHQjkN",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OHIaXw_W51Vn"
      },
      "id": "OHIaXw_W51Vn",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3lRfKQqI51Il"
      },
      "id": "3lRfKQqI51Il",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GNPtx7aS50ic"
      },
      "id": "GNPtx7aS50ic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bs_qEOpGR0qK"
      },
      "id": "bs_qEOpGR0qK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Ba0V9NZRbSE"
      },
      "id": "2Ba0V9NZRbSE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a630285a-f240-42f9-bb32-0c34cb70c8ff",
      "metadata": {
        "id": "a630285a-f240-42f9-bb32-0c34cb70c8ff"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "354e0fa3-eb37-4101-a9f2-fed9ee28f58a",
      "metadata": {
        "id": "354e0fa3-eb37-4101-a9f2-fed9ee28f58a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8",
      "metadata": {
        "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (spark-env)",
      "language": "python",
      "name": "spark-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "name": "Notebook-1_Intro-EDA-Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}