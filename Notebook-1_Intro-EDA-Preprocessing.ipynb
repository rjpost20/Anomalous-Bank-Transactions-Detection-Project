{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook-1_Intro-EDA-Preprocessing",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNlLEGb2KUmOYSfXPxtcSVZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kwcbs5bBIV2"
      },
      "source": [
        "<img src=\"https://github.com/rjpost20/Anomalous-Bank-Transactions-Detection-Project/blob/main/data/AdobeStock_319163865.jpeg?raw=true\">\n",
        "Image by <a href=\"https://stock.adobe.com/contributor/200768506/andsus?load_type=author&prev_url=detail\" >AndSus</a> on Adobe Stock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1853614c-bb9b-4feb-9c74-dd82342eae6a"
      },
      "source": [
        "# Phase 5 Project: *Detecting Anomalous Financial Transactions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4f907d-ec9f-4226-940a-a649a5decf3a"
      },
      "source": [
        "## Notebook 1: Intro, EDA and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a18e8c-9808-4a71-b98f-97f2d040dbc3"
      },
      "source": [
        "### By Ryan Posternak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4d4f196-97af-4b2a-beab-2288fd5327ae"
      },
      "source": [
        "Flatiron School, Full-Time Live NYC<br>\n",
        "Project Presentation Date: August 25th, 2022<br>\n",
        "Instructor: Joseph Mata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5ec167-a06b-4ae7-9813-ffe0d77bc69b"
      },
      "source": [
        "## Goal: Build a model for FinCEN that can accurately identify anomalous financial transactions, as measured by area under the precision-recall curve (AUPRC)\n",
        "\n",
        "*This is a project for learning purposes. FinCEN is not involved with this project in any way.*\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1886193f-a8e7-4fa3-8416-18d84d24a49e"
      },
      "source": [
        "# Overview and Business Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a59a008-35b9-4955-bcd5-2e4ba5da116c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17ed2fb-d360-4f43-a6a0-b7bdae492089"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870710cc-f45e-4f2a-b61a-6cc21e13c8bb"
      },
      "source": [
        "# Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3c287e4-7d1c-40e5-afb8-f497f77d8bc1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2da9320e-b8f3-469d-b4f9-7438fde1d655"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776806df-5137-44ac-8d76-c06d7906e0c1"
      },
      "source": [
        "# Imports, Reading in Data, and Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a628d4-f6f7-46d4-a6d4-4a54f51a02c1"
      },
      "source": [
        "### Google colab compatibility downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b9a80a0-0e96-4cd3-9ad0-e86cfd27090b"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "!pip install pyspark==3.3.0\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_EJVZLDUCRdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e16b828-9c73-4c82-b7db-69dc33a9ff4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f82477-fb1f-4bc6-9de8-07d8ee0486c2"
      },
      "source": [
        "### Import libraries, packages and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ec43d7d-2077-4d43-bd12-cc6c351f746d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "from itertools import chain\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, TimestampType\n",
        "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "# from pyspark.ml import Pipeline\n",
        "# from pyspark.ml.classification import LogisticRegression\n",
        "# from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "# from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML, display\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vc_V7XVRDx1U",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "9ca5e6a2-f5dd-46fc-a6a6-487631e1edf5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4af53b18-9934-4b56-8813-131244afdd67\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4af53b18-9934-4b56-8813-131244afdd67\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving helper_functions.py to helper_functions.py\n"
          ]
        }
      ],
      "source": [
        "helper_functions = files.upload()\n",
        "from helper_functions import spark_resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9c9165-4b95-403f-9e90-6ae09612b92d"
      },
      "outputs": [],
      "source": [
        "# Check Colab GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Colab RAM info\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "8RdtiSPt22Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aa3b4878-4efd-472f-9c10-61710c893d10"
      },
      "outputs": [],
      "source": [
        "# Set text to wrap in Google colab notebook\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "      pre {\n",
        "          white-space: pre-wrap;\n",
        "      }\n",
        "    </style>\n",
        "    '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2d54949d-dc79-446b-bc2a-cce6e6ae92da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0fdb6442-ca81-46f7-9336-312a5b12096d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f58d6670f50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1ecf99480c0f:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c45c0da-3252-4fc8-89c6-4d8260418fc9"
      },
      "source": [
        "### Description of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8590747c-e25c-405c-815b-3c83f10309ea"
      },
      "source": [
        "**Dataset 1 – Transactions:**\n",
        "\n",
        "`MessageId` - Globally unique identifier within this dataset for individual transactions<br>\n",
        "`UETR` - The Unique End-to-end Transaction Reference—a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction<br>\n",
        "`TransactionReference` - Unique identifier for an individual transaction<br>\n",
        "`Timestamp` - Time at which the individual transaction was initiated<br>\n",
        "`Sender` - Institution (bank) initiating/sending the individual transaction<br>\n",
        "`Receiver` - Institution (bank) receiving the individual transaction<br>\n",
        "`OrderingAccount` - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction<br>\n",
        "`OrderingName` - Name for the originating ordering entity<br>\n",
        "`OrderingStreet` - Street address for the originating ordering entity<br>\n",
        "`OrderingCountryCityZip` - Remaining address details for the originating ordering entity<br>\n",
        "`BeneficiaryAccount` - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction<br>\n",
        "`BeneficiaryName` - Name for the final beneficiary entity<br>\n",
        "`BeneficiaryStreet` - Street address for the final beneficiary entity<br>\n",
        "`BeneficiaryCountryCityZip` - Remaining address details for the final beneficiary entity<br>\n",
        "`SettlementDate` - Date the individual transaction was settled<br>\n",
        "`SettlementCurrency` - Currency used for transaction<br>\n",
        "`SettlementAmount` - Value of the transaction net of fees/transfer charges/forex<br>\n",
        "`InstructedCurrency` - Currency of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`InstructedAmount` - Value of the individual transaction as instructed to be paid by the Sender<br>\n",
        "`Label` - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.<br>\n",
        "<br>\n",
        "**Dataset 2 – Banks:**\n",
        "\n",
        "`Bank` - Identifier for the bank<br>\n",
        "`Account` - Identifier for the account<br>\n",
        "`Name` - Name of the account<br>\n",
        "`Street` - Street address associated with the account<br>\n",
        "`CountryCityZip` - Remaining address details associated with the account<br>\n",
        "`Flags` - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are below:<br>\n",
        "00 - No flags<br>\n",
        "01 - Account closed<br>\n",
        "03 - Account recently opened<br>\n",
        "04 - Name mismatch<br>\n",
        "05 - Account under monitoring<br>\n",
        "06 - Account suspended<br>\n",
        "07 - Account frozen<br>\n",
        "08 - Non-transaction account<br>\n",
        "09 - Beneficiary deceased<br>\n",
        "10 - Invalid company ID<br>\n",
        "11 - Invalid individual ID<br>\n",
        "<br>\n",
        "Additional information from data providers:<br>\n",
        "\"Because each end-to-end transaction is defined by one originating orderer and one final beneficiary, the `OrderingAccount` and `BeneficiaryAccount` in a given row may not necessarily belong to the bank in that row's `Sender` and the bank in that row's `Receiver`, respectively. The correct way to associate an `OrderingAccount` to the correct bank is to identify the `Sender` bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a `BeneficiaryAccount` to the correct bank is to identify the `Receiver` bank in the final (last) individual transaction in that end-to-end transaction.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25f65a8-1444-468f-a5ee-9611e4d794ef"
      },
      "source": [
        "## Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dd295db2-0718-42c6-8a70-df41e93e72fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b6d7c90e-c6e5-4936-c847-ff658f6f3554"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Read in transactions training and testing data csv files to Spark DataFrames\n",
        "train_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_train_dataset.csv', header=True, inferSchema=True)\n",
        "test_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/transaction_test_dataset.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Read in banks data csv file to a Spark DataFrame\n",
        "banks_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/bank_dataset.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "u9ePgp3IRE2i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529532f1-4b2e-406d-bd3d-63f8784a168c"
      },
      "source": [
        "## Preliminary EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0bc0590-9efa-4f6b-943e-c509163b4f8d"
      },
      "outputs": [],
      "source": [
        "# Print shape of dataframes\n",
        "print(f\"train_df:  {train_df.count():,} Rows, {len(train_df.columns)} Columns\")\n",
        "print(f\"test_df:  {test_df.count():,} Rows, {len(test_df.columns)} Columns\")\n",
        "print(f\"banks_df:  {banks_df.count():,} Rows, {len(banks_df.columns)} Columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0133a49-0718-421f-9e8d-2d60913780ce"
      },
      "outputs": [],
      "source": [
        "# Print schema of train_df dataframe\n",
        "train_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1d1e353-ef6e-404e-b87d-9657ee0cd922"
      },
      "outputs": [],
      "source": [
        "# Print schema of banks_df dataframe\n",
        "banks_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ab8183-bd76-48c2-b647-75285e34e4ac"
      },
      "outputs": [],
      "source": [
        "# Display first row of train_df\n",
        "train_df.show(n=1, truncate=False, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44472749-3189-4abf-b646-e0767399adbd"
      },
      "outputs": [],
      "source": [
        "# Display first 5 rows of banks dataframe\n",
        "banks_df.show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2896607b-d653-4cda-9601-f1c4e709a0e0"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column of train_df; sample 1% of dataframe for efficiency\n",
        "train_df_unique = train_df.sample(False, 0.01).agg(*(F.countDistinct(F.col(c)).alias(c) for c in train_df.columns))\n",
        "total_rows = train_df_unique.collect()[0]['MessageId']\n",
        "\n",
        "print(f\"Number of unique values per column (in sample of 1% of dataframe):\\n\")\n",
        "print(f\"Total rows in sample: {total_rows}\")\n",
        "train_df_unique.show(truncate=False, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ea7f1f1-2aae-42c0-bbef-6a4128ee6990"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values in each column of banks_df\n",
        "banks_df_unique = banks_df.agg(*(F.countDistinct(F.col(c)).alias(c) for c in banks_df.columns))\n",
        "\n",
        "print(f\"Number of unique values per column:\\n\")\n",
        "print(f\"Total rows: {banks_df.count()}\")\n",
        "banks_df_unique.show(truncate=False, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35dc1315-8b58-4e4d-a042-9fa3a660bce1"
      },
      "outputs": [],
      "source": [
        "# Print number of null/NaN values in each column of train_df\n",
        "train_df_null = train_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in train_df.columns if c != 'Timestamp'])\n",
        "\n",
        "print('Number of null/NaN values per column:\\n')\n",
        "train_df_null.show(truncate=False, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f635e4-c5c0-4383-9e9b-6e67791c72b4"
      },
      "outputs": [],
      "source": [
        "# # Print number of null/NaN values in each column of banks_df\n",
        "banks_df_null = banks_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in banks_df.columns])\n",
        "\n",
        "print('Number of null/NaN values per column:\\n')\n",
        "banks_df_null.show(truncate=False, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DJZvwkuvZbs"
      },
      "outputs": [],
      "source": [
        "# Display summary statistics table of numeric columns in transactions dataframe\n",
        "num_cols = [item[0] for item in train_df.dtypes if item[1] == 'int' or item[1] == 'double']\n",
        "\n",
        "train_df.select(num_cols).summary().show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display value counts for 'Flags' feature of banks_df\n",
        "flag_counts = banks_df.groupBy('Flags').count()\n",
        "\n",
        "# Create list of accounts with non-zero flags in banks_df\n",
        "non_zero_accounts = []\n",
        "for row in banks_df.filter(banks_df.Flags != 0).collect()[:]:\n",
        "    non_zero_accounts.append(row['Account'])\n",
        "\n",
        "# Count occurences of ordering accounts associated with non-zero flags in training data\n",
        "transactions_ordering_flagged_train = train_df.where(F.col('OrderingAccount').isin(non_zero_accounts)).count()\n",
        "# Count occurences of beneficiary accounts associated with non-zero flags in training data\n",
        "transactions_beneficiary_flagged_train = train_df.where(F.col('BeneficiaryAccount').isin(non_zero_accounts)).count()\n",
        "# Count occurences of ordering accounts associated with non-zero flags in testing data\n",
        "transactions_ordering_flagged_test = test_df.where(F.col('OrderingAccount').isin(non_zero_accounts)).count()\n",
        "# Count occurences of beneficiary accounts associated with non-zero flags in testing data\n",
        "transactions_beneficiary_flagged_test = test_df.where(F.col('BeneficiaryAccount').isin(non_zero_accounts)).count()\n",
        "\n",
        "print(f\"Accounts with non-zero flags: {len(non_zero_accounts)}\\n\")\n",
        "print(f\"Transactions with non-zero flag associated with OrderingAccount (train): {transactions_ordering_flagged_train}\")\n",
        "print(f\"Transactions with non-zero flag associated with BeneficiaryAccount (train): {transactions_beneficiary_flagged_train}\\n\")\n",
        "print(f\"Transactions with non-zero flag associated with OrderingAccount (test): {transactions_ordering_flagged_test}\")\n",
        "print(f\"Transactions with non-zero flag associated with BeneficiaryAccount (test): {transactions_beneficiary_flagged_test}\\n\")\n",
        "print('Value counts of Flags feature of banks_df:\\n')\n",
        "flag_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "Yh4iw0RmnPZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f632020-f200-4ecc-b55b-211b67ce6fdc"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) in train_df\n",
        "class_counts = train_df.groupBy('Label').count().withColumn('percent', F.col('count')/train_df.count())\n",
        "\n",
        "class_counts.show(truncate=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba284473-cb1a-45d6-89e6-94b7bdc1cc55"
      },
      "source": [
        "**Remarks:**\n",
        "- This is an extremely imbalanced dataset - only about 0.1% of the data is in the positive class. We will need to address this class imbalance as part of the modeling process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URhSVjScLgfE"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kmSqoPErFV1"
      },
      "source": [
        "## Detailed EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTzpjevZ-3P9"
      },
      "outputs": [],
      "source": [
        "# Sample 2% of train_df for visualizations (approximately 94k observations)\n",
        "viz_cols = ['Sender', 'Receiver', 'InstructedCurrency', 'SettlementCurrency', 'Label']\n",
        "viz_df = train_df.select(viz_cols).sample(withReplacement=False, fraction=0.02, seed=42).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8MmRG_z0UnV"
      },
      "source": [
        "### Visualize target class distributions of sender and receiver banks used in transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZk2VXXfnCKj"
      },
      "outputs": [],
      "source": [
        "# Display unique senders in training dataset\n",
        "print(f\"train_df, {train_df.select('Sender').distinct().count()} unique senders:\")\n",
        "train_df.select('Sender').distinct().show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique senders in training dataset where transaction is anomalous\n",
        "unique_anom_senders_count = train_df.filter(train_df.Label == 1)\\\n",
        "                                    .select('Sender').distinct().count()\n",
        "\n",
        "print(f\"train_df, {unique_anom_senders_count} unique senders among anomalous transactions:\")\n",
        "train_df.filter(train_df.Label == 1).select('Sender').distinct().show(5)"
      ],
      "metadata": {
        "id": "31jOa17PTIHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft4XQOtuqqvG"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Set consistent color palette where y-axis values in both axes, otherwise silver\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {}\n",
        "for val, color in zip(viz_df['Sender'].value_counts().index, palette):\n",
        "    if val in viz_df[viz_df.Label == 0]['Sender'].value_counts().index \\\n",
        "    and val in viz_df[viz_df.Label == 1]['Sender'].value_counts().index:\n",
        "        palette_map[val] = color\n",
        "    else:\n",
        "        palette_map[val]= 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 0], ax=ax1, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 0]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(y='Sender', data=viz_df[viz_df.Label == 1], ax=ax2, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 1]['Sender'].value_counts().index) # Order descending\n",
        "\n",
        "# Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Sender Banks of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Count (2% Sample)', fontsize=16)\n",
        "ax1.set_xticklabels(['{:,}'.format(int(x)) for x in ax1.get_xticks()], fontsize=12)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax1.set_yticklabels(ax1.get_yticklabels(), fontsize=11)\n",
        "ax2.set_title('Sender Banks of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Count (2% Sample)', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticks().astype(int), fontsize=12)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticklabels(), fontsize=11);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEq48KnKpYCt"
      },
      "outputs": [],
      "source": [
        "# Display unique receivers in training dataset\n",
        "print(f\"train_df, {train_df.select('Receiver').distinct().count()} unique receivers:\")\n",
        "train_df.select('Receiver').distinct().show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique receivers in training dataset where transaction is anomalous\n",
        "unique_anom_receivers_count = train_df.filter(train_df.Label == 1)\\\n",
        "                                      .select('Receiver').distinct().count()\n",
        "\n",
        "print(f\"train_df, {unique_anom_receivers_count} unique receivers among anomalous transactions:\")\n",
        "train_df.filter(train_df.Label == 1).select('Receiver').distinct().show(5)"
      ],
      "metadata": {
        "id": "IeVQ6hS2UWLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABDfjBJ1zOa6"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(23, 14))\n",
        "\n",
        "# Set consistent color palette where y-axis values in both axes, otherwise silver\n",
        "palette = sns.color_palette('muted', as_cmap=True)*2\n",
        "palette_map = {}\n",
        "for val, color in zip(viz_df['Receiver'].value_counts().index, palette):\n",
        "    if val in viz_df[viz_df.Label == 0]['Receiver'].value_counts().index \\\n",
        "    and val in viz_df[viz_df.Label == 1]['Receiver'].value_counts().index:\n",
        "        palette_map[val] = color\n",
        "    else:\n",
        "        palette_map[val]= 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "ax1_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 0], ax=ax1, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 0]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Update palette_map with values not found above\n",
        "for val, color in zip(viz_df[viz_df.Label == 1]['Receiver'].value_counts().index, palette):\n",
        "    if val not in palette_map:\n",
        "        palette_map[val] = 'silver'  # Assign values not found above to silver\n",
        "\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "ax2_plot = sns.countplot(y='Receiver', data=viz_df[viz_df.Label == 1], ax=ax2, palette=palette_map, \n",
        "              order=viz_df[viz_df.Label == 1]['Receiver'].value_counts().index)  # Order descending\n",
        "\n",
        "# Print percentages to the right of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 0].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax1.annotate(percentage, (x, y))\n",
        "\n",
        "# Print percentages to the right of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_width()/viz_df[viz_df.Label == 1].shape[0])\n",
        "    x = p.get_x() + p.get_width()\n",
        "    y = p.get_y() + p.get_height()/2\n",
        "    ax2.annotate(percentage, (x, y))\n",
        "\n",
        "ax1.set_title('Receiver Banks of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Count (2% Sample)', fontsize=16)\n",
        "ax1.set_xticklabels(['{:,}'.format(int(x)) for x in ax1.get_xticks()], fontsize=12)\n",
        "ax1.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax1.set_yticklabels(ax1.get_yticklabels(), fontsize=11)\n",
        "ax2.set_title('Receiver Banks of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Count (2% Sample)', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticks().astype(int), fontsize=12)\n",
        "ax2.set_ylabel('Institution (Bank)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticklabels(), fontsize=11);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sr4eFI81RLe"
      },
      "source": [
        "**Remarks:**\n",
        "- It looks like the choice of sender bank is very informative in terms of determining whether a transaction is anomalous or not, while the choice of receiver bank is not nearly as valuable.\n",
        "- Only 4 out of 16 sender banks tend to be utilized in anomalous transactions, while nearly all are utilized in non-anomalous transactions.\n",
        "- Looking at receiver banks, 12 out of 16 tend to be utilized for both anomalous and non-anomalous transactions, and in roughly equal distributions.\n",
        "- There is no need to choose between sender and receiver banks when selecting our features; we can engineer features in sender-receiver bank combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmD5pLD-0hIe"
      },
      "source": [
        "### Visualize target class distributions of instructed and settlement currencies used in transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx_M2kFJm9yx"
      },
      "outputs": [],
      "source": [
        "# Display unique instructed currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('InstructedCurrency').distinct().count()} unique instructed currencies:\")\n",
        "train_df.select('InstructedCurrency').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzOj50aB_IRR"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent color palette where x-axis values in both axes, otherwise silver\n",
        "palette_map = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "for val in viz_df[viz_df.Label == 1]['InstructedCurrency'].value_counts().index:\n",
        "    if val not in viz_df[viz_df.Label == 0]['InstructedCurrency'].value_counts().index:\n",
        "        palette_map[val] = 'silver'\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='InstructedCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['InstructedCurrency'].value_counts().index,  # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.25\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "ax1.set_title('Instructed Currencies of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Instructed Currency', fontsize=16)\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), fontsize=12)\n",
        "ax1.set_ylabel('Count (2% Sample)', fontsize=16)\n",
        "ax1.set_yticklabels(['{:,}'.format(int(x)) for x in ax1.get_yticks()], fontsize=12)\n",
        "ax2.set_title('Instructed Currencies of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Instructed Currency', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), fontsize=12)\n",
        "ax2.set_ylabel('Count (2% Sample)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticks().astype(int), fontsize=12);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--a2nbKmqtlW"
      },
      "outputs": [],
      "source": [
        "# Display unique settlement currencies used in transactions\n",
        "print(f\"train_df, {train_df.select('SettlementCurrency').distinct().count()} unique settlement currencies:\")\n",
        "train_df.select('SettlementCurrency').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_4pIbQGN8GL"
      },
      "outputs": [],
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Set consistent color palette\n",
        "palette_map = {'USD': 'dodgerblue', 'EUR': '#003399', 'GBP':'#C8102E', 'JPY': 'tan'}\n",
        "\n",
        "# Plot countplot of non-anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 0], ax=ax1, \n",
        "              order=viz_df[viz_df.Label == 0]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Plot countplot of anomalous transactions\n",
        "sns.countplot(x='SettlementCurrency', data=viz_df[viz_df.Label == 1], ax=ax2, \n",
        "              order=viz_df[viz_df.Label == 1]['SettlementCurrency'].value_counts().index, # Order descending\n",
        "              palette=palette_map)\n",
        "\n",
        "# Print percentages on top of bars (ax1)\n",
        "for p in ax1.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 0].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+400\n",
        "    ax1.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "# Print percentages on top of bars (ax2)\n",
        "for p in ax2.patches:\n",
        "    txt = str(round(p.get_height() / viz_df[viz_df.Label == 1].shape[0]*100, 1)) + '%'\n",
        "    txt_x = p.get_x()+0.31\n",
        "    txt_y = p.get_height()+0.5\n",
        "    ax2.text(txt_x, txt_y, txt, fontsize=12)\n",
        "\n",
        "ax1.set_title('Settlement Currencies of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Settlement Currency', fontsize=16)\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), fontsize=12)\n",
        "ax1.set_ylabel('Count (2% Sample)', fontsize=16)\n",
        "ax1.set_yticklabels(['{:,}'.format(int(x)) for x in ax1.get_yticks()], fontsize=12)\n",
        "ax2.set_title('Settlement Currencies of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Settlement Currency', fontsize=16)\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), fontsize=12)\n",
        "ax2.set_ylabel('Count (2% Sample)', fontsize=16)\n",
        "ax2.set_yticklabels(ax2.get_yticks().astype(int), fontsize=12);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO7iezULOz8P"
      },
      "source": [
        "**Remarks:**\n",
        "- Instructed currencies seems to be more informative in terms of being correlated with whether or not a transaction is anomalous.\n",
        "- Among instructed currencies, we see the opposite trend as we saw with chosen banks; anomalous transactions tend to use a broader selection of instructed currencies, rather than a more narrow selection as we saw with chosen sender banks.\n",
        "- Among settlement currencies, we see the same four currencies being utilized among both target classes, but in slightly different frequencies.\n",
        "- We will keep the instructed currencies (and one hot encode them) as a feature in the final dataset and drop the settlement currencies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "x_TRgs-i5mX4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7188a23-a1c8-492f-8167-eb0e6f42fbf4"
      },
      "source": [
        "# Preprocessing & Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528ffb7d-1150-4903-9294-4f34ef1c22f3"
      },
      "source": [
        "Steps:\n",
        "1. Create `InstructedAmountUSD` feature by converting all currencies in `InstructedAmount` to USD-scale\n",
        "2. Create `OriginalSender` and `FinalReceiver` features\n",
        "3. Create `IntermediaryTransactions` feature\n",
        "4. Create `BeneficiaryAccountFlag` feature by joining `Flags` column of `banks_df`\n",
        "5. Create `SenderHourFreq` feature\n",
        "6. Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features\n",
        "7. Create `SenderReceiverFreq` feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `InstructedAmountUSD` feature\n",
        "\n",
        "Here we are going to create a new column with standardized instructed transaction amounts, as it wouldn't make much sense to leave the amounts in completely different scales. We'll scale all amounts to their USD conversion rates on 2022/01/12, the median transaction date in `train_df`.\n",
        "\n",
        "Exchange rates were obtained from <a href='https://www.xe.com/currencytables/' >xe.com</a>."
      ],
      "metadata": {
        "id": "dIJtcRwffYNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exchange rate values on 2022/01/12 (median date of transactions in dataframe)\n",
        "'1 NZD = 0.6845945591 USD'\n",
        "'1 GBP = 1.3696247772 USD'\n",
        "'1 CAD = 0.7999730579 USD'\n",
        "'1 EUR = 1.1431474656 USD'\n",
        "'1 AUD = 0.7277363105 USD'\n",
        "'1 JPY = 0.0087157154 USD'\n",
        "'1 INR = 0.0135530538 USD'\n",
        "\n",
        "conversion_rates = {'NZD': 0.6845945591, 'GBP': 1.3696247772, 'CAD': 0.7999730579, 'EUR': 1.1431474656, \\\n",
        "                    'AUD': 0.7277363105, 'JPY': 0.0087157154, 'INR': 0.0135530538, 'USD': 1.0}\n",
        "\n",
        "# Create new column in train and test dataframes with conversion_rates dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*conversion_rates.items())])\n",
        "\n",
        "train_df = train_df.withColumn('InstructedAmountUSD', \\\n",
        "                               mapping_expr[F.col('InstructedCurrency')]*F.col('InstructedAmount'))\n",
        "test_df = test_df.withColumn('InstructedAmountUSD', \\\n",
        "                             mapping_expr[F.col('InstructedCurrency')]*F.col('InstructedAmount'))"
      ],
      "metadata": {
        "id": "RV99fdyYhoEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample of transactions to verify accuracy of InstructedAmountUSD column\n",
        "train_df.select('InstructedCurrency', 'InstructedAmount', 'InstructedAmountUSD').show(10, truncate=False)"
      ],
      "metadata": {
        "id": "XJpQC56Jq4y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize target class distributions of instructed transaction amounts in USD\n",
        "\n",
        "We'll only plot instructed transaction amounts, since settlement amounts are essentially equivalent to instructed amounts less deductions for fees and transfer/forex charges. We'll also only plot the `InstructedAmountUSD` column values, i.e. instructed amounts that have been standardized to the USD exchange rate, as it wouldn't be very informative to plot the wildly varying scales of the different currencies together."
      ],
      "metadata": {
        "id": "0LS-Q_MKP-Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 2% of train_df for visualizations (approximately 94k observations)\n",
        "viz_cols = ['InstructedAmountUSD', 'Label']\n",
        "viz_df = train_df.select(viz_cols).sample(withReplacement=False, fraction=0.02, seed=42).toPandas()"
      ],
      "metadata": {
        "id": "tQzkGhCqxlbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define figure and axes\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(22, 9))\n",
        "\n",
        "# Plot histogram of non-anomalous transactions; log x-axis due to extreme right-skew\n",
        "sns.histplot(x='InstructedAmountUSD', data=viz_df[viz_df.Label == 0], ax=ax1, bins=10, log_scale=True, color='#85bb65')\n",
        "\n",
        "# Plot histogram of anomalous transactions; log x-axis due to extreme right-skew\n",
        "sns.histplot(x='InstructedAmountUSD', data=viz_df[viz_df.Label == 1], ax=ax2, bins=10, log_scale=True, color='#85bb65')\n",
        "\n",
        "ax1.set_title('Instructed Transaction Amounts of Non-Anomalous Transactions (Label 0)', fontsize=17)\n",
        "ax1.set_xlabel('Instructed Amount USD (Log-Scale)', fontsize=16)\n",
        "ax1.set_ylabel('Count (2% Sample)', fontsize=16)\n",
        "ax2.set_title('Instructed Transaction Amounts of Anomalous Transactions (Label 1)', fontsize=17)\n",
        "ax2.set_xlabel('Instructed Amount USD (Log-Scale)', fontsize=16)\n",
        "ax2.set_ylabel('Count (2% Sample)', fontsize=16);\n",
        "\n",
        "# Set xticks to match bin widths\n",
        "ax1.set_xticks([bin.get_x() for bin in ax1.patches] + [viz_df[viz_df.Label == 0]['InstructedAmountUSD'].max()])\n",
        "ax2.set_xticks([bin.get_x() for bin in ax2.patches] + [viz_df[viz_df.Label == 1]['InstructedAmountUSD'].max()]);\n",
        "# Change xtick labels to more readable format\n",
        "ax1.set_xticklabels(['$100', '$1.2k', '$11k', '$93k', '$820k', '$7.2M', '$63M', '$560M', '$4.9B', '$43B', '$380B'], fontsize=12)\n",
        "ax1.set_yticklabels(['{:,}'.format(int(x)) for x in ax1.get_yticks()], fontsize=12)\n",
        "ax2.set_xticklabels(['$30k', '$98k', '$320k', '$1.1M', '$3.5M', '$12M', '$38M', '$120M', '$410M', '$1.3B', '$4.4B'], fontsize=12)\n",
        "ax2.set_yticklabels(ax2.get_yticks().astype(int), fontsize=12);"
      ],
      "metadata": {
        "id": "A_RVSEVPP_C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- The distributions of instructed transaction amounts for both non-anomalous and anomalous transactions are both extremely right-skewed, which we can tell because the distributions look more or less normal after log-scaling.\n",
        "- Non-anomalous transactions appear to have a much larger range of values, ranging from \\$100 all the way to \\$380B. The range for anomalous transactions is much more narrow, ranging from \\$30k to just under $4.5B. This is only a 2% sample, so these aren't necessarily the actual minima and maxima, but the sample is definitely large enough to tell us that anomalous transactions have a tighter range of values."
      ],
      "metadata": {
        "id": "WuV-8cQ6Mkrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "vAAgzvMkPo6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `IntermediaryTransactions` feature by counting number of duplicate UETR occurrences for each unique UETR code\n",
        "\n",
        "This feature will tell us the number of intermediary transactions conducted in each end-to-end transaction. This can be calculated for each end-to-end transaction by counting how many times the UETR for that end-to-end transaction code appears."
      ],
      "metadata": {
        "id": "cbFubaRbM93B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "\n",
        "# Create new feature using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH UETRCounts AS (\n",
        "SELECT UETR, \n",
        "COUNT(UETR)-1 AS IntermediaryTransactions\n",
        "FROM train_df_sql\n",
        "GROUP BY UETR\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "UETRCounts.IntermediaryTransactions\n",
        "FROM train_df_sql\n",
        "LEFT JOIN UETRCounts\n",
        "    ON train_df_sql.UETR = UETRCounts.UETR\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_b5ZaGXFKO7n",
        "outputId": "14390865-6498-4fa5-b20d-38c9396d009f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy check"
      ],
      "metadata": {
        "id": "d_jUjLAuPs06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve random UETR codes of transactions where transactor used 0, 1 and 2 intermediary banks\n",
        "sample1_UETR = train_df.filter(train_df.IntermediaryTransactions == 0).sample(False, 0.00001).limit(1).collect()[0]['UETR']\n",
        "sample2_UETR = train_df.filter(train_df.IntermediaryTransactions == 1).sample(False, 0.00001).limit(1).collect()[0]['UETR']\n",
        "sample3_UETR = train_df.filter(train_df.IntermediaryTransactions == 2).sample(False, 0.0001).limit(1).collect()[0]['UETR']\n",
        "\n",
        "# Display sample of transactions to verify accuracy of OriginalSender and FinalReceiver columns\n",
        "# IntermediaryTransactions value should match number of rows in sample\n",
        "cols_to_show = ['MessageId', 'UETR', 'Sender', 'Receiver', 'IntermediaryTransactions']\n",
        "train_df.filter(train_df.UETR == sample1_UETR).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == sample2_UETR).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == sample3_UETR).select(cols_to_show).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "LkfTn9ffPe_g",
        "outputId": "203214ab-0dae-4a88-a57e-78747a41fd03"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "      pre {\n",
              "          white-space: pre-wrap;\n",
              "      }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "|MessageId |UETR                                |Sender  |Receiver|IntermediaryTransactions|\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "|TRVFBLE4MW|a80d52dc-958a-4126-9d84-bf2c94a4cfba|DPSUFRPP|ABVVUS6S|0                       |\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "|MessageId |UETR                                |Sender  |Receiver|IntermediaryTransactions|\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "|TRVKLTRIOZ|76aab4d0-6616-4d02-ab2c-84a33ba9b377|WVOLDEMM|DPSUFRPP|1                       |\n",
            "|TRFO4ICCJF|76aab4d0-6616-4d02-ab2c-84a33ba9b377|DPSUFRPP|DECKJPJJ|1                       |\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "|MessageId |UETR                                |Sender  |Receiver|IntermediaryTransactions|\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "|TRKY4ZZR8H|a08ee475-3a86-402e-985f-f2d0447dd2c6|ABVVUS6S|DPSUFRPP|2                       |\n",
            "|TRV6I17YMB|a08ee475-3a86-402e-985f-f2d0447dd2c6|DPSUFRPP|WVOLDEMM|2                       |\n",
            "|TRZPUCU3OE|a08ee475-3a86-402e-985f-f2d0447dd2c6|WVOLDEMM|DECKJPJJ|2                       |\n",
            "+----------+------------------------------------+--------+--------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat for `test_df`"
      ],
      "metadata": {
        "id": "z5d2N12eNUZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "test_df.createOrReplaceTempView('train_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH UETRCounts AS (\n",
        "SELECT UETR, \n",
        "COUNT(UETR)-1 AS IntermediaryTransactions\n",
        "FROM test_df_sql\n",
        "GROUP BY UETR\n",
        ")\n",
        "SELECT test_df_sql.*, \n",
        "UETRCounts.IntermediaryTransactions\n",
        "FROM test_df_sql\n",
        "LEFT JOIN UETRCounts\n",
        "    ON test_df_sql.UETR = UETRCounts.UETR\n",
        "\"\"\"\n",
        "\n",
        "test_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "hy_mi5XwNXGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "V-zwi4-VOySw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `OriginalSender` and `FinalReceiver` features by identifying original `Sender` and final `Receiver` for each end-to-end transaction\n",
        "\n",
        "As described in the preliminary EDA section, the correct way to associate ordering and beneficiary accounts with their proper banks is to identify the `Sender` bank in the first transaction of the end-to-end transaction and the `Receiver` bank in the last transaction of the end-to-end transaction. Below, we'll create new columns which do just that."
      ],
      "metadata": {
        "id": "1hNM-s9Jt6gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM train_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM train_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT train_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM train_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON train_df_sql.UETR = EarliestTransaction.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON train_df_sql.UETR = LatestTransaction.UETR\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "gRk-mbypt61M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy check"
      ],
      "metadata": {
        "id": "Qh-DOM0uPgpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three random UETR codes of transactions where transactor used intermediary banks\n",
        "sample = train_df.filter(train_df.IntermediaryTransactions > 0).sample(False, 0.0001).limit(3).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(3)]\n",
        "\n",
        "# Display sample of transactions to verify accuracy of OriginalSender and FinalReceiver columns\n",
        "cols_to_show = ['Timestamp', 'UETR', 'Sender', 'Receiver', 'OriginalSender', 'FinalReceiver']\n",
        "train_df.filter(train_df.UETR == UETRs[0]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[1]).select(cols_to_show).show(truncate=False)\n",
        "train_df.filter(train_df.UETR == UETRs[2]).select(cols_to_show).show(truncate=False)"
      ],
      "metadata": {
        "id": "OMFQEpbbvUXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat for `test_df`"
      ],
      "metadata": {
        "id": "3tKAQvb1D5fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "test_df.createOrReplaceTempView('test_df_sql')\n",
        "\n",
        "# Create new features using SQL\n",
        "join_sql = \"\"\"\n",
        "WITH EarliestTransaction AS (\n",
        "SELECT UETR, \n",
        "Sender, \n",
        "Timestamp\n",
        "FROM test_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MIN(Timestamp) \n",
        "    FROM test_df_sql \n",
        "    GROUP BY UETR)\n",
        "), \n",
        "LatestTransaction AS (\n",
        "SELECT UETR, \n",
        "Receiver, \n",
        "Timestamp\n",
        "FROM test_df_sql \n",
        "WHERE (UETR, Timestamp) IN\n",
        "    (SELECT UETR, MAX(Timestamp) \n",
        "    FROM test_df_sql \n",
        "    GROUP BY UETR)\n",
        ")\n",
        "SELECT test_df_sql.*, \n",
        "EarliestTransaction.Sender AS OriginalSender, \n",
        "LatestTransaction.Receiver AS FinalReceiver\n",
        "FROM test_df_sql\n",
        "LEFT JOIN EarliestTransaction\n",
        "    ON test_df_sql.UETR = EarliestTransaction.UETR\n",
        "LEFT JOIN LatestTransaction\n",
        "    ON test_df_sql.UETR = LatestTransaction.UETR\n",
        "\"\"\"\n",
        "\n",
        "test_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "i8y8H-yIDw9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "FbDrx_YM09YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "jicp-_mvPm7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create `BeneficiaryAccountFlag` feature by joining `Flags` column of `banks_df`\n",
        "\n",
        "As we saw in the preliminary EDA section, none of the accounts with non-zero flags are associated with `OrderingAccount`s; they are all associated with `BeneficiaryAccount`s. It therefore wouldn't make any sense to create a column of `OrderingAccontFlag`s as every value in it would be `0`. We'll only make a column of `BeneficiaryAccountFlag`s."
      ],
      "metadata": {
        "id": "wYeuL8b51iGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary tables to use for SQL query\n",
        "train_df.createOrReplaceTempView('train_df_sql')\n",
        "banks_df.createOrReplaceTempView('banks_df_sql')\n",
        "\n",
        "# Create new feature using SQL\n",
        "join_sql = \"\"\"\n",
        "SELECT train_df_sql.*, \n",
        "banks_df_sql.Account AS MatchingBeneficiaryAccount, \n",
        "banks_df_sql.Flags AS BeneficiaryAccountFlag\n",
        "FROM train_df_sql\n",
        "LEFT JOIN banks_df_sql\n",
        "    ON train_df_sql.BeneficiaryAccount = banks_df_sql.Account\n",
        "\"\"\"\n",
        "\n",
        "train_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "fLOzGMds5K77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy check"
      ],
      "metadata": {
        "id": "jO1i-_EpPktA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve three random UETR codes of transactions with non-zero flags\n",
        "sample = train_df.filter(train_df.BeneficiaryAccountFlag != 0).sample(False, 0.1).limit(3).collect()\n",
        "UETRs = [sample[row]['UETR'] for row in range(3)]\n",
        "\n",
        "# Only show relevant columns of train_df\n",
        "cols_to_show_train = ['Timestamp', 'UETR', 'OrderingAccount', 'BeneficiaryAccount', \\\n",
        "                      'MatchingBeneficiaryAccount', 'BeneficiaryAccountFlag']\n",
        "\n",
        "# Only show relevant columns of banks_df\n",
        "cols_to_show_bank = ['Account', 'Flags']\n",
        "\n",
        "# Get selected beneficiary accounts of train_df to display side-by-side with matching account in banks_df\n",
        "first_account = train_df.filter(train_df.UETR == UETRs[0]).collect()[0]['BeneficiaryAccount']\n",
        "second_account = train_df.filter(train_df.UETR == UETRs[1]).collect()[0]['BeneficiaryAccount']\n",
        "third_account = train_df.filter(train_df.UETR == UETRs[2]).collect()[0]['BeneficiaryAccount']\n",
        "\n",
        "train_df.filter(train_df.UETR == UETRs[0]).select(cols_to_show_train).show(truncate=False, vertical=True)\n",
        "banks_df.filter(banks_df.Account == first_account).select(cols_to_show_bank).show(truncate=False, vertical=True)\n",
        "print('\\n')\n",
        "train_df.filter(train_df.UETR == UETRs[1]).select(cols_to_show_train).show(truncate=False, vertical=True)\n",
        "banks_df.filter(banks_df.Account == second_account).select(cols_to_show_bank).show(truncate=False, vertical=True)\n",
        "print('\\n')\n",
        "train_df.filter(train_df.UETR == UETRs[2]).select(cols_to_show_train).show(truncate=False, vertical=True)\n",
        "banks_df.filter(banks_df.Account == third_account).select(cols_to_show_bank).show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "GUyncvPdsHUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat for `test_df`"
      ],
      "metadata": {
        "id": "HnKfgguueMyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table to use for SQL query\n",
        "test_df.createOrReplaceTempView('test_df_sql')\n",
        "\n",
        "# Create new feature using SQL\n",
        "join_sql = \"\"\"\n",
        "SELECT test_df_sql.*, \n",
        "banks_df_sql.Account AS MatchingBeneficiaryAccount, \n",
        "banks_df_sql.Flags AS BeneficiaryAccountFlag\n",
        "FROM test_df_sql\n",
        "LEFT JOIN banks_df_sql\n",
        "    ON test_df_sql.BeneficiaryAccount = banks_df_sql.Account\n",
        "\"\"\"\n",
        "\n",
        "test_df = spark.sql(join_sql)"
      ],
      "metadata": {
        "id": "v39V4pGZsmxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in new BeneficiaryAccountFlag column\n",
        "train_df.where(F.col('BeneficiaryAccountFlag').isNull() | F.isnan('BeneficiaryAccountFlag')).count()"
      ],
      "metadata": {
        "id": "NTWhPJNGuS5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace null values in BeneficiaryAccountFlag with value of '2' which will signify that there is no \n",
        "# matching account in banks_df\n",
        "train_df = train_df.fillna(value=2, subset='BeneficiaryAccountFlag')\n",
        "test_df = test_df.fillna(value=2, subset='BeneficiaryAccountFlag')\n",
        "\n",
        "assert train_df.where(F.col('BeneficiaryAccountFlag').isNull() | F.isnan('BeneficiaryAccountFlag')).count() == 0\n",
        "assert test_df.where(F.col('BeneficiaryAccountFlag').isNull() | F.isnan('BeneficiaryAccountFlag')).count() == 0"
      ],
      "metadata": {
        "id": "9wBHF1LAugQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check distribution of flagged beneficiary accounts in train_df\n",
        "train_df.groupBy('Label', 'BeneficiaryAccountFlag').agg({'Label': 'count'}).show()"
      ],
      "metadata": {
        "id": "GWkwLBiyixkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:**\n",
        "- The math checks out on the number of flagged beneficiary accounts - we saw 800 flagged accounts in the EDA section, and after filling in the 111 missing values with 2s, we have 4,100 zero-flagged anomalous transactions and 800 non-zero-flagged anomalous transactions.\n",
        "- It looks like there are no non-anomalous beneficiary accounts that are non-zero-flagged. In other words, every single non-anomalous transaction has a flag of 0 associated with the beneficiary account. This doesn't mean the classification will be easy now though; 3,989 out of 4,900 anomalous transactions are also zero-flagged."
      ],
      "metadata": {
        "id": "HgwCI3bgxK8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "M2wCEEddPksM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyY21cvovTXM"
      },
      "source": [
        "## Create `SenderHourFreq` feature: transaction hour frequency for each sender\n",
        "\n",
        "This feature will tell us the frequency with which each sender initiated transactions for each hour of the day. This should capture some of the signal of the correlation between the sender and target class as well as the correlation between transaction hour and target class.\n",
        "\n",
        "We use sender banks and not receiver banks here because, as we saw in EDA, there is a greater difference among non-anomalous and anomalous transactions in the choice of sender bank than receiver bank. We also use the newly created `OriginalSender` column, as we know that the correct way to identify an account with its `Sender` bank is to look at the original sender in the end-to-end transaction.\n",
        "\n",
        "For the testing dataset, we will map the sender hour frequencies from the training dataset; if we were to repeat the process entirely it would result in data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kz9zFgrR5Nj"
      },
      "outputs": [],
      "source": [
        "# Define UDF to extract hour from timestamp\n",
        "hour = F.udf(lambda x: x.hour, IntegerType())\n",
        "\n",
        "# Create new column of transaction hours\n",
        "train_df = train_df.withColumn('Hour', hour(train_df.Timestamp))\n",
        "test_df = test_df.withColumn('Hour', hour(test_df.Timestamp))\n",
        "\n",
        "# Create list of unique original senders\n",
        "senders = train_df.select('OriginalSender').toPandas()['OriginalSender'].unique()\n",
        "\n",
        "# Create column of senders concatenated with hours\n",
        "train_df = train_df.withColumn('SenderHour', F.concat(F.col('OriginalSender'), F.col('Hour').cast(StringType())))\n",
        "test_df = test_df.withColumn('SenderHour', F.concat(F.col('OriginalSender'), F.col('Hour').cast(StringType())))\n",
        "\n",
        "pd_df = train_df.select('OriginalSender', 'Hour').toPandas()\n",
        "\n",
        "# Create dictionary of sender hour frequency values to map from sender hour values\n",
        "sender_hour_frequency = {}\n",
        "\n",
        "for sender in senders:\n",
        "    sender_rows = pd_df[pd_df['OriginalSender'] == sender]\n",
        "    for hour in range(24):\n",
        "        sender_hour_frequency[sender + str(hour)] = len(sender_rows[sender_rows['Hour'] == hour])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_hour_frequency dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_hour_frequency.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])\n",
        "test_df = test_df.withColumn('SenderHourFreq', mapping_expr[F.col('SenderHour')])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "vyYKWfRaPicy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKfj2pFGQGOW"
      },
      "source": [
        "## Create `SenderCurrencyFreq` and `SenderCurrencyAmtAvg` features: transaction currency frequency and average transaction amount per currency for each sender\n",
        "\n",
        "These features will tell us the frequency with which each sender initiated transactions for each currency, in the case of the first feature. For the second feature, it will tell us the average amount with which each sender sent each currency. These features may also be correlated with anomalous transactions.\n",
        "\n",
        "We'll make sure to use `InstructedAmountUSD` and not `InstructedAmount` to keep a consistent scale. Like above, we'll use the `OriginalSender`, and we'll use the training data to map values to the testing data to avoid data leakage (the testing data is converted to a Pandas dataframe and used in the `for` loop only to retrieve `Sender`-currency combinations that might not be in the training data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZINrnCpHQjkN"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with instructed currencies\n",
        "train_df = train_df.withColumn('SenderCurrency', F.concat(F.col('OriginalSender'), F.col('InstructedCurrency')))\n",
        "test_df = test_df.withColumn('SenderCurrency', F.concat(F.col('OriginalSender'), F.col('InstructedCurrency')))\n",
        "\n",
        "pd_train_df = train_df.select('SenderCurrency', 'InstructedAmountUSD').toPandas()\n",
        "pd_test_df = test_df.select('SenderCurrency', 'InstructedAmountUSD').toPandas()\n",
        "\n",
        "# Create dictionary of sender currency frequency values to map from sender currency values\n",
        "sender_currency_freq = {}\n",
        "# Create dictionary of average sender currency values to map from sender currency values\n",
        "sender_currency_avg = {}\n",
        "\n",
        "for sc in set(\n",
        "    list(pd_train_df['SenderCurrency'].unique()) + list(pd_test_df['SenderCurrency'].unique())\n",
        "):\n",
        "    sender_currency_freq[sc] = len(pd_train_df[pd_train_df['SenderCurrency'] == sc])\n",
        "    sender_currency_avg[sc] = pd_train_df[pd_train_df['SenderCurrency'] == sc][\n",
        "        'InstructedAmountUSD'\n",
        "    ].mean()\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyFreq', mapping_expr[F.col('SenderCurrency')])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_currency_avg dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_currency_avg.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])\n",
        "test_df = test_df.withColumn('SenderCurrencyAmtAvg', mapping_expr[F.col('SenderCurrency')])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "hZ9qYnOISRj-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lRfKQqI51Il"
      },
      "source": [
        "## Create `SenderReceiverFreq` feature: `OriginalSender`-`FinalReceiver` combination frequency for each sender and receiver pair\n",
        "\n",
        "This feature will tell us the frequency with which each original sender and final receiver executed transactions to one another. This should capture some of the signal in the `FinalReceiver` column that we haven't captured yet up to this point, since we've mostly been focused on the senders in the transactions.\n",
        "\n",
        "Once again, we will use the training data to map the values to the testing data. The testing data is only used to retrieve any `Sender`-`Receiver` pairs that might not be in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNPtx7aS50ic"
      },
      "outputs": [],
      "source": [
        "# Create column of senders concatenated with receivers\n",
        "train_df = train_df.withColumn('SenderReceiver', F.concat(F.col('OriginalSender'), F.col('FinalReceiver')))\n",
        "test_df = test_df.withColumn('SenderReceiver', F.concat(F.col('OriginalSender'), F.col('FinalReceiver')))\n",
        "\n",
        "# Create dictionary of sender receiver frequency values to map from sender receiver values\n",
        "sender_receiver_freq = {}\n",
        "\n",
        "pd_train_df = train_df.select('SenderReceiver').toPandas()\n",
        "pd_test_df = test_df.select('SenderReceiver').toPandas()\n",
        "\n",
        "for sr in set(\n",
        "    list(pd_train_df['SenderReceiver'].unique()) + list(pd_test_df['SenderReceiver'].unique())\n",
        "):\n",
        "    sender_receiver_freq[sr] = len(pd_train_df[pd_train_df['SenderReceiver'] == sr])\n",
        "\n",
        "# Create new column in train and test dataframes with sender_receiver_freq dictionary\n",
        "mapping_expr = F.create_map([F.lit(x) for x in chain(*sender_receiver_freq.items())])\n",
        "\n",
        "train_df = train_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])\n",
        "test_df = test_df.withColumn('SenderReceiverFreq', mapping_expr[F.col('SenderReceiver')])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "fzLlb62cee_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review dataframes before resampling"
      ],
      "metadata": {
        "id": "KwvPrpeCehpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify we didn't forget to repeat any steps for test_df\n",
        "assert train_df.columns == test_df.columns"
      ],
      "metadata": {
        "id": "Ck5sIrNTfygj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display random row of train_df\n",
        "train_df.sample(False, 0.0000001).limit(1).show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "mgpciYdaFdae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display random row of test_df\n",
        "test_df.sample(False, 0.00001).limit(1).show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "JWYsNKzQFiAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "QkrTTJ9_S2kc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFkdce4XbMAi"
      },
      "source": [
        "## Drop extraneous columns that will not be used in modeling\n",
        "\n",
        "We're going to drop all categorical columns here, save for the one which we are one hot encoding which is `BeneficiaryAccountFlag`. We'll also drop `SettlementAmount` for the reasons already mentioned.\n",
        "\n",
        "We'll keep `MessageId` for now as an index to identify unique transactions, but we're going to drop it before modeling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = [\n",
        "    'Timestamp',\n",
        "    'UETR',\n",
        "    'Sender',\n",
        "    'Receiver',\n",
        "    'TransactionReference',\n",
        "    'OrderingAccount',\n",
        "    'OrderingName',\n",
        "    'OrderingStreet',\n",
        "    'OrderingCountryCityZip',\n",
        "    'BeneficiaryAccount',\n",
        "    'BeneficiaryName',\n",
        "    'BeneficiaryStreet',\n",
        "    'BeneficiaryCountryCityZip',\n",
        "    'SettlementDate',\n",
        "    'SettlementCurrency',\n",
        "    'SettlementAmount', \n",
        "    'InstructedCurrency', \n",
        "    'InstructedAmount', \n",
        "    'SenderHour',\n",
        "    'SenderCurrency',\n",
        "    'SenderReceiver'\n",
        "]\n",
        "\n",
        "train_df = train_df.drop(*cols_to_drop)\n",
        "test_df = test_df.drop(*cols_to_drop)"
      ],
      "metadata": {
        "id": "_Qki9kYKS5Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null/NaN values in train_df and test_df\n",
        "train_df_null = train_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                                 .alias(c) for c in train_df.columns if c != 'Timestamp'])\n",
        "\n",
        "test_df_null = test_df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c))\\\n",
        "                               .alias(c) for c in test_df.columns if c != 'Timestamp'])\n",
        "\n",
        "print('Number of null/NaN values per column, train_df:\\n')\n",
        "train_df_null.show(truncate=False, vertical=True)\n",
        "\n",
        "print('Number of null/NaN values per column, test_df:\\n')\n",
        "test_df_null.show(truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "rrOkHQ_lzM8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks**:\n",
        "- No null/NaN values - good to go!"
      ],
      "metadata": {
        "id": "wmvSjExHuazz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "9_ZAOfg1J187"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyxiNwEpbYsD"
      },
      "source": [
        "# Resample Training Dataset\n",
        "\n",
        "As we saw above, the training dataset is extremely imbalanced in regards to target class distribution. In order to improve modeling performance, we'll rebalance the dataset through a combination of undersampling the majority class (non-amomalous transactions) and oversampling the minority class (anomalous transactions). Using the function I wrote in helper_functions.py, we'll specify an even positive to negative class balance in the new resampled dataframe, with 500,000 observations (so approximately 250k of each)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark_resample.__doc__)"
      ],
      "metadata": {
        "id": "Lf4Ww6LtPD7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJkpUEvM-uc5"
      },
      "outputs": [],
      "source": [
        "# Resample train_df; specify 500,000 observations in new resampled dataframe, with \n",
        "# balanced (0.5) ratio of positive target class (1) to negative target class (0)\n",
        "train_df_resampled = spark_resample(train_df, ratio=0.5, new_count=500000, \n",
        "                                    class_field='Label', pos_class=1, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efDPlelAE7W"
      },
      "outputs": [],
      "source": [
        "# Print shape of resampled dataframe\n",
        "print(f\"train_df_resampled:  {train_df_resampled.count()} Rows, {len(train_df_resampled.columns)} Columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86041fe4-e330-4402-bdac-8b7bf0eac6c8"
      },
      "outputs": [],
      "source": [
        "# Preview resampled dataframe\n",
        "train_df_resampled.show(3, vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmckUs8hfF5r"
      },
      "outputs": [],
      "source": [
        "# Display value counts for 'Label' column (classification target) of resampled dataframe\n",
        "resampled_class_counts = train_df_resampled.groupBy('Label')\\\n",
        "                                           .count()\\\n",
        "                                           .withColumn('percent', F.col('count')/train_df_resampled.count())\n",
        "\n",
        "resampled_class_counts.show(truncate=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save resampled training dataframe and preprocessed test dataframe as CSV files"
      ],
      "metadata": {
        "id": "dNen6kOq3P17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOsBhtSO6QUw"
      },
      "outputs": [],
      "source": [
        "train_df_resampled.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/train_df_resampled.csv', header=True)\n",
        "test_df.coalesce(1).write.csv('/content/drive/MyDrive/Colab Notebooks/test_df_preprocessed.csv', header=True)"
      ]
    }
  ]
}